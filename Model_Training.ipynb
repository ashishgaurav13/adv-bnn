{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, pickle, re, copy\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100, model_name=\"model.pt\"):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"models/%s\" % model_name)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train without Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train, val = random_split(train_dataset,[50000,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2370.3: Val_Loss = 854.657\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 1: Loss = 497.245: Val_Loss = 290.611\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 2: Loss = 205.638: Val_Loss = 155.894\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 3: Loss = 130.801: Val_Loss = 117.28\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 4: Loss = 105.963: Val_Loss = 100.841\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 5: Loss = 96.4253: Val_Loss = 95.1524\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 6: Loss = 91.376: Val_Loss = 90.8357\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 7: Loss = 87.7963: Val_Loss = 88.8543\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 8: Loss = 87.1672: Val_Loss = 88.7424\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 9: Loss = 86.3191: Val_Loss = 89.4872\n",
      "Epoch 10: Loss = 86.4159: Val_Loss = 88.4104\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 11: Loss = 86.3792: Val_Loss = 88.6365\n",
      "Epoch 12: Loss = 85.6451: Val_Loss = 86.6553\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 13: Loss = 86.3234: Val_Loss = 86.6286\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 14: Loss = 85.3079: Val_Loss = 86.1681\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 15: Loss = 85.4789: Val_Loss = 85.6036\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 16: Loss = 85.1741: Val_Loss = 86.2563\n",
      "Epoch 17: Loss = 85.4019: Val_Loss = 87.118\n",
      "Epoch 18: Loss = 84.6323: Val_Loss = 84.8783\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 19: Loss = 85.4995: Val_Loss = 85.8879\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "K = 100\n",
    "modelname = \"BNN\"\n",
    "# Train with SVI\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        loss  += svi.step(images, labels)\n",
    "    loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        val_loss += svi.evaluate_loss(images, labels)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "    if val_loss == min(val_losses):\n",
    "        print(\"New minimum loss was reached\")\n",
    "        # Sample k models from the posterior\n",
    "        sampled_models = [guide(None, None) for i in range(K)]\n",
    "        # Save the models\n",
    "        nn_dicts = []\n",
    "        for i in range(len(sampled_models)):\n",
    "            nn_dicts += [sampled_models[i].state_dict()]\n",
    "        torch.save(nn_dicts, \"models/%s_%s.pt\" % (modelname, round(val_loss,5)))\n",
    "        print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "epochs = 10\n",
    "modelname = \"FFNN\"\n",
    "lr     = 0.01\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model = NN(28*28, 1024, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fcn  = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g\" % (epoch, total_loss))\n",
    "\n",
    "torch.save(model, \"models/%s.pt\" % modelname)\n",
    "print(\"Saved the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN with Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs[:100]:\n",
    "    with open(\"mnist_adv/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 9 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdvBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 88.2253: Val_Loss = 88.6896\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 1: Loss = 86.894: Val_Loss = 86.0552\n",
      "New minimum loss was reached\n",
      "Saved 100 models\n",
      "Epoch 2: Loss = 86.8599: Val_Loss = 87.0401\n",
      "Epoch 3: Loss = 86.5119: Val_Loss = 87.8889\n",
      "Epoch 4: Loss = 87.1113: Val_Loss = 86.2929\n",
      "Epoch 5: Loss = 87.3033: Val_Loss = 86.7481\n",
      "Epoch 6: Loss = 86.679: Val_Loss = 86.8712\n",
      "Epoch 7: Loss = 86.8535: Val_Loss = 86.5329\n",
      "Epoch 8: Loss = 86.3234: Val_Loss = 87.3801\n",
      "Epoch 9: Loss = 86.9357: Val_Loss = 87.2249\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "K = 100\n",
    "modelname = \"AdvBNN_champ_eps0.18\"\n",
    "# Train with SVI\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        loss  += svi.step(images, labels)\n",
    "    loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        val_loss += svi.evaluate_loss(images, labels)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "    if val_loss == min(val_losses):\n",
    "        print(\"New minimum loss was reached\")\n",
    "        # Sample k models from the posterior\n",
    "        sampled_models = [guide(None, None) for i in range(K)]\n",
    "        # Save the models\n",
    "        nn_dicts = []\n",
    "        for i in range(len(sampled_models)):\n",
    "            nn_dicts += [sampled_models[i].state_dict()]\n",
    "        torch.save(nn_dicts, \"models/%s_%s.pt\" % (modelname, val_loss))\n",
    "        print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdvFFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        x = self.logsoftmax(x)\n",
    "        return x\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fwd = torch.nn.Sequential(\n",
    "            torch.nn.Linear(ni, nh), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh, nh), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh, no),\n",
    "            torch.nn.LogSoftmax(dim=-1))\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.fwd(x)\n",
    "        return x\n",
    "class DeepNNwBN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(DeepNNwBN, self).__init__()\n",
    "        self.fwd = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(ni),\n",
    "            torch.nn.Linear(ni, nh), torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(nh),\n",
    "            torch.nn.Linear(nh, nh), torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(nh),\n",
    "            torch.nn.Linear(nh, no),\n",
    "            torch.nn.LogSoftmax(dim=-1))\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.fwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "epochs = 10\n",
    "modelname = \"AdvFFNN_eps0.18\"\n",
    "lr     = 0.01\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model = NN(28*28, 1024, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fcn  = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    model.eval()\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y    = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, total_loss,val_loss))\n",
    "    if val_loss == min(val_losses):\n",
    "        print(\"New minimum loss was reached, saving\")\n",
    "        torch.save(model, \"models/%s_loss%s.pt\" % (modelname, round(val_loss,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.00252582: Val_Loss = 0.00136129\n",
      "New minimum loss was reached, saving\n",
      "Epoch 1: Loss = 0.00119984: Val_Loss = 0.00107643\n",
      "New minimum loss was reached, saving\n",
      "Epoch 2: Loss = 0.000991049: Val_Loss = 0.00133119\n",
      "Epoch 3: Loss = 0.000886158: Val_Loss = 0.00126877\n",
      "Epoch 4: Loss = 0.000834433: Val_Loss = 0.00133933\n",
      "Epoch 5: Loss = 0.000769396: Val_Loss = 0.00140969\n",
      "Epoch 6: Loss = 0.000665719: Val_Loss = 0.00116027\n",
      "Epoch 7: Loss = 0.000706301: Val_Loss = 0.00117497\n",
      "Epoch 8: Loss = 0.000663628: Val_Loss = 0.00145015\n",
      "Epoch 9: Loss = 0.00060275: Val_Loss = 0.00118844\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "epochs = 10\n",
    "modelname = \"AdvDeepFFNN_eps0.18\"\n",
    "lr     = 0.01\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model = DeepNN(28*28, 1024, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fcn  = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    model.eval()\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y    = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, total_loss,val_loss))\n",
    "    if val_loss == min(val_losses):\n",
    "        print(\"New minimum loss was reached, saving\")\n",
    "        torch.save(model, \"models/%s_loss%s.pt\" % (modelname, round(val_loss,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.00236821: Val_Loss = 0.000956861\n",
      "New minimum loss was reached, saving\n",
      "Epoch 1: Loss = 0.000868838: Val_Loss = 0.000847592\n",
      "New minimum loss was reached, saving\n",
      "Epoch 2: Loss = 0.00056936: Val_Loss = 0.00109725\n",
      "Epoch 3: Loss = 0.000447485: Val_Loss = 0.000872235\n",
      "Epoch 4: Loss = 0.000438757: Val_Loss = 0.000729959\n",
      "New minimum loss was reached, saving\n",
      "Epoch 5: Loss = 0.000354677: Val_Loss = 0.000774602\n",
      "Epoch 6: Loss = 0.00031378: Val_Loss = 0.00121488\n",
      "Epoch 7: Loss = 0.000298046: Val_Loss = 0.000650184\n",
      "New minimum loss was reached, saving\n",
      "Epoch 8: Loss = 0.00027027: Val_Loss = 0.000768691\n",
      "Epoch 9: Loss = 0.000276499: Val_Loss = 0.000714513\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "epochs = 10\n",
    "modelname = \"AdvDeepFFNNwBN_eps0.18\"\n",
    "lr     = 0.01\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model = DeepNNwBN(28*28, 1024, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fcn  = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    model.eval()\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y    = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, total_loss,val_loss))\n",
    "    if val_loss == min(val_losses):\n",
    "        print(\"New minimum loss was reached, saving\")\n",
    "        torch.save(model, \"models/%s_loss%s.pt\" % (modelname, round(val_loss,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = -12779.9\n",
      "Epoch 1: Loss = -92655.6\n",
      "Epoch 2: Loss = -244018\n",
      "Epoch 3: Loss = -455806\n",
      "Epoch 4: Loss = -720546\n",
      "Epoch 5: Loss = -1.03274e+06\n",
      "Epoch 6: Loss = -1.3884e+06\n",
      "Epoch 7: Loss = -1.78493e+06\n",
      "Epoch 8: Loss = -2.22063e+06\n",
      "Epoch 9: Loss = -2.69407e+06\n",
      "Saved the model\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "epochs = 10\n",
    "K      = 100\n",
    "modelname = \"AdvFFNN_champ_eps0.18\"\n",
    "lr     = 0.01\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model = NN(28*28, 1024, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fcn  = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        y = model(images)\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g\" % (epoch, total_loss))\n",
    "\n",
    "torch.save(model, \"models/%s.pt\" % modelname)\n",
    "print(\"Saved the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Adversarial Test Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Adv Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\"mnist_adv_eps0.05/\") if \"test_images_med\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_eps0.05/\"+d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "test_dataset.data    = None\n",
    "test_dataset.targets = None\n",
    "\n",
    "test_dataset.data    = images\n",
    "test_dataset.targets = targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "test_loss = 0.\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    test_loss += svi.evaluate_loss(images, labels)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(\"Test_Loss = %g\" % (test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fooling_rate(sampled_models, adv_image, target):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass again on adv. example\n",
    "        pred = forward_pass(sampled_models[k], adv_image)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(target != pred)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True,\n",
    "                                         generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 sample models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models_BNN = load_models(K = 100,model_name=\"model.pt\")\n",
    "sampled_models_AdvBNN = load_models(K = 100,model_name=\"AdvBNN_85.0808.pt\")\n",
    "AdvFFNN = torch.load(\"models/AdvFFNN.pt\")\n",
    "FFNN    = torch.load(\"models/FFNN.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 500 / 13696\n",
      "AdvBNN: 0.3100600000000001, BNN 0.3393200000000002, AdvFFNN 0.014, FFNN 0.002\n",
      "\n",
      " Processing 1000 / 13696\n",
      "AdvBNN: 0.29595000000000016, BNN 0.3285899999999999, AdvFFNN 0.012, FFNN 0.005\n",
      "\n",
      " Processing 1500 / 13696\n",
      "AdvBNN: 0.30207999999999985, BNN 0.3344866666666663, AdvFFNN 0.013333333333333334, FFNN 0.006666666666666667\n",
      "\n",
      " Processing 2000 / 13696\n",
      "AdvBNN: 0.29584, BNN 0.3286549999999997, AdvFFNN 0.0115, FFNN 0.0055\n",
      "\n",
      " Processing 2500 / 13696\n",
      "AdvBNN: 0.2983879999999998, BNN 0.33291599999999977, AdvFFNN 0.012, FFNN 0.0064\n",
      "\n",
      " Processing 3000 / 13696\n",
      "AdvBNN: 0.2973466666666662, BNN 0.33291333333333295, AdvFFNN 0.010666666666666666, FFNN 0.007666666666666666\n",
      "\n",
      " Processing 3500 / 13696\n",
      "AdvBNN: 0.29775142857142745, BNN 0.33520857142857013, AdvFFNN 0.01, FFNN 0.0074285714285714285\n",
      "\n",
      " Processing 4000 / 13696\n",
      "AdvBNN: 0.29621499999999823, BNN 0.33281499999999764, AdvFFNN 0.00875, FFNN 0.00875\n",
      "\n",
      " Processing 4500 / 13696\n",
      "AdvBNN: 0.29555333333333045, BNN 0.33191999999999705, AdvFFNN 0.008444444444444444, FFNN 0.008888888888888889\n",
      "\n",
      " Processing 5000 / 13696\n",
      "AdvBNN: 0.29621399999999665, BNN 0.33260199999999646, AdvFFNN 0.0084, FFNN 0.0092\n",
      "\n",
      " Processing 5500 / 13696\n",
      "AdvBNN: 0.29601636363635964, BNN 0.33215090909090544, AdvFFNN 0.008545454545454545, FFNN 0.009272727272727273\n",
      "\n",
      " Processing 6000 / 13696\n",
      "AdvBNN: 0.29612833333332883, BNN 0.33179666666666274, AdvFFNN 0.008333333333333333, FFNN 0.0095\n",
      "\n",
      " Processing 6500 / 13696\n",
      "AdvBNN: 0.29700307692307176, BNN 0.3321230769230737, AdvFFNN 0.008615384615384615, FFNN 0.009230769230769232\n",
      "\n",
      " Processing 7000 / 13696\n",
      "AdvBNN: 0.2976485714285662, BNN 0.33246142857142524, AdvFFNN 0.009, FFNN 0.009\n",
      "\n",
      " Processing 7500 / 13696\n",
      "AdvBNN: 0.29711866666666364, BNN 0.331698666666664, AdvFFNN 0.009066666666666667, FFNN 0.0088\n",
      "\n",
      " Processing 8000 / 13696\n",
      "AdvBNN: 0.2972024999999984, BNN 0.33233999999999825, AdvFFNN 0.009, FFNN 0.00875\n",
      "\n",
      " Processing 8312 / 13696"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e4cffa70519b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfool_rates_BNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models_BNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfool_rates_AdvBNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models_AdvBNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfool_rates_AdvFFNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAdvFFNN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfool_rates_FFNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFFNN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-414fc882cb46>\u001b[0m in \u001b[0;36mfooling_rate\u001b[0;34m(sampled_models, adv_image, target)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Forward pass again on adv. example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# If we change the class, we fool the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfool\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-223be20b0df8>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, images, loss_target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwhich_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-cc38fccd73fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Two layer neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         for hook in itertools.chain(\n\u001b[0m\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 self._forward_hooks.values()):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "fool_rates_BNN     = []\n",
    "fool_rates_AdvBNN  = []\n",
    "fool_rates_AdvFFNN = []\n",
    "fool_rates_FFNN    = []\n",
    "counter = 1\n",
    "for images, labels in test_loader:\n",
    "    print(\"\\r Processing %s / %s\" % (counter,len(test_loader.dataset.targets)), end=\"\")\n",
    "    images = images.view(-1, 28*28)\n",
    "    fool_rates_BNN.append(fooling_rate(sampled_models_BNN,images,labels))\n",
    "    fool_rates_AdvBNN.append(fooling_rate(sampled_models_AdvBNN,images,labels))\n",
    "    fool_rates_AdvFFNN.append(fooling_rate([AdvFFNN],images,labels))\n",
    "    fool_rates_FFNN.append(fooling_rate([FFNN],images,labels))\n",
    "    if counter % 500 == 0 or counter == len(test_loader.dataset.targets):\n",
    "        bnn_rate      = sum(fool_rates_BNN)     / len(fool_rates_BNN)\n",
    "        advbnn_rate   = sum(fool_rates_AdvBNN)  / len(fool_rates_AdvBNN)\n",
    "        advffnn_rate  = sum(fool_rates_AdvFFNN) / len(fool_rates_AdvFFNN)\n",
    "        ffnn_rate     = sum(fool_rates_FFNN)    / len(fool_rates_FFNN)\n",
    "        print(\"\\nAdvBNN: %s, BNN %s, AdvFFNN %s, FFNN %s\\n\" % (advbnn_rate, bnn_rate, advffnn_rate, ffnn_rate))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champ Adv Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\"mnist_adv_eps0.05/\") if \"test_images_champ\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_eps0.05/\"+d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "test_dataset.data    = None\n",
    "test_dataset.targets = None\n",
    "\n",
    "test_dataset.data    = images\n",
    "test_dataset.targets = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True,\n",
    "                                         generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 1000 / 13696\n",
      "AdvBNN: 0.3002600000000002, BNN 0.29577000000000014, AdvFFNN 0.016, FFNN 0.024\n",
      "\n",
      " Processing 1053 / 13696"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e63eba774eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfool_rates_BNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models_BNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfool_rates_AdvBNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models_AdvBNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfool_rates_AdvFFNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAdvFFNN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfool_rates_FFNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfooling_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFFNN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-414fc882cb46>\u001b[0m in \u001b[0;36mfooling_rate\u001b[0;34m(sampled_models, adv_image, target)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Forward pass again on adv. example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# If we change the class, we fool the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfool\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-223be20b0df8>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, images, loss_target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwhich_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5a9baedaad42>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Two layer neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "fool_rates_BNN     = []\n",
    "fool_rates_AdvBNN  = []\n",
    "fool_rates_AdvFFNN = []\n",
    "fool_rates_FFNN    = []\n",
    "counter = 1\n",
    "for images, labels in test_loader:\n",
    "    print(\"\\r Processing %s / %s\" % (counter,len(test_loader.dataset.targets)), end=\"\")\n",
    "    images = images.view(-1, 28*28)\n",
    "    fool_rates_BNN.append(fooling_rate(sampled_models_BNN,images,labels))\n",
    "    fool_rates_AdvBNN.append(fooling_rate(sampled_models_AdvBNN,images,labels))\n",
    "    fool_rates_AdvFFNN.append(fooling_rate([AdvFFNN],images,labels))\n",
    "    fool_rates_FFNN.append(fooling_rate([FFNN],images,labels))\n",
    "    if counter % 1000 == 0 or counter == len(test_loader.dataset.targets):\n",
    "        bnn_rate      = sum(fool_rates_BNN)     / len(fool_rates_BNN)\n",
    "        advbnn_rate   = sum(fool_rates_AdvBNN)  / len(fool_rates_AdvBNN)\n",
    "        advffnn_rate  = sum(fool_rates_AdvFFNN) / len(fool_rates_AdvFFNN)\n",
    "        ffnn_rate     = sum(fool_rates_FFNN)    / len(fool_rates_FFNN)\n",
    "        print(\"\\nAdvBNN: %s, BNN %s, AdvFFNN %s, FFNN %s\\n\" % (advbnn_rate, bnn_rate, advffnn_rate, ffnn_rate))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
