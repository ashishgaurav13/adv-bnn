{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, pickle\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"models/model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency,images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros(28, 28)\n",
    "    combined_mean = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.flatten()\n",
    "    combined_mean = combined_mean.flatten()\n",
    "    champ         = saliencies[success.index(max(success))].flatten()\n",
    "    return combined_med, combined_mean, champ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN with Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\"mnist_adv/\") if d.startswith(\"train_images_med\")]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[51024,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exmp = next(iter(train_loader))[0][17,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_exmp.detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2328.9: Val_Loss = 822.611\n",
      "Epoch 1: Loss = 476.272: Val_Loss = 280.244\n",
      "Epoch 2: Loss = 198.731: Val_Loss = 148.165\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "K = 100\n",
    "modelname = \"AdvBNN.pt\"\n",
    "# Train with SVI\n",
    "for epoch in range(epochs):\n",
    "    loss = 0.\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        loss  += svi.step(images, labels)\n",
    "    loss /= len(train_loader.dataset)\n",
    "    # Evaluation\n",
    "    val_loss = 0.\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 28*28)\n",
    "        val_loss += svi.evaluate_loss(images, labels)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "# Sample k models from the posterior\n",
    "sampled_models = [guide(None, None) for i in range(K)]\n",
    "# Save the models\n",
    "nn_dicts = []\n",
    "for i in range(len(sampled_models)):\n",
    "    nn_dicts += [sampled_models[i].state_dict()]\n",
    "torch.save(nn_dicts, modelname)\n",
    "print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Adversarial Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\"mnist_adv/\") if \"test_images_med\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv/\"+d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "test_dataset.data    = None\n",
    "test_dataset.targets = None\n",
    "\n",
    "test_dataset.data    = images\n",
    "test_dataset.targets = targets\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                                         generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
