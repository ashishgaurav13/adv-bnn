{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()- (eps*saliency), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone() + (eps*saliency), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bim(image, eps, model, steps, alpha, targets):\n",
    "    with torch.no_grad():\n",
    "        image_adv = torch.clone(image)\n",
    "        image_adv.grad = None\n",
    "        image_adv.requires_grad = True\n",
    "\n",
    "    for n in range(steps):\n",
    "        image_adv.requires_grad = True\n",
    "        image_adv.grad = None\n",
    "        \n",
    "        output = model(image_adv)             #forward pass\n",
    "        output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "        model.zero_grad()\n",
    "        loss_fct = torch.nn.NLLLoss()\n",
    "        loss = loss_fct(output, targets)       #compute loss\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_grad =  alpha * torch.sign(image_adv.grad)\n",
    "            \n",
    "            adv_temp = image_adv.data + x_grad  #add perturbation to img_variable which also contains perturbation from previous iterations\n",
    "        \n",
    "            total_grad = torch.clamp(adv_temp - image, -eps, eps)\n",
    "            \n",
    "            #total_grad = torch.clamp(total_grad, -eps, eps)\n",
    "\n",
    "            image_adv.data = image + total_grad\n",
    "\n",
    "        #print(image_adv.mean())\n",
    "            #image_adv = image + total_grad             #altered image used in next pass\n",
    "    #return torch.clamp(image_adv,0,1)\n",
    "    \n",
    "    return image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new = bim(images, 0.18, sampled_models[1], 1, 0.2, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency, images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        \n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        #new_images = otcm(images, eps, saliency)\n",
    "        \n",
    "        # Fast Gradient Sign Method (FGSM)\n",
    "        #new_images = fgsm(images, eps, saliency)\n",
    "        \n",
    "        #Basic Iterative Method (BIM-untargeted)\n",
    "        new_images = bim(images, eps, sampled_models[k], 1, 0.2, targets)\n",
    "        \n",
    "        \n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27]\n"
     ]
    }
   ],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    \n",
    "    #Using OTCM:\n",
    "    #old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([8])])\n",
    "    \n",
    "    #Using FGSM or BIM:\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), targets])\n",
    "    \n",
    "    \n",
    "    # Compute adversarial examples\n",
    "    \n",
    "    #Using OTCM:\n",
    "    #new_images = otcm(images, EPS, images.grad.sign())\n",
    "    \n",
    "    #Using FGSM:\n",
    "    #new_images = fgsm(images, EPS, images.grad.sign())\n",
    "    \n",
    "    #Using BIM:\n",
    "    new_images = bim(images, EPS, sampled_models[k], 1, 0.2, targets)\n",
    "    \n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "\n",
    "    #print(images.grad.sign())\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "        saliencies += [images.grad.sign().view(28, 28)]\n",
    "print(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "newsaliency = torch.zeros(28, 28)\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        # choose median perturbation\n",
    "        newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "newsaliency = newsaliency.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3373737373737374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((newsaliency*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
