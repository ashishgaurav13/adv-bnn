{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 100     # for cifar100, output classes = 10\n",
    "inputs = 3       # for cifar100, color channels = 3\n",
    "modelname = 'alexnet-cifar100.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(root='.', train=True, download=True, transform=transform_cifar100)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBAlexNet(common.ModuleWrapper):\n",
    "    '''The architecture of AlexNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBAlexNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 64, 11, stride=4, padding=5, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(64, 192, 5, padding=2, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = BBBConv2d(192, 384, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = BBBConv2d(384, 256, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.conv5 = BBBConv2d(256, 128, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(1 * 1 * 128)\n",
    "        self.classifier = BBBLinear(1 * 1 * 128, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follows the procedure for sampling in the forward methods of BBBConv and \n",
    "# BBBLinear forward to create a fixed set of weights to use for the sampled model\n",
    "def sample_conv2d(bbb_layer):\n",
    "    conv_W_mu = bbb_layer.W_mu\n",
    "    conv_W_rho = bbb_layer.W_rho\n",
    "    conv_W_eps = torch.empty(conv_W_mu.size()).normal_(0,1).to(device)\n",
    "    conv_W_sigma = torch.log1p(torch.exp(conv_W_rho))\n",
    "    conv_weight = conv_W_mu + conv_W_eps * conv_W_sigma\n",
    "    if bbb_layer.use_bias:\n",
    "        conv_bias_mu = bbb_layer.bias_mu\n",
    "        conv_bias_rho = bbb_layer.bias_rho\n",
    "        conv_bias_eps = torch.empty(conv_bias_mu.size()).normal_(0,1).to(device)\n",
    "        conv_bias_sigma = torch.log1p(torch.exp(conv_bias_rho))\n",
    "        conv_bias = conv_bias_mu + conv_bias_eps * conv_bias_sigma\n",
    "    else:\n",
    "        conv_bias = None\n",
    "    return conv_weight.data, conv_bias.data\n",
    "\n",
    "def sample_linear(bbb_layer):\n",
    "        fc_W_mu = bbb_layer.W_mu\n",
    "        fc_W_rho = bbb_layer.W_rho\n",
    "        fc_W_eps = torch.empty(fc_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc_W_sigma = torch.log1p(torch.exp(fc_W_rho))\n",
    "        fc_weight = fc_W_mu + fc_W_eps * fc_W_sigma\n",
    "        if bbb_layer.use_bias:\n",
    "            fc_bias_mu = bbb_layer.bias_mu\n",
    "            fc_bias_rho = bbb_layer.bias_rho\n",
    "            fc_bias_eps = torch.empty(fc_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc_bias_sigma = torch.log1p(torch.exp(fc_bias_rho))\n",
    "            fc_bias = fc_bias_mu + fc_bias_eps * fc_bias_sigma\n",
    "        else:\n",
    "            fc_bias = None\n",
    "        \n",
    "        return fc_weight.data, fc_bias.data\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base AlexNet model that matches the architecture of BayesianAlexNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBAlexNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 64, 11, stride=4, padding=5, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 192, 5, padding=2, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(192,384,3, padding=1, bias=True)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384,256,3, padding=1, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 128, 3, padding=1, bias=True)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # call x.view explicitly in forward\n",
    "        self.classifier = nn.Linear(1 * 1 * 128, outputs, bias=True)\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=bbbnet.pool3.kernel_size, stride=bbbnet.pool3.stride)\n",
    "\n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(bbbnet.conv3.in_channels, bbbnet.conv3.out_channels, bbbnet.conv3.kernel_size,\n",
    "                                stride=bbbnet.conv3.stride, padding=bbbnet.conv3.padding, dilation=bbbnet.conv3.dilation,\n",
    "                                groups=bbbnet.conv3.groups)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(bbbnet.conv4.in_channels, bbbnet.conv4.out_channels, bbbnet.conv4.kernel_size,\n",
    "                        stride=bbbnet.conv4.stride, padding=bbbnet.conv4.padding, dilation=bbbnet.conv4.dilation,\n",
    "                        groups=bbbnet.conv4.groups)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(bbbnet.conv5.in_channels, bbbnet.conv5.out_channels, bbbnet.conv5.kernel_size,\n",
    "                        stride=bbbnet.conv5.stride, padding=bbbnet.conv5.padding, dilation=bbbnet.conv5.dilation,\n",
    "                        groups=bbbnet.conv5.groups)\n",
    "\n",
    "        # Sample convolutional layers\n",
    "        self.conv1.weight.data, self.conv1.bias.data = sample_conv2d(bbbnet.conv1)\n",
    "        self.conv2.weight.data, self.conv2.bias.data = sample_conv2d(bbbnet.conv2)\n",
    "        self.conv3.weight.data, self.conv3.bias.data = sample_conv2d(bbbnet.conv3)\n",
    "        self.conv4.weight.data, self.conv4.bias.data = sample_conv2d(bbbnet.conv4)\n",
    "        self.conv5.weight.data, self.conv5.bias.data = sample_conv2d(bbbnet.conv5)\n",
    "\n",
    "        ### Create Linear Layers\n",
    "        self.classifier = nn.Linear(bbbnet.classifier.in_features, bbbnet.classifier.out_features, bbbnet.classifier.use_bias)\n",
    "        self.classifier.weight.data, self.classifier.bias.data = sample_linear(bbbnet.classifier)            \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(-1, 1 * 1 * 128)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBAlexNet(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = AlexNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(modelname, K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [AlexNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(modelname)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 100, K = 100, modelname = modelname)\n",
    "sampled_models = load_models(modelname, K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR100(root='.', train=False, download=True, transform=transform_cifar100)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.1\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "unbatched_shape = saliencies.shape[1:]\n",
    "print(unbatched_shape)\n",
    "newsaliency = torch.zeros(unbatched_shape)\n",
    "\n",
    "for i in range(unbatched_shape[0]):\n",
    "    for j in range(unbatched_shape[1]):\n",
    "        for k in range(unbatched_shape[2]):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j, k] = np.percentile(saliencies[:, i, j, k].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157291666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "images = images.squeeze() # squeeze out batch dimension\n",
    "image_transpose = np.transpose( images.detach().numpy() , (1,2,0))  \n",
    "plt.imshow(image_transpose)\n",
    "plt.subplot(1, 3, 2)\n",
    "perturbation = newsaliency * EPS\n",
    "perturbation_transpose = np.transpose(perturbation.detach().numpy(), (1,2,0))\n",
    "plt.imshow(perturbation_transpose, vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "new_images = new_images.squeeze()\n",
    "print(torch.max(new_images))\n",
    "new_images_transpose = np.transpose(new_images.detach().numpy(), (1,2,0))\n",
    "plt.imshow(new_images_transpose)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
