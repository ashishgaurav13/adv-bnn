{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os,sys\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "\n",
    "import re, pickle\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform_mnist)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by \n",
    "        sampling the BBBLeNet model.\n",
    "        '''    \n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        \n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "        \n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and \n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "        \n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=n_epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load('models/model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train without Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.027148\n",
      "Epoch 1: Loss = 0.0240098\n",
      "Epoch 2: Loss = 0.0233979\n",
      "Epoch 3: Loss = 0.0233313\n",
      "Epoch 4: Loss = 0.0232598\n",
      "Epoch 5: Loss = 0.0231919\n",
      "Epoch 6: Loss = 0.0232035\n",
      "Epoch 7: Loss = 0.0231663\n",
      "Epoch 8: Loss = 0.0231402\n",
      "Epoch 9: Loss = 0.0231431\n",
      "Epoch 10: Loss = 0.0231321\n",
      "Epoch 11: Loss = 0.0231338\n",
      "Epoch 12: Loss = 0.0231039\n",
      "Epoch 13: Loss = 0.0231189\n",
      "Epoch 14: Loss = 0.023094\n",
      "Epoch 15: Loss = 0.0231079\n",
      "Epoch 16: Loss = 0.0231121\n",
      "Epoch 17: Loss = 0.0230984\n",
      "Epoch 18: Loss = 0.0230931\n",
      "Epoch 19: Loss = 0.0230665\n",
      "Saved the model\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "epochs = 20\n",
    "modelname = \"CNN_v2\"\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model      = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# lr_sched   = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "loss_fcn   = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "softmax    = torch.nn.Softmax()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        y    = softmax(model(images))\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    # lr_sched.step()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g\" % (epoch, total_loss))\n",
    "\n",
    "torch.save(model, \"models/%s.pt\" % modelname)\n",
    "print(\"Saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.05/\") if d.startswith(\"train_images_med\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.05//\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:8949.661, TrainAcc:0.973, ValLoss:8815.307, ValAcc:0.975, KL:35952.331\n",
      "Epoch:1, TrainLoss:8331.902, TrainAcc:0.975, ValLoss:8980.602, ValAcc:0.973, KL:35115.529\n",
      "Epoch:2, TrainLoss:7778.730, TrainAcc:0.978, ValLoss:8261.433, ValAcc:0.977, KL:34635.471\n",
      "Epoch:3, TrainLoss:7666.301, TrainAcc:0.978, ValLoss:8226.040, ValAcc:0.975, KL:34100.247\n",
      "Epoch:4, TrainLoss:7431.978, TrainAcc:0.979, ValLoss:8325.451, ValAcc:0.976, KL:33713.422\n",
      "Epoch:5, TrainLoss:7143.370, TrainAcc:0.981, ValLoss:8008.452, ValAcc:0.976, KL:33158.879\n",
      "Epoch:6, TrainLoss:6964.101, TrainAcc:0.980, ValLoss:8216.730, ValAcc:0.975, KL:32673.102\n",
      "Epoch:7, TrainLoss:6826.943, TrainAcc:0.982, ValLoss:8213.214, ValAcc:0.975, KL:32222.349\n",
      "Epoch:8, TrainLoss:6733.833, TrainAcc:0.982, ValLoss:7880.064, ValAcc:0.977, KL:31912.593\n",
      "Epoch:9, TrainLoss:6353.305, TrainAcc:0.983, ValLoss:7803.279, ValAcc:0.980, KL:31599.982\n",
      "Epoch:10, TrainLoss:6362.029, TrainAcc:0.983, ValLoss:7619.875, ValAcc:0.979, KL:31359.545\n",
      "Epoch:11, TrainLoss:6460.006, TrainAcc:0.984, ValLoss:7891.625, ValAcc:0.979, KL:31118.374\n",
      "Epoch:12, TrainLoss:6209.604, TrainAcc:0.984, ValLoss:7932.998, ValAcc:0.980, KL:31090.918\n",
      "Epoch:13, TrainLoss:6209.339, TrainAcc:0.984, ValLoss:7619.793, ValAcc:0.981, KL:30886.282\n",
      "Epoch:14, TrainLoss:6010.746, TrainAcc:0.985, ValLoss:7381.291, ValAcc:0.979, KL:30659.194\n",
      "Epoch:15, TrainLoss:6052.156, TrainAcc:0.985, ValLoss:7377.170, ValAcc:0.980, KL:30414.547\n",
      "Epoch:16, TrainLoss:5863.056, TrainAcc:0.986, ValLoss:7180.469, ValAcc:0.982, KL:30154.216\n",
      "Epoch:17, TrainLoss:5834.112, TrainAcc:0.985, ValLoss:7690.705, ValAcc:0.979, KL:29989.518\n",
      "Epoch:18, TrainLoss:5740.364, TrainAcc:0.986, ValLoss:7222.262, ValAcc:0.982, KL:30025.836\n",
      "Epoch:19, TrainLoss:5629.161, TrainAcc:0.986, ValLoss:7364.658, ValAcc:0.981, KL:29862.409\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn-adv-med0.05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training Champ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.05/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.05/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:5748.950, TrainAcc:0.985, ValLoss:7297.304, ValAcc:0.978, KL:28318.823\n",
      "Epoch:1, TrainLoss:5604.724, TrainAcc:0.986, ValLoss:6953.296, ValAcc:0.982, KL:28425.381\n",
      "Epoch:2, TrainLoss:5523.580, TrainAcc:0.986, ValLoss:7027.331, ValAcc:0.980, KL:28654.955\n",
      "Epoch:3, TrainLoss:5468.697, TrainAcc:0.987, ValLoss:6825.225, ValAcc:0.982, KL:28711.553\n",
      "Epoch:4, TrainLoss:5469.264, TrainAcc:0.987, ValLoss:7226.810, ValAcc:0.982, KL:28803.916\n",
      "Epoch:5, TrainLoss:5562.123, TrainAcc:0.986, ValLoss:6808.629, ValAcc:0.981, KL:29043.431\n",
      "Epoch:6, TrainLoss:5333.678, TrainAcc:0.988, ValLoss:6939.312, ValAcc:0.982, KL:29171.288\n",
      "Epoch:7, TrainLoss:5457.837, TrainAcc:0.987, ValLoss:7125.187, ValAcc:0.981, KL:29041.696\n",
      "Epoch:8, TrainLoss:5298.181, TrainAcc:0.988, ValLoss:7094.158, ValAcc:0.981, KL:28945.895\n",
      "Epoch:9, TrainLoss:5208.028, TrainAcc:0.988, ValLoss:7566.662, ValAcc:0.981, KL:28774.984\n",
      "Epoch:10, TrainLoss:5101.284, TrainAcc:0.988, ValLoss:6604.559, ValAcc:0.982, KL:28617.199\n",
      "Epoch:11, TrainLoss:5127.127, TrainAcc:0.988, ValLoss:6991.694, ValAcc:0.982, KL:28627.920\n",
      "Epoch:12, TrainLoss:4991.676, TrainAcc:0.989, ValLoss:7022.069, ValAcc:0.983, KL:28430.669\n",
      "Epoch:13, TrainLoss:5131.023, TrainAcc:0.989, ValLoss:7135.683, ValAcc:0.982, KL:28430.843\n",
      "Epoch:14, TrainLoss:5060.428, TrainAcc:0.989, ValLoss:6791.852, ValAcc:0.984, KL:28366.075\n",
      "Epoch:15, TrainLoss:4952.064, TrainAcc:0.989, ValLoss:6901.688, ValAcc:0.983, KL:28299.291\n",
      "Epoch:16, TrainLoss:4930.176, TrainAcc:0.989, ValLoss:6891.714, ValAcc:0.983, KL:28138.205\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:17, TrainLoss:4892.830, TrainAcc:0.989, ValLoss:6610.577, ValAcc:0.984, KL:28137.388\n",
      "Epoch:18, TrainLoss:4568.035, TrainAcc:0.991, ValLoss:6671.523, ValAcc:0.983, KL:28074.439\n",
      "Epoch:19, TrainLoss:4529.751, TrainAcc:0.991, ValLoss:6699.654, ValAcc:0.985, KL:27905.277\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn-adv-champ0.05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.05/\") if d.startswith(\"train_images_mean\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.05/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn-adv-mean0.05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.18/\") if d.startswith(\"train_images_med\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.18/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:6328.549, TrainAcc:0.982, ValLoss:6058.759, ValAcc:0.983, KL:28087.034\n",
      "Epoch:1, TrainLoss:6107.050, TrainAcc:0.983, ValLoss:6719.151, ValAcc:0.980, KL:28581.676\n",
      "Epoch:2, TrainLoss:5874.656, TrainAcc:0.985, ValLoss:6654.492, ValAcc:0.980, KL:28847.441\n",
      "Epoch:3, TrainLoss:5688.220, TrainAcc:0.986, ValLoss:6139.992, ValAcc:0.982, KL:29071.327\n",
      "Epoch:4, TrainLoss:5593.068, TrainAcc:0.986, ValLoss:6237.446, ValAcc:0.982, KL:29128.288\n",
      "Epoch:5, TrainLoss:5347.973, TrainAcc:0.988, ValLoss:6199.795, ValAcc:0.984, KL:28983.227\n",
      "Epoch:6, TrainLoss:5237.087, TrainAcc:0.988, ValLoss:6407.690, ValAcc:0.983, KL:28864.655\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:7, TrainLoss:5344.323, TrainAcc:0.987, ValLoss:6852.710, ValAcc:0.982, KL:28852.665\n",
      "Epoch:8, TrainLoss:4919.120, TrainAcc:0.989, ValLoss:5830.979, ValAcc:0.985, KL:28871.863\n",
      "Epoch:9, TrainLoss:4996.001, TrainAcc:0.989, ValLoss:5803.739, ValAcc:0.985, KL:28731.249\n",
      "Epoch:10, TrainLoss:4837.497, TrainAcc:0.990, ValLoss:6024.384, ValAcc:0.984, KL:28611.868\n",
      "Epoch:11, TrainLoss:4694.949, TrainAcc:0.990, ValLoss:5649.666, ValAcc:0.985, KL:28469.260\n",
      "Epoch:12, TrainLoss:4801.222, TrainAcc:0.990, ValLoss:5768.590, ValAcc:0.985, KL:28344.251\n",
      "Epoch:13, TrainLoss:4729.761, TrainAcc:0.990, ValLoss:5757.487, ValAcc:0.984, KL:28213.859\n",
      "Epoch:14, TrainLoss:4639.682, TrainAcc:0.990, ValLoss:6005.585, ValAcc:0.985, KL:28102.292\n",
      "Epoch:15, TrainLoss:4601.280, TrainAcc:0.991, ValLoss:5672.935, ValAcc:0.986, KL:27979.231\n",
      "Epoch:16, TrainLoss:4719.523, TrainAcc:0.990, ValLoss:6162.788, ValAcc:0.984, KL:27888.139\n",
      "Epoch:17, TrainLoss:4502.378, TrainAcc:0.991, ValLoss:5498.535, ValAcc:0.986, KL:27789.447\n",
      "Epoch:18, TrainLoss:4662.367, TrainAcc:0.990, ValLoss:5686.018, ValAcc:0.986, KL:27695.607\n",
      "Epoch:19, TrainLoss:4589.298, TrainAcc:0.990, ValLoss:5620.519, ValAcc:0.987, KL:27622.799\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-med0.18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.18, champ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.18/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.18/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:5160.353, TrainAcc:0.987, ValLoss:6292.461, ValAcc:0.982, KL:27207.201\n",
      "Epoch:1, TrainLoss:5070.413, TrainAcc:0.988, ValLoss:6765.373, ValAcc:0.981, KL:27369.277\n",
      "Epoch:2, TrainLoss:5380.323, TrainAcc:0.987, ValLoss:6474.751, ValAcc:0.984, KL:27846.429\n",
      "Epoch:3, TrainLoss:5080.209, TrainAcc:0.988, ValLoss:6512.425, ValAcc:0.983, KL:28110.186\n",
      "Epoch:4, TrainLoss:5079.756, TrainAcc:0.988, ValLoss:6503.413, ValAcc:0.982, KL:28056.009\n",
      "Epoch:5, TrainLoss:5125.309, TrainAcc:0.988, ValLoss:6360.784, ValAcc:0.983, KL:28119.157\n",
      "Epoch:6, TrainLoss:4976.825, TrainAcc:0.989, ValLoss:6396.181, ValAcc:0.983, KL:28137.779\n",
      "Epoch:7, TrainLoss:4958.926, TrainAcc:0.989, ValLoss:5970.411, ValAcc:0.984, KL:28019.700\n",
      "Epoch:8, TrainLoss:4828.485, TrainAcc:0.989, ValLoss:7112.312, ValAcc:0.982, KL:27940.775\n",
      "Epoch:9, TrainLoss:4892.673, TrainAcc:0.989, ValLoss:6161.597, ValAcc:0.985, KL:27869.739\n",
      "Epoch:10, TrainLoss:4908.512, TrainAcc:0.989, ValLoss:6152.966, ValAcc:0.985, KL:27877.910\n",
      "Epoch:11, TrainLoss:4800.579, TrainAcc:0.989, ValLoss:6350.641, ValAcc:0.984, KL:27829.217\n",
      "Epoch:12, TrainLoss:4879.934, TrainAcc:0.989, ValLoss:6092.205, ValAcc:0.984, KL:27851.558\n",
      "Epoch:13, TrainLoss:4812.423, TrainAcc:0.990, ValLoss:6182.821, ValAcc:0.984, KL:27876.379\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:14, TrainLoss:4837.644, TrainAcc:0.989, ValLoss:6446.608, ValAcc:0.983, KL:27734.907\n",
      "Epoch:15, TrainLoss:4491.932, TrainAcc:0.991, ValLoss:5714.341, ValAcc:0.986, KL:27660.305\n",
      "Epoch:16, TrainLoss:4419.275, TrainAcc:0.991, ValLoss:5804.200, ValAcc:0.986, KL:27538.684\n",
      "Epoch:17, TrainLoss:4411.149, TrainAcc:0.991, ValLoss:5859.347, ValAcc:0.986, KL:27408.838\n",
      "Epoch:18, TrainLoss:4434.558, TrainAcc:0.991, ValLoss:5629.992, ValAcc:0.987, KL:27295.888\n",
      "Epoch:19, TrainLoss:4322.917, TrainAcc:0.992, ValLoss:5877.044, ValAcc:0.986, KL:27184.804\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-champ0.18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.18, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.18/\") if d.startswith(\"train_images_mean\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.18/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:6988.521, TrainAcc:0.978, ValLoss:6624.928, ValAcc:0.980, KL:26125.820\n",
      "Epoch:1, TrainLoss:6046.010, TrainAcc:0.983, ValLoss:6339.429, ValAcc:0.981, KL:26684.099\n",
      "Epoch:2, TrainLoss:5653.364, TrainAcc:0.985, ValLoss:6168.116, ValAcc:0.982, KL:27106.052\n",
      "Epoch:3, TrainLoss:5598.903, TrainAcc:0.985, ValLoss:6065.987, ValAcc:0.982, KL:27328.547\n",
      "Epoch:4, TrainLoss:5461.733, TrainAcc:0.986, ValLoss:5892.176, ValAcc:0.984, KL:27465.236\n",
      "Epoch:5, TrainLoss:5351.436, TrainAcc:0.987, ValLoss:5600.215, ValAcc:0.985, KL:27498.719\n",
      "Epoch:6, TrainLoss:5137.735, TrainAcc:0.988, ValLoss:5911.918, ValAcc:0.983, KL:27595.175\n",
      "Epoch:7, TrainLoss:5082.059, TrainAcc:0.988, ValLoss:5602.695, ValAcc:0.985, KL:27467.322\n",
      "Epoch:8, TrainLoss:5074.883, TrainAcc:0.988, ValLoss:6210.474, ValAcc:0.983, KL:27352.501\n",
      "Epoch:9, TrainLoss:4868.651, TrainAcc:0.989, ValLoss:5919.469, ValAcc:0.985, KL:27317.718\n",
      "Epoch:10, TrainLoss:4790.870, TrainAcc:0.989, ValLoss:5487.045, ValAcc:0.985, KL:27164.760\n",
      "Epoch:11, TrainLoss:4763.591, TrainAcc:0.989, ValLoss:6215.794, ValAcc:0.984, KL:27128.703\n",
      "Epoch:12, TrainLoss:4811.595, TrainAcc:0.989, ValLoss:5560.214, ValAcc:0.985, KL:27034.550\n",
      "Epoch:13, TrainLoss:4627.880, TrainAcc:0.990, ValLoss:5848.407, ValAcc:0.986, KL:26831.784\n",
      "Epoch:14, TrainLoss:4622.190, TrainAcc:0.990, ValLoss:5617.237, ValAcc:0.986, KL:26648.468\n",
      "Epoch:15, TrainLoss:4536.098, TrainAcc:0.989, ValLoss:5810.366, ValAcc:0.986, KL:26433.632\n",
      "Epoch:16, TrainLoss:4551.006, TrainAcc:0.990, ValLoss:6029.175, ValAcc:0.984, KL:26387.878\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:17, TrainLoss:4539.874, TrainAcc:0.990, ValLoss:5946.150, ValAcc:0.984, KL:26221.814\n",
      "Epoch:18, TrainLoss:4298.322, TrainAcc:0.991, ValLoss:5282.947, ValAcc:0.988, KL:26150.587\n",
      "Epoch:19, TrainLoss:4207.761, TrainAcc:0.991, ValLoss:5591.219, ValAcc:0.985, KL:26022.084\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-mean0.18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.50/\") if d.startswith(\"train_images_med\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.50/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:675825.725, TrainAcc:0.457, ValLoss:391848.985, ValAcc:0.795, KL:5854286.303\n",
      "Epoch:1, TrainLoss:299571.643, TrainAcc:0.855, ValLoss:228370.298, ValAcc:0.899, KL:2726808.864\n",
      "Epoch:2, TrainLoss:182268.922, TrainAcc:0.924, ValLoss:145837.886, ValAcc:0.939, KL:1674113.636\n",
      "Epoch:3, TrainLoss:121519.875, TrainAcc:0.943, ValLoss:99517.553, ValAcc:0.953, KL:1101698.697\n",
      "Epoch:4, TrainLoss:85786.966, TrainAcc:0.951, ValLoss:73239.719, ValAcc:0.952, KL:760531.627\n",
      "Epoch:5, TrainLoss:63224.721, TrainAcc:0.955, ValLoss:54725.575, ValAcc:0.955, KL:543066.877\n",
      "Epoch:6, TrainLoss:47765.225, TrainAcc:0.958, ValLoss:42697.884, ValAcc:0.958, KL:396992.910\n",
      "Epoch:7, TrainLoss:37334.873, TrainAcc:0.961, ValLoss:33615.304, ValAcc:0.960, KL:296689.622\n",
      "Epoch:8, TrainLoss:29801.570, TrainAcc:0.964, ValLoss:27063.225, ValAcc:0.963, KL:225528.364\n",
      "Epoch:9, TrainLoss:24390.927, TrainAcc:0.965, ValLoss:22116.514, ValAcc:0.967, KL:174635.017\n",
      "Epoch:10, TrainLoss:20403.011, TrainAcc:0.967, ValLoss:19217.222, ValAcc:0.965, KL:137497.293\n",
      "Epoch:11, TrainLoss:17607.871, TrainAcc:0.967, ValLoss:17096.116, ValAcc:0.965, KL:110694.699\n",
      "Epoch:12, TrainLoss:15215.655, TrainAcc:0.970, ValLoss:15123.416, ValAcc:0.966, KL:90803.284\n",
      "Epoch:13, TrainLoss:13794.768, TrainAcc:0.970, ValLoss:14348.424, ValAcc:0.964, KL:76325.990\n",
      "Epoch:14, TrainLoss:12486.118, TrainAcc:0.970, ValLoss:13018.270, ValAcc:0.969, KL:66099.607\n",
      "Epoch:15, TrainLoss:11630.369, TrainAcc:0.972, ValLoss:12171.377, ValAcc:0.967, KL:58449.831\n",
      "Epoch:16, TrainLoss:10657.257, TrainAcc:0.974, ValLoss:11689.882, ValAcc:0.968, KL:52847.584\n",
      "Epoch:17, TrainLoss:10014.053, TrainAcc:0.974, ValLoss:11309.218, ValAcc:0.967, KL:48770.842\n",
      "Epoch:18, TrainLoss:9891.375, TrainAcc:0.974, ValLoss:10629.726, ValAcc:0.971, KL:45937.188\n",
      "Epoch:19, TrainLoss:9420.207, TrainAcc:0.976, ValLoss:11250.221, ValAcc:0.968, KL:43678.183\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-med0.50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.50, champ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.50/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.50/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:11447.010, TrainAcc:0.964, ValLoss:12110.118, ValAcc:0.960, KL:41448.903\n",
      "Epoch:1, TrainLoss:10468.662, TrainAcc:0.968, ValLoss:11019.912, ValAcc:0.966, KL:41057.046\n",
      "Epoch:2, TrainLoss:10102.776, TrainAcc:0.970, ValLoss:11801.397, ValAcc:0.965, KL:40567.095\n",
      "Epoch:3, TrainLoss:9841.269, TrainAcc:0.971, ValLoss:11380.596, ValAcc:0.966, KL:39981.229\n",
      "Epoch:4, TrainLoss:9370.015, TrainAcc:0.973, ValLoss:10870.233, ValAcc:0.968, KL:39506.889\n",
      "Epoch:5, TrainLoss:9230.336, TrainAcc:0.973, ValLoss:11161.190, ValAcc:0.965, KL:39060.530\n",
      "Epoch:6, TrainLoss:9182.682, TrainAcc:0.974, ValLoss:10442.827, ValAcc:0.971, KL:38616.062\n",
      "Epoch:7, TrainLoss:8928.359, TrainAcc:0.975, ValLoss:10332.060, ValAcc:0.971, KL:38268.065\n",
      "Epoch:8, TrainLoss:8604.883, TrainAcc:0.976, ValLoss:10822.201, ValAcc:0.966, KL:37845.888\n",
      "Epoch:9, TrainLoss:8527.583, TrainAcc:0.976, ValLoss:10335.241, ValAcc:0.970, KL:37333.202\n",
      "Epoch:10, TrainLoss:8416.549, TrainAcc:0.977, ValLoss:10067.828, ValAcc:0.973, KL:37064.518\n",
      "Epoch:11, TrainLoss:8340.633, TrainAcc:0.977, ValLoss:10421.281, ValAcc:0.969, KL:36763.116\n",
      "Epoch:12, TrainLoss:8234.467, TrainAcc:0.977, ValLoss:9870.592, ValAcc:0.970, KL:36513.398\n",
      "Epoch:13, TrainLoss:8063.946, TrainAcc:0.978, ValLoss:10641.822, ValAcc:0.969, KL:36224.334\n",
      "Epoch:14, TrainLoss:7891.395, TrainAcc:0.978, ValLoss:9872.094, ValAcc:0.971, KL:35846.176\n",
      "Epoch:15, TrainLoss:7842.192, TrainAcc:0.979, ValLoss:10094.554, ValAcc:0.970, KL:35688.165\n",
      "Epoch:16, TrainLoss:7859.030, TrainAcc:0.979, ValLoss:9845.285, ValAcc:0.974, KL:35749.657\n",
      "Epoch:17, TrainLoss:7611.191, TrainAcc:0.980, ValLoss:9958.098, ValAcc:0.973, KL:35431.840\n",
      "Epoch:18, TrainLoss:7771.407, TrainAcc:0.979, ValLoss:9868.910, ValAcc:0.971, KL:35252.504\n",
      "Epoch:19, TrainLoss:7498.261, TrainAcc:0.980, ValLoss:9533.185, ValAcc:0.973, KL:35270.972\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-champ0.50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.50, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.50/\") if d.startswith(\"train_images_mean\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.50/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:7816.244, TrainAcc:0.978, ValLoss:8978.572, ValAcc:0.975, KL:34282.621\n",
      "Epoch:1, TrainLoss:7175.247, TrainAcc:0.981, ValLoss:8651.983, ValAcc:0.975, KL:34162.568\n",
      "Epoch:2, TrainLoss:6896.807, TrainAcc:0.982, ValLoss:8236.008, ValAcc:0.978, KL:33810.304\n",
      "Epoch:3, TrainLoss:6808.142, TrainAcc:0.982, ValLoss:8411.853, ValAcc:0.977, KL:33527.100\n",
      "Epoch:4, TrainLoss:6552.725, TrainAcc:0.984, ValLoss:8473.825, ValAcc:0.977, KL:33169.438\n",
      "Epoch:5, TrainLoss:6624.577, TrainAcc:0.983, ValLoss:8340.197, ValAcc:0.977, KL:32946.340\n",
      "Epoch:6, TrainLoss:6496.584, TrainAcc:0.984, ValLoss:8526.166, ValAcc:0.978, KL:32697.437\n",
      "Epoch:7, TrainLoss:6362.987, TrainAcc:0.984, ValLoss:8186.024, ValAcc:0.979, KL:32575.957\n",
      "Epoch:8, TrainLoss:6394.714, TrainAcc:0.984, ValLoss:7991.355, ValAcc:0.979, KL:32498.601\n",
      "Epoch:9, TrainLoss:6245.356, TrainAcc:0.985, ValLoss:7960.434, ValAcc:0.978, KL:32414.938\n",
      "Epoch:10, TrainLoss:6341.532, TrainAcc:0.984, ValLoss:7843.233, ValAcc:0.979, KL:32178.055\n",
      "Epoch:11, TrainLoss:6095.167, TrainAcc:0.985, ValLoss:7943.919, ValAcc:0.979, KL:32204.681\n",
      "Epoch:12, TrainLoss:6086.969, TrainAcc:0.985, ValLoss:8034.670, ValAcc:0.978, KL:31920.936\n",
      "Epoch:13, TrainLoss:6037.735, TrainAcc:0.985, ValLoss:8148.796, ValAcc:0.979, KL:31759.244\n",
      "Epoch:14, TrainLoss:6108.361, TrainAcc:0.985, ValLoss:8051.015, ValAcc:0.978, KL:31629.445\n",
      "Epoch:15, TrainLoss:6025.054, TrainAcc:0.986, ValLoss:7590.977, ValAcc:0.980, KL:31703.189\n",
      "Epoch:16, TrainLoss:5974.817, TrainAcc:0.986, ValLoss:8013.912, ValAcc:0.978, KL:31533.872\n",
      "Epoch:17, TrainLoss:5910.154, TrainAcc:0.986, ValLoss:7827.307, ValAcc:0.978, KL:31490.563\n",
      "Epoch:18, TrainLoss:5865.651, TrainAcc:0.986, ValLoss:8342.492, ValAcc:0.977, KL:31383.619\n",
      "Epoch:19, TrainLoss:5894.874, TrainAcc:0.986, ValLoss:8354.401, ValAcc:0.978, KL:31166.413\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-mean0.50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "import re, pickle\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "common.set_seed(1)\n",
    "\n",
    "# Settings\n",
    "HOME_DIR = \"/home/fcbeylun/adv-bnn/\"\n",
    "HOME_DIR = \"./\"\n",
    "EPS  = 0.05\n",
    "COMB = \"champ\"\n",
    "\n",
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configuration parameters\n",
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1\n",
    "\n",
    "net_type = 'lenet'   # (lenet/alexnet)\n",
    "dataset  = 'mnist'    # (mnist/cifar10/cifar100)\n",
    "\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 1\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.MNIST(root=HOME_DIR, train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.CIFAR10(root=HOME_DIR, train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar100':\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 100\n",
    "    trainset = torchvision.datasets.CIFAR100(root=HOME_DIR, train=True, download=True, transform=transform)\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported dataset\")\n",
    "\n",
    "\n",
    "if net_type == 'lenet':\n",
    "    net_class = common.non_bayesian_models.LeNet\n",
    "    bbb_model = common.bayesian_models.BBBLeNet\n",
    "elif net_type == 'alexnet':\n",
    "    net_class = common.non_bayesian_models.AlexNet\n",
    "    bbb_model = common.bayesian_models.BBBAlexNet\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported network type\")\n",
    "\n",
    "\n",
    "net = bbb_model(outputs, inputs, priors, layer_type, activation_type).to(device)\n",
    "\n",
    "\n",
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\", force_train=False):\n",
    "    if os.path.exists(modelname) and not force_train:\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader,\n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens,\n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = net_class(outputs, inputs, layer_type, activation_type).to(device)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = trainset\n",
    "\n",
    "dirs    = [d for d in os.listdir(HOME_DIR + \"mnist_adv_CNN_itcm_%s/\" % EPS) if d.startswith(\"train_images_%s\" % COMB)]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))\n",
    "\n",
    "transform_back = transforms.Compose([transforms.Resize((28,28))])\n",
    "\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(HOME_DIR + \"mnist_adv_CNN_itcm_%s/\" % EPS + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "\n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "print(images.shape)\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(sum([t.detach().tolist() for t in targets],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
