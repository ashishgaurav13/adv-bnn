{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, sys, pickle\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "train = True\n",
    "HOME_DIR = \"/home/fcbeylun/adv-bnn/\"\n",
    "HOME_DIR = \"./\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "\n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly\n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "\n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by\n",
    "        sampling the BBBLeNet model.\n",
    "        '''\n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "\n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "\n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and\n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "\n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def load_models(K = 50):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type).to(device) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(HOME_DIR + '/models/model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 sample models\n"
     ]
    }
   ],
   "source": [
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type).to(device)\n",
    "sampled_models = load_models(K = 50)\n",
    "sampled_models = [m.to(device) for m in sampled_models]\n",
    "softplus = torch.nn.Softplus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(HOME_DIR, train=True, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False,\n",
    "                                          generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(HOME_DIR, train=False, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                                          generator=torch.Generator().manual_seed(156))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial example methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itcm(image, eps, model, steps, alpha, targets):\n",
    "    with torch.no_grad():\n",
    "        image_adv = torch.clone(image)\n",
    "    for n in range(steps):\n",
    "        image_adv.requires_grad = True\n",
    "        image_adv.grad = None\n",
    "        output   = model(image_adv)             #forward pass\n",
    "        output   = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "        loss_fct = torch.nn.NLLLoss()\n",
    "        loss     = loss_fct(output, targets)       #compute loss\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            x_grad         =  alpha * torch.sign(image_adv.grad)    \n",
    "            adv_temp       = image_adv.data + x_grad  #add perturbation to img_variable which \n",
    "            #also contains perturbation from previous iterations\n",
    "            total_grad     = torch.clamp(adv_temp - image, -eps, eps)\n",
    "            image_adv.data = image - total_grad\n",
    "    \n",
    "    return image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, new_images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "    \n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets  = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adv_image(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    adv_images     = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), targets])\n",
    "        # Compute adversarial example\n",
    "        new_images = itcm(images, EPS, sampled_models[k], 15, EPS*0.25, target)\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, new_images)]\n",
    "            #saliencies += [(new_images-images).sign().detach().view(28, 28)]\n",
    "            adv_images += [new_images.view(32, 32)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return adv_images, how_many_fooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = generate_adv_image(0.18,9,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(aa[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 32, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(aa[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(adv_images,success):\n",
    "    # distributional saliency map\n",
    "    combined_med  = torch.zeros_like(adv_images[0])\n",
    "    combined_mean = torch.zeros_like(adv_images[0])\n",
    "\n",
    "    adv_images = torch.stack(adv_images)\n",
    "    # print(saliencies.shape)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(adv_images[:, i, j].detach().numpy(), 50)\n",
    "            combined_mean[i, j] = adv_images[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.reshape(-1,32,32)\n",
    "    combined_mean = combined_mean.reshape(-1,32,32)\n",
    "    champ         = adv_images[success.index(max(success))].reshape(-1,32,32)\n",
    "    return combined_med, combined_mean, champ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = combine_images(aa[0],aa[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "med, mn, ch = bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 32])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack([med,med]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACECAYAAACJbXCEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnjklEQVR4nO2deYwc153fv7/q7pmenvvizJAcckjzlkRJps5IsiXLkrVrGdrdeC3bcMxdGFEMeLE2NtjYWARBgqwD7z/eYBdJDAEyxGQdy7Itr2XJXlumSF0RJZK6KJLm6eEhDm/OfXV3vfwxPe/3e62q7pqrZ3rm9wEEPVbX8ap+9Wrqfet3kDEGiqIoSvnhzXcHFEVRlOmhD3BFUZQyRR/giqIoZYo+wBVFUcoUfYAriqKUKfoAVxRFKVNm9AAnooeI6AgRHSeib81Wp5T5Re26eFHbLi5oun7gRBQDcBTAAwDOAtgL4AvGmEOz1z2l1KhdFy9q28VHfAbb3gbguDHmJAAQ0VMAHgEQejNUJKpNsrIBAGA8sstNnALXJ5//uFDW/UND6Sz/w/eDD+iJCQZRcDtkWxOPBe8zv4+ZbPAPcr9xvszyvCGaJkaB6xS6BnIbbzzkGgQwMt6L8cxw8EWfjl1jKVOVqM91hHfrx/n6k3xRkOfk571AyOvm/BTyokEhpxG2LUWcdIZdHdlfL+SeEm0jl8tDyy7lXQPH/pkp2DXdh/FsqF2BKdo2UVFtkqnGia4Pj9vl2ZrKon2hTN54HRjmthdsAxM2jqurih5vxgyN2KbfWG3bsb6RoLWdvjrrD47ZdpTrlI+8F+R9MHTt7GVjTGv++jN5gK8AcEb8+yyA2wttkKxswO1bvwoAyKQSdvloc3A3EsN8kRJ9aee3inO9/I/h4IuMFBveJMRDtJLb3uBo4KaZZXXB+8wjfrE/+AfRJ39ZI7dTFdwWD7h0PV+PdIqXF7oGcpuqMwOR+gsAe44+UejnKdu1KlGPO7u2AwBMFZ/feHPKtr0sn0dskB8G3mjG2RcN8kBHJsIf6YpE8HJxPGTFfpIRB1XYH4Yx7rtJJQP74fwxruTl2Up+KfDGuU/emHsN/CreJnZ1KFp/AbzevaPYKlOybTLViJvv+UsAQPX+03Z57z1dRfuSvOKeU/zF/bbtparzVwcA+EMh57p1a9HjzZg979nm4AN8Sep++X7g6rKvcv2GV7ptO8p1ykeOd/kc2PP0X58KWn/OP2IS0WNEtI+I9qXT0W9GZWEj7TqeHS6+gVIWOON1XMfrQmcmb+AfAOgU/16ZW+ZgjHkcwOMAUFezws4Porx1Vx++ZNv+hUvOevJ9TP41jK/tCtxvtjEVuHyks9a2YyO8V9m/mjMhb/gAsmfOhf5m95sKmQKKt3E55ZTXQPbJy5tOV14ZwxwwZbvWp5abyTdvX8xuYmP8lhkbEm/d/fzAz7crqvit1ozw7MhrbOB1Yvze4TfU8PoJfsOlcX4D9JP8Riv7F7/GdqWsK4XRoLB5hvclp86UFm+Z8o0/yXY1CbF+lvvtjfJsypEDAXjZOctPVNS20q41TZ1m8i1wdMvKwB027L8QuDxzsju0E87b65/y22vYOBtr5llTpHtevE3PFK+NVYtC5zRJobfumh+/Ydtjn77VtuU5yRn9h8ZGUP+KrhHOXgDriWgNEVUA+DyAZ2ewP2VhoHZdvKhtFxnTfgM3xmSI6C8A/BpADMD3jTEHZ61nyrygdl28qG0XHzORUGCM+SWAX0Zd36/wMNg5ISfE0jxN9EQ7TDbJ/8DhVfOHEErwlFV+fJRTr9A+JeTHquCPh6EfKgGgc3nRYzjHE7KJRHriSNmk8oM+23am7EDoh9KZMlW7mhghU5ec3NgulxKK18f2M73ieuZJF9JLhISc4jezXaVUEvax0fYnj5jwpiDxQfJDSC+pGpbfpNeRX8vSmPQQ8pM8rLI1bO/YMNtPepdQf57WLCQb1NdiNpmqbSeRkqIcG1FkhULIj4Qj924JXEd+zAMqA5fLPhUa9WESaxjyeeLVbLJtKb2GUUh6rXx+r21LGSkh5SLpSPF68H40ElNRFKVM0Qe4oihKmTIjCWWqkG+cqc4k0oc5ypfXfAb+6OYZ9WuSrJBTCk1/QvuxdVnRdaRcJJHTQdkebW4O3Zfsowy0KDkGoJwXhpQGvD7hXjgqvAeMkA9qgn2CASC9eRVvEhLsZUIkFOl3jhAXcicIa8SNBzCNPH31hVeJ88ojjp1JBg8l6evv14n91HLba3I9pBLCOwb5stkCQMqcsneZT2yzben3nY+UDKLgyCPCY6PyilgpxPNESq359G5rm1I/pPzpyjrBTMrFls7g85beKfLapKX3mkooiqIoiwt9gCuKopQppZVQssaGg8tQ+DBvEzn9KfT1OCzwJVvFf5/ktG+uyCYKpaGIvk4Y+fIT7f8d/2Pz2mnvd6ZQ1ke8d2LaTzKnxCWe40rhiMTUkKrcaaap4q/wMi9ErD9YIopdEx4cwivHZIKDbKhSBNkIjxnKl2KkPFIv/BrEarEREeCT4HvNyXcThtzPWJ4nzgfnub2spfi+SoCc5vtiXDpjdJiDk2SgCuBKH2Hh4hIZICSfDzKwxlk+Re+SqHxIBpkB8hrK6+aHrBNFatI3cEVRlDJFH+CKoihlSmkllHTWSifmGgeohGUhk8v9k+GBPOHfmYORwQdh0owR+TZQILVsuoVzcQy38noNJ3k6KYOFZNrXweWcoyPZx8tHmvjYNR/wND150U0a5dWJPCChPSwBvgGNTEyRjfQ2cVKpiuspJA0z4nr7kMg1kpAeGOMiE6MI/jEyO6DYF1UIj49E8G0u1zF17l0kA4GG23m9in4+drpa5FUZFn0S2QgzQiIYq+X7IzYu5KFRV0KJJUU+mMCelx45TkID7IQnSKFgmuopyh1SNglbHnW8SqY6Xisv8v01tIbHnhyvkpa33QBAr7nJtrNXrnJ3xfUY2sznFJZnxtln0TUURVGUBYk+wBVFUcoUfYAriqKUKSXVwP1k3Go8VbtZR4u3c0SUCUkwlL12zd2X1N6EVCR1sezps/yDSJLk1YpENML1bHwjJ6bKJFkfG2vkyzSwyv2blxhglbJvC2uZwx28TbqO16m6wMtb32bd7dQjrLvduPmkbQ9lWH/tfs3Nydz1M27n57MuKR7Z6jQy4RbViescD77VzLCr65sxoaEPDDrH4B0LG8ioznqOnpSuin4dRzqma1mdHWvha9vf6faPhPjce73Q4n1R7iwl8p2f530te4s37n2Uo4y/vOFN276cZg31mZddd7GPPN1u2/HLg1hoOK58J4O/XxUi7BuUrJol7U29fA0zqzna2RcVjuR4lbY7d7dbsSk2IpKUbWON+lwH36vjzWzXiiu8fW23qJr1mV7b/tI6tutvL2y27e5VMvW6O14hNHCpe0ucqmAnAlfRN3BFUZRyRR/giqIoZUpp3QgzxkZkOdOwGpH3+T2OLpTTKxp0p2qxkDzc0p0o/bEbbTuT4imWdBO6cLuI1tzMMs1NbVxpanvra4HHyud/9twXuPziME/PzjRx3u7Ty3navW49l2a7MMzT6/M9vH7771ynsthF7q/Ml11yiGxSKZLFo0VUpTnHOhelWNIw0j0QgNfKybuMdAcT+x3t4OvjV7D9htp5natb+Vq1b7po29tajtv2f21/2bYHfFeCuurzvl4ZXm/bwz7b7Mwou4Xtae2y7Qud3L9/vZbrJRwdYqnwzR5O1NX4vhu5mejptW0T4gJZamaa9zuKu+7Yino+Xkq6Aoox8zAvbehgCWR1A4+Fv+r8daQ+PXXlDtu+2sH35AUxXk/Xso2vNrHt19eyrPPPZ/k5c+4cr19ovA7lRaoG4USBajIrRVGUxYU+wBVFUcqUks7PjEfIpBIfOrA5zF4X8c7g6tf5kon8Qju0gqcafWt56vXl7TyV+mztu7ZdK75wp0VCo3qPp0gvj0YrZfX01duKrnPqNCckat8pqraLaLyR3Xx+qavs9bCxT+T87u5x9pveyF+5Y4NzUqE+OpOvAiKvtunnaSZV8Bd8SoqEVTVuLuxsk5RH2JbXNvF6f/h1lj6+3MDJf+pDkkglhNeKL+w9Jtr5KcMPjK2w7bThfiSIpZadpzbYdvLXfD82i+DSXb+5k9fp5W3br7C3VUX3affgsfJ8ryqUcE6O14HVbEvp1fXn2//Ftj9bx1GdtcJ+0pYvjTbwckTzwjowymPm6jj3o6mCvaEOX2Kpq/ptfrbEhIPc0C6+P5LXxHjtjTZeZXKvKKUfwyjPO0VRFEXRB7iiKEq5UlIJxRvPINk9kSNaJsQxaTE3ifjVXU47jPgzVNnL0+LBLHu3NIhkSo0xnjpdzrJ3S0pIKB9L8vS/z+f+/d2le51+9I7zFOvjTUdtuzPBubD3vbPOtuuP8n69EyLQSGBEMIzMa202uTm/Zb7secUYUHpiCiv7K2UTVAp7Ce+UfLnAFxXnpYRSMcjnmvZ5uZTDWmLF05oNC1umRLKtH/e51/bIMAfT3F3Hdu1KXLbtfzj/Kdte/aLI4T0QEtwi7VWg8ny2RVRC7596ab9SErXKu/SokIE2crwOiPG6d5Qlxda4mxRqkn9VycEwB9IiWEtIXr/odcstXh5jie7bK5+17Y4Yj/0bjnAwzvqXOOkeZbmv3qVe23Zzz7OUk9noBvKEjdco5dnCKLolEX2fiC4S0ftiWRMRvUBEx3L/byy0D2XhoXZdvKhtlw5RHv1PAngob9m3AOw0xqwHsDP3b6W8eBJq18XKk1DbLgmK6hXGmJeJqCtv8SMA7s21dwDYDeCbRY+WzgRWnZe5vWWecGqs/9C6QTgl1UTV96ee+5htP3v9DbxfMYfb0Mz9uauBEw7I6Zwv6l89t/sW99j9/Dfw9XYO+Fi3gb9AmyqeVg2s5Skc1m4KOp1pUXdkalXpZ9WuWR80MPEV38hq8DERjCGmj5O5wwHYHCpByAr3ySsc8POTf7nLtn+7daNtZ7Jsi+tb+fp/qZWjIGLEfXprpMu2n3jmQefYiX62+fPLudp6xxYOCjLiPhpbxQEcgGwzYVNoMu5yI6SdimlIKLNq2wAKVXqPhDjf1CW28f997uO2XX0DyyNyvA4M8f3y9a27bFvKpa9cYcny6P/rcg5dIez6ydXsRSTHqz/O94gzXiXrWfaqORNso0ISZ5jnibMvkV89jOmKL23GmMkzPg+grdDKStmgdl28qG0XITP2QjETlWFD/9QQ0WNEtI+I9o2b0bDVlAXGlOzqL+wPbYpLIdtKu6bHFl4mRMVlul4oF4iowxjTQ0QdAC6GrWiMeRzA4wBQn2w3kzlQolSaLlTbu+qM8OYYDP7DUHm5wbYH3+d27Sl+4BzfwlPwC4/ytOi6BvYq+PWuj9p22768+96Ir86/56lXTzd/ga5z1I15TPtanOnZtbLNWG8S6Xkip5BGSiv83iCr2APuDen8Jqbda/tYohjay+2aQb6271zPOVWG/5g9DDbVck6WH+5iKWbVa3k5WYQUlz4mptTH2TuleUzeC3xsKYnICvWh1erzb6koVe2nTiTbSrvWUZORVdInCStxJsnPnVKzrHiunra9LINU/kp4rUj57Tr2Ntm5nCXIlale2z7yZpdtr/sRLweA4U4e46me2Rmvs1m5Pn6RPW78rUJifTdgZUz/DfxZANtz7e0Afj7N/SgLC7Xr4kVtuwiJ4kb4Q0zkwtpIRGeJ6CsAvgPgASI6BuCTuX8rZYTadfGitl06RPFC+ULIT/dP9WAmHrM5ETxZ2TrAMwWAUy0HKXeaEiabSCrO9dp240H2bsn28TSlMbHVtk8c5enxyBqRu0P46ScG3SlVupr/BsZHWCaoP4lApMfMTBz4Z8ps2hVEMJPSybCwiwnxSAmrXA+A+lgaM0KKILFe7CzfL7WnWBIxo3zsphhPP9+7lfNWNK7leyo2xvuUVeUBt7J8YoB/a/pdsFQip/l+WC4TeapSXaLZlUxmy7ZUWYH4yi4A00gne8fW0J+kTCBzpFSdZ/vFjnGQW1Z4psnx+s4BDr5qv401Bjle003uc0PaLHWZV2w8MrVcQjOVTcKeA73bQr4tz7KEoiiKoswz+gBXFEUpU0pckSdrp0+ZoeB8ETJIINMV7qrqDfOn4ihyihkX61fx1+7YZU4jWXWWp0U33soVed7/KE93rl1gmQUA6rt5GlZ1YapuklNLI5kvucigpXnFTNgWAPwBlkCQEDKULGpcJ4Ij8uWDEFnCKWosC1/LSjo+26nyikhre4qPl1jH699yD1d/Ovk+eyMBQHUPe6XERsScXPZXBuCI5Z5MTpufpzYCCybHjcATHhGZVEWBNYsjZRMppziIAtWekNzCxitEVufb7j1s2wcubHF227aPt6fsNIyTQwbcyKCcQrJow36W+5yCxYi2fRD6Bq4oilKm6ANcURSlTCltxdSQXCjTwRfTOD/KlC4kkMB7j4vcNh5rsO2XujmfwqMb37Lt/7ON1wEAv4LlGC/DU6H4EE/VY6MZBCGrckSh8or778nqRvOOMcBkcWIpmwhZQVbhcQJ8Yq6E4hTxFSmAZfCPqWWZjUTqXU+2T3OcyvJXear98oaP2PZ/u+Fntv2NW10JpeUtvqcq+7lPFf18jPgQyyzOdDwkLY2R6otImzuZijdwX/NINpWwXhHSa0Iy1Xs4n0yEAB85dmNXWXpNXuH7aFc35yH6+W3fs+2Htrlpgqsu83iVFZKijNcw5DWQYzRUHgLcPCchBY7rjvQFLpfoG7iiKEqZog9wRVGUMqW0EkoibvMoeAjOp+DXhKcXnQu89mW2XXe417bTz3GOjStr2Ivhvg1cnQUAXq3kKdoHrTy1b3uT/zbWHWTPjEwD53LITyNajHj3Beff3oBINvSRTswbHtkqO56QPWSqWCe/h5BW/EpXBvKGhCePlE2E5CC3N/IWFl4oJAKHag6xnDL6zx22fXg9B/hsv5cLJQPAT1ffaNvXjjTYdttePl7yOO/Xl7KO6Ic8v9i4qFYk8ryYvHwwkBWqlrVgIRDmHZEWAXZhaVVnk74b+XpUXeXrnHmVc5z8Q9cnbDt/vL6WXGPbsXd4Gz/Odlq5i89DpjSONF6FNBJViAmTYMZbiqft1TdwRVGUMkUf4IqiKGVKaSUUQZgzv/QoCataAcz863dQP2ROitRFngC99COuyHLPo+yRAgDbN3O6TX8zb//kijtt20tz+cHa/RwgJAvb+st4nbDAJH9ggeZnNsZ6g5gaEVwhvE1MBd9q2epwryFTKT1M2DNABm4hJNDFVIr9yrS2Yupbc5b3s+OpB2z7i4++6Ozrrzbt5P5u5Pec73TKSmVceLf+DbarGRH5VkQxZ8erRhbCTee5rZC+V0nC8o5IWS4hil6/8BtO/3z/g28723x505v8D5Gt9fvv83jN7pGPRZbiEv1Tq3pVCvROURRFKVP0Aa4oilKm6ANcURSlTCmpBu5XxDHaNVHqKlsl/3aIxDVC64yJSMP4sFvyai5w8jsL96Hmg3zsN793s7PNzgc5Muzhde/b9ue27LftHz/Kmtxowyrbbn3xjG3773FipahxeJRgzXdOinBFJRaDX5/TroXOS9JtTurZ4yLyMC+ZlYxWdNwFhTueE7k4LqIhhfueiYt+pHl5fJDvr/a9vP9nrtzn9CP7UK9tf2kd66Zfu3G3bX8vfo9tjzayG2fbC6yH+5eEX5h0Q5OuhqPuNw+Z+MtrDa5wP5/IpEwystqXCeoK5AOXRE0ENYmMCJXfweoPddt2/DOcwOrNk+54ffFT/B3pM3K8buZvWz/8kztse/mL3Keqw1xmMXOG85XPJ/oGriiKUqboA1xRFKVMKamE4o1nkOy+8qHlMvpSuhFORzYJK9cUqX9CvqkS+zGiBFsKLpUDG2z7+etvt+3aj/J53rCSp9RHH+HIz3M1LKe0P3mN+xGh6nc+85r+yPet66OpLJ5gyxsSLqCFEjdJSUus54sIT5LJr8Q6nlPajfcTH+B80PFzvEqy240A7r/MEX9PbH3QtlffxbLXlzbtte3f1G+27e6Wlbz+/+jlnYroUBIuj2Ty7tNs8UropYB8YyWLqt2H7PJI95pM1gQ3z7+kWtzrYYnu/JDaARJ5xep/sIePW1vrrNd0iCOnf3szuw4O3MWun2vXsVRyspL7N1bLMlnzEyyhxNd2Fe0f4D6PvAMn+AfR9m/gZGujzcUfz/oGriiKUqboA1xRFKVMmbdITImMPIyU23saFMzNmyN7hufUmfzouBAaXv69bad6ODLvgy18Hl/dtNu2B9o5quyJxrtte/wQ56OuONtr2wslN3RBDKxXBY0J2UtKAUI+MLJqu5f3DpEN91Cxi6XnSUasL708RoMjdWWle4yJdQbdKNf6vWz/1Dn2BDm+lqfU/2nNs7Z933qWGL5d8bBtD7/Hdq1+r4cPkBbnkHcNFkpBNeobRuXzOZkoRAKJIm8U2sY/Gbx9vJ3LKU7nGHZbWeIPgLePy60tP8fHOL6CZa/GVSyzbVzNckpPI0sg2ZPsWRaLOF6diHORfG6kk2WeqUaYF30DJ6JOItpFRIeI6CARfT23vImIXiCiY7n/Nxbbl7JwULsuTtSuS4soEkoGwL83xmwBcAeArxHRFgDfArDTGLMewM7cv5XyQe26OFG7LiGKSijGmB4APbn2ABEdBrACwCMA7s2ttgPAbgDfjHrgqXqITGe/0qskbLkj2YT0SSa5Ste7Es+1dr6EY428XlMNl0OKEU+rEsTBLTc18JfsnZ2rbbs5JEYg/5rJIIiqMwP5qxdkdu1qrHxhkuL6SLlC5gkXZdRkwBTgllST5dJMXOQDl8tlgM8o29UpuyamtU5O5yr2PMm0Nzj96FvH/kYjLdz3rnY2TjWxDJIQNpZ2famaPY1MnejTCE+VJ3OpT5JuZJktcXH+7EqVFYiv7AIAW1oNCA+mKUSYsBCWqEoGC0kvj1BPFem90SLLHLqC1JAYr6NNfB+2bmOp5C+Wc2KzX/TeZNu9o9xXQ/n+aB8mf7yGnWtYHvUoJdWmpIETUReAmwG8AaAtd7MAwHkAbSHbPAbgMQBIxmuDVlHmmRnbNaZ2XYjoeF38RPZCIaIaAD8F8A1jjPNF0Ex8GQr89mKMedwYc4sx5paKWPG/WkppmR27Br9ZKPOHjtelQaQ3cCJKYOJm+IEx5pnc4gtE1GGM6SGiDgAXw/cwgYnH5kw6CSLMoyVsufSOSNfwpRnq4Gl63zpnE3gb2XthcxtPw+5v4dwmMTFWLmX4/Hed50raMk91tplLuBVipjnRZ8uu8DyYmoDBLnOZyNwkQjYx+V4o8p9SNhGYZGXw8lrxh0SWbRPHztTytv2ruX3tOndfdZs5EOuONg7eubOOgy48Yruez7LNXr/MZbtqTrNHg7wGstxcPrGhmeX9mTW7Cup+yXlDpFwxU8Lkg9DnhMzfL8drLY/RkRa+b/p4iAEAvA08Xq9rZ6+gTzazd4on5LAvNnFQ0M8OcF6VTaev2jaJ/PRS4vHy5J6aPcW9aWTAUxTvmyheKATgCQCHjTHfFT89C2B7rr0dwM+LHk1ZMKhdFydq16VFlDfwuwD8GwAHiOid3LK/AfAdAE8T0VcAnALwuTnpoTJXqF0XJ2rXJUQUL5RXEZ6t9P6pHIwyWevMXkopJR9Ziimb5Esw0srTsGuijNbHHuayTH+5zC291eRxIMlLI+ycf2qcc2n85Oqttr3/MgcMXBtg2cETeQ9qSpA6dzbtiqwP6p3wlnCklBAJxK8Iv+0oLA+IrDgvgyWkTJNk+2WFTDbSznLFpZu4T1vuO2bbf7t8t3O49QnOTXPJZ6nlwCjb+MUhrsl1cJAr3Hef5Hw36+LCllUiT4yYducHfzjl46bIbNrVT8YxtHkicKkqxPsjjPxyiDOR+6KM1557+Rr+wS08Xr/WusvZlxyvxzIse333zKds+1A1B+TtvcReRBUptkvfzWxjKQNJ2SQ/p1FY0JJEbtO/7Xr+4emfBK9fdI+KoijKgkQf4IqiKGVKSXOhlNoLRQbgyEnleANPr69u4mnYwEae7j5487u2fX0150iR02YASBuekg/7vN89V9kT4fDbHKSz4iVR6SfBnYqlF0oGjGkQ82yQipE5TxLBEoqUSSibd95jPE2VX/edoCDRlgE+fhXLN1e3iPY9vM9/t40lsBurTtt2Q4y9RQDg3fF22z45xtPlN/u6uP0Ouzh0vMS2XC+m1DJoSUoBXkZKQgvT9pQxVvqYTc+TMMLGa6Kn17bTG1hi6Lmf76M/uPmAbV9Xzembdw9zumcAGPN5vI+Kti8O+Pwr22w7bLyGIa+TFyH/0kzRN3BFUZQyRR/giqIoZUpJJRQaTyPePZHjYGgbf92N8oX6Q+lgh3ma6g8MIgiviROuZVvqbfvSx9lZPn47exv827Vc2DTpRfMEefEypwvt/hXLJs2HOF/HxpN8jN7ruU8VgzwFnM5Xeic9ZU14YMick/VBuUo32ZXNdrFTfFhAw6LY8aArXRhhV1MR7LWBlgbbHOtgSe70QyxhbbvriG3f28jt2hinLh4S3iW9WTcQ6Z96uLDtiV1s1/Y9LMdsuMz3nQwQcoKWpLeJbEsJZdi1faE8KaWERsZs5ZgoEoqsMlM5jRSw8WZO2+uv6rDt7s+zh0/YeE3Foo0fOV6Pvt5l2+17+F5d0882Hl4mKicJ+4UFIMlKRJngNQC4+V1knpmpom/giqIoZYo+wBVFUcqU0lbk8TwgNZGvQkoGjhRQwBFeItOF+l08BTn5xyJlp6issWYZ57Z4oIGn1CsqeUqWFKler2Z4Pz88wl+lUy+6eUpa3+Gp4qornAtFVo3JdLBsUnvKlQymQpSqQvOC59ncHrEBtisNCTlEtElKI3nIYr9+C1+3Y3/WYNvV6zjN5g3LeNr+6Tr2KllZwfau9rhPMhfNP564z7YHX2JPEwBY9jZPo9d0c2pTGhDVZNp4yu+Ny8pA4nxkKtu0aI8IbxtZgBkIrUS0UJBSiVNdp8A2sfVcTFjm+jn5J3K88r7WtHKuETleV1Xy8kohc14T4/U3PVxgevBn7E0EAE1H+F7oGhMFrq9xu+96tutUZZNCTFU2CT2eQN/AFUVRyhR9gCuKopQp81bU2JEDhOeBlE2yjTwt6tvgJpe/spWnmZkVPC26e/1B216Z7LXtlgR7DCSIp7syEGdvPwfc7Olmz4Pq17gfNefdb8u+CFYxLdxHp/LLDIiL6fuk/LTwMFxcWBYZFpVzpGySXcbSSH+eXS9y2hjUb+Tp8n9YxwWEN1SwVJUS8oi05ajh4706yJ4H/3TwNttu+C1fz7Zu14uhUqQLdaoJNbE3U7ZapCX2g+0tZRNvXFyPPq60YwrdK/PohWJ8nyUSIROESSW0jXPyFhqv2ZUsGd21jsfrd1f+yrb/d98Nti2lEhmI05lgmewX57fa9sAv2IOlNm+8kqjQM7RSVD5qZFtGkS6iyiaSybwyUcmkwqXGSfQNXFEUpUzRB7iiKEqZog9wRVGUMqW0Gng6Y90EvVrhjhei7Y62cXTcxdvc3/78fs7z+9XG/bbdEmO9+kSade+dIqnNsRF24dl5lpcPvcvuQw3HIdrsYpSfu3kukN8HMueDq3MvKHzffsdw9FwZeSjcA0eXs1177nOv53/8OGvdj9Swu1qtx9u/P877/dUAa58nhlljfOcCR+8NHWTNvfkQb9twkK/zh6JGRX+dhFQiWZeTYEvmYcqE6d58P8roYarIK/Fn5DWZv8LC5HnwUhPjSboLht2HQwXG698/vMO270letu1GUXfz9yKhW71ILnZ2nMfl/l6O4P7Ho5zevPVVfpS1RhyvstxdFBz3yQjrD/7p7VPafz6jzcUfz/oGriiKUqboA1xRFKVMoYIuTLNMfWq5uWPDV0p2PCWYPUefQN/wuVkL96tPdpg7u7YXX1GZU17v3oG+0Z5Zs2sdNZnbaUKmkHJAw/4LgeuPrWap48QX3Fzwnasv56/+Ia4Mspwycp4lVm+Y3zOTl/n0ak+zkFH3++lHOEcmiuvgHSzpDXbOntvvnqf/er8x5pb85foGriiKUqboA1xRFKVMmbdITEVRyoco0YkjLRw5uPwF97eqM8W9aVaK9mCn/IWlksQwt2dS6T4yEWSTsU9z+LDsU6FrFiavyPOLjRT3dSn6Bk5ESSJ6k4jeJaKDRPRfcsvXENEbRHSciH5ERBXF9qUsHNSuixO169IiioQyBuATxpgbAdwE4CEiugPA3wH4e2PMOgDXAOjXyfJC7bo4UbsuIYpKKGbCTWUy6iCR+88A+ASAL+aW7wDwnwH8r9nvojIXqF0XJ3NmVyklhATySO+UzLK6wHUKIQPYGkJy30dJCDXWzAnApiWzTDFR1XSOkbzCAV7xYVG+URw7SiBQpI+YRBQjoncAXATwAoATAHqNMZO9OAtgRci2jxHRPiLaN56Zep08Ze6YNbtmS+DCpURmtuyaRgk0ZmVGRHqAG2OyxpibMPGd4TYAm6IewBjzuDHmFmPMLRXx6uIbKCVj1uwaSxXfQCkZs2XXBOYvla0SjSl5oRhjeoloF4A7ATQQUTz3V30lgA/mooPK3KN2XZzMpl1nmtdDEuadIWUX6aVR8+M3bLtSrCNzk4zcuyVwn1JOAYDqw1yyUR7PyT8U2nMm7HrUHekLXA4A/Rs5l7yUUBzJRwQCFdrXJFG8UFqJqCHXrgLwAIDDAHYB+Gxute0Afl70aMqCQe26OFG7Li2ivIF3ANhBRDFMPPCfNsY8R0SHADxFRH8L4G0AT8xhP5XZR+26OFG7LiFKmguFiC4BGAJQPDHC4qMFC+e8VxtjplbfqQA5u57CwjrHUrGQzlntOnsstHMOtG1JH+AAQET7gpKyLHaWwnkvhXPMZymc81I4x3zK5Zw1F4qiKEqZog9wRVGUMmU+HuCPz8MxFwJL4byXwjnmsxTOeSmcYz5lcc4l18AVRVGU2UElFEVRlDKlpA9wInqIiI7kUlp+q5THLhVE1ElEu4joUC6d59dzy5uI6AUiOpb7f2OxfZULS8GuwNKzrdp14du1ZBJKLrDgKCYiw84C2AvgC8aYQyXpQIkgog4AHcaYt4ioFsB+AH8E4M8AXDXGfCc3GBqNMd+cv57ODkvFrsDSsq3atTzsWso38NsAHDfGnDTGjAN4CsAjJTx+STDG9Bhj3sq1BzARxrwCE+e6I7faDkzcIIuBJWFXYMnZVu1aBnYt5QN8BYAz4t+hKS0XC0TUBeBmAG8AaDPG9OR+Og+gbb76NcssObsCS8K2atcysKt+xJwjiKgGwE8BfMMY42SnzyXdV/efMkVtuzgpR7uW8gH+AQBZqnTRpiologQmboQfGGOeyS2+kNPaJjW3i/PVv1lmydgVWFK2VbuWgV1L+QDfC2A9TRRXrQDweQDPlvD4JYGICBOZ3g4bY74rfnoWE2k8gcWVznNJ2BVYcrZVu5aBXUudjfAPAfx3ADEA3zfGfLtkBy8RRHQ3gFcAHADg5xb/DSY0tacBrMJEhrfPGWOuzksnZ5mlYFdg6dlW7brw7aqRmIqiKGWKfsRUFEUpU/QBriiKUqboA1xRFKVM0Qe4oihKmaIPcEVRlDJFH+CKoihlij7AFUVRyhR9gCuKopQp/x8wv/QaRwkx2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(med.detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mn.detach().numpy())\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(ch.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 sample models\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, sys, pickle\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import re\n",
    "\n",
    "\n",
    "# Settings\n",
    "train = True\n",
    "HOME_DIR = \"/home/fcbeylun/adv-bnn/\"\n",
    "HOME_DIR = \"./\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\"\n",
    "\n",
    "# Reproducibility\n",
    "common.set_seed(156)\n",
    "\n",
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1\n",
    "\n",
    "\n",
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "\n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly\n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "\n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by\n",
    "        sampling the BBBLeNet model.\n",
    "        '''\n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "\n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "\n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and\n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "\n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def load_models(K = 50):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type).to(device) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(HOME_DIR + '/models/model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models\n",
    "\n",
    "\n",
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type).to(device)\n",
    "sampled_models = load_models(K = 50)\n",
    "sampled_models = [m.to(device) for m in sampled_models]\n",
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "\n",
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class\n",
    "\n",
    "\n",
    "def itcm(image, eps, model, steps, alpha, targets):\n",
    "    with torch.no_grad():\n",
    "        image_adv = torch.clone(image)\n",
    "    for n in range(steps):\n",
    "        image_adv.requires_grad = True\n",
    "        image_adv.grad = None\n",
    "        output   = model(image_adv)             #forward pass\n",
    "        output   = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "        loss_fct = torch.nn.NLLLoss()\n",
    "        loss     = loss_fct(output, targets)       #compute loss\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            x_grad         =  alpha * torch.sign(image_adv.grad)\n",
    "            adv_temp       = image_adv.data + x_grad  #add perturbation to img_variable which\n",
    "            #also contains perturbation from previous iterations\n",
    "            total_grad     = torch.clamp(adv_temp - image, -eps, eps)\n",
    "            image_adv.data = image - total_grad\n",
    "\n",
    "    return image_adv\n",
    "\n",
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, new_images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "\n",
    "    return fool/len(sampled_models)\n",
    "\n",
    "\n",
    "def generate_adv_image(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    adv_images     = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), targets])\n",
    "        # Compute adversarial example\n",
    "        new_images = itcm(images, EPS, sampled_models[k], 15, EPS*0.25, target)\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, new_images)]\n",
    "            #saliencies += [(new_images-images).sign().detach().view(28, 28)]\n",
    "            adv_images += [new_images.view(32, 32)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return adv_images, how_many_fooled\n",
    "\n",
    "def combine_images(adv_images,success):\n",
    "    # distributional saliency map\n",
    "    combined_med  = torch.zeros_like(adv_images[0])\n",
    "    combined_mean = torch.zeros_like(adv_images[0])\n",
    "\n",
    "    adv_images = torch.stack(adv_images)\n",
    "    # print(saliencies.shape)\n",
    "    for i in range(combined_med.shape[0]):\n",
    "        for j in range(combined_med.shape[0]):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(adv_images[:, i, j].detach().numpy(), 50)\n",
    "            combined_mean[i, j] = adv_images[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.reshape(-1,32,32)\n",
    "    combined_mean = combined_mean.reshape(-1,32,32)\n",
    "    champ         = adv_images[success.index(max(success))].reshape(-1,32,32)\n",
    "    return combined_med, combined_mean, champ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(HOME_DIR, train=True, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False,\n",
    "                                          generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(HOME_DIR, train=False, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                                          generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
