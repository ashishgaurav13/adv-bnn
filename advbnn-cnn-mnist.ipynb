{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform_mnist)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by \n",
    "        sampling the BBBLeNet model.\n",
    "        '''    \n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        \n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "        \n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and \n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "        \n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=n_epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load('model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 100, modelname = \"model-cnn.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().view(32, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "newsaliency = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        # choose median perturbation\n",
    "        newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45999999999999996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(32, 32).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((newsaliency*EPS).reshape(32, 32).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(32, 32).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.7.10",
   "language": "python",
   "name": "python-3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
