{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform_mnist)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by \n",
    "        sampling the BBBLeNet model.\n",
    "        '''    \n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        \n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "        \n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and \n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1).to(device)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1).to(device)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1).to(device)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1).to(device)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "        \n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load('model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:915371.601, TrainAcc:0.359, ValLoss:592382.968, ValAcc:0.843, KL:8097309.237\n",
      "Epoch:1, TrainLoss:499981.667, TrainAcc:0.872, ValLoss:426447.502, ValAcc:0.889, KL:4747498.250\n",
      "Epoch:2, TrainLoss:373459.296, TrainAcc:0.916, ValLoss:329510.085, ValAcc:0.919, KL:3565847.630\n",
      "Epoch:3, TrainLoss:293623.119, TrainAcc:0.934, ValLoss:262545.578, ValAcc:0.941, KL:2808167.285\n",
      "Epoch:4, TrainLoss:237181.117, TrainAcc:0.946, ValLoss:213957.115, ValAcc:0.952, KL:2266241.943\n",
      "Epoch:5, TrainLoss:195199.545, TrainAcc:0.953, ValLoss:178405.599, ValAcc:0.953, KL:1860102.887\n",
      "Epoch:6, TrainLoss:163183.041, TrainAcc:0.957, ValLoss:150318.937, ValAcc:0.955, KL:1548068.722\n",
      "Epoch:7, TrainLoss:137946.266, TrainAcc:0.961, ValLoss:127105.634, ValAcc:0.963, KL:1302536.688\n",
      "Epoch:8, TrainLoss:117557.071, TrainAcc:0.964, ValLoss:109611.650, ValAcc:0.962, KL:1106251.570\n",
      "Epoch:9, TrainLoss:101107.321, TrainAcc:0.967, ValLoss:94452.058, ValAcc:0.964, KL:946546.502\n",
      "Epoch:10, TrainLoss:87577.032, TrainAcc:0.969, ValLoss:81822.769, ValAcc:0.968, KL:814606.393\n",
      "Epoch:11, TrainLoss:76431.340, TrainAcc:0.969, ValLoss:71556.198, ValAcc:0.971, KL:705403.779\n",
      "Epoch:12, TrainLoss:66945.658, TrainAcc:0.971, ValLoss:63456.781, ValAcc:0.967, KL:613603.376\n",
      "Epoch:13, TrainLoss:59027.372, TrainAcc:0.971, ValLoss:55607.929, ValAcc:0.974, KL:536241.816\n",
      "Epoch:14, TrainLoss:52165.768, TrainAcc:0.973, ValLoss:49931.963, ValAcc:0.969, KL:469956.739\n",
      "Epoch:15, TrainLoss:46100.448, TrainAcc:0.975, ValLoss:43830.470, ValAcc:0.974, KL:413092.281\n",
      "Epoch:16, TrainLoss:41201.997, TrainAcc:0.975, ValLoss:39910.260, ValAcc:0.972, KL:364312.771\n",
      "Epoch:17, TrainLoss:36707.223, TrainAcc:0.976, ValLoss:35784.717, ValAcc:0.970, KL:321812.350\n",
      "Epoch:18, TrainLoss:33093.774, TrainAcc:0.976, ValLoss:31705.756, ValAcc:0.976, KL:285270.994\n",
      "Epoch:19, TrainLoss:29721.073, TrainAcc:0.976, ValLoss:29032.834, ValAcc:0.973, KL:253161.223\n",
      "Saved 100 models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 100, modelname = \"model-cnn.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().view(32, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "newsaliency = torch.zeros(32, 32)\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        # choose median perturbation\n",
    "        newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10941176470588235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(32, 32).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((newsaliency*EPS).reshape(32, 32).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(32, 32).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
