{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3e3be3a97b40cb99edac6c33bf4891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().view(28, 28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "newsaliency = torch.zeros(28, 28)\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        # choose median perturbation\n",
    "        newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "newsaliency = newsaliency.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32599999999999996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((newsaliency*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
