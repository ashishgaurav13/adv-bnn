{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10     # for cifar10, output classes = 10\n",
    "inputs = 3       # for cifar10, color channels = 3\n",
    "modelname = 'alexnet-cifar10.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform_cifar10)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBAlexNet(common.ModuleWrapper):\n",
    "    '''The architecture of AlexNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBAlexNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 64, 11, stride=4, padding=5, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(64, 192, 5, padding=2, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = BBBConv2d(192, 384, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = BBBConv2d(384, 256, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.conv5 = BBBConv2d(256, 128, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(1 * 1 * 128)\n",
    "        self.classifier = BBBLinear(1 * 1 * 128, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follows the procedure for sampling in the forward methods of BBBConv and \n",
    "# BBBLinear forward to create a fixed set of weights to use for the sampled model\n",
    "def sample_conv2d(bbb_layer):\n",
    "    conv_W_mu = bbb_layer.W_mu\n",
    "    conv_W_rho = bbb_layer.W_rho\n",
    "    conv_W_eps = torch.empty(conv_W_mu.size()).normal_(0,1).to(device)\n",
    "    conv_W_sigma = torch.log1p(torch.exp(conv_W_rho))\n",
    "    conv_weight = conv_W_mu + conv_W_eps * conv_W_sigma\n",
    "    if bbb_layer.use_bias:\n",
    "        conv_bias_mu = bbb_layer.bias_mu\n",
    "        conv_bias_rho = bbb_layer.bias_rho\n",
    "        conv_bias_eps = torch.empty(conv_bias_mu.size()).normal_(0,1).to(device)\n",
    "        conv_bias_sigma = torch.log1p(torch.exp(conv_bias_rho))\n",
    "        conv_bias = conv_bias_mu + conv_bias_eps * conv_bias_sigma\n",
    "    else:\n",
    "        conv_bias = None\n",
    "    return conv_weight.data, conv_bias.data\n",
    "\n",
    "def sample_linear(bbb_layer):\n",
    "        fc_W_mu = bbb_layer.W_mu\n",
    "        fc_W_rho = bbb_layer.W_rho\n",
    "        fc_W_eps = torch.empty(fc_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc_W_sigma = torch.log1p(torch.exp(fc_W_rho))\n",
    "        fc_weight = fc_W_mu + fc_W_eps * fc_W_sigma\n",
    "        if bbb_layer.use_bias:\n",
    "            fc_bias_mu = bbb_layer.bias_mu\n",
    "            fc_bias_rho = bbb_layer.bias_rho\n",
    "            fc_bias_eps = torch.empty(fc_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc_bias_sigma = torch.log1p(torch.exp(fc_bias_rho))\n",
    "            fc_bias = fc_bias_mu + fc_bias_eps * fc_bias_sigma\n",
    "        else:\n",
    "            fc_bias = None\n",
    "        \n",
    "        return fc_weight.data, fc_bias.data\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base AlexNet model that matches the architecture of BayesianAlexNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBAlexNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 64, 11, stride=4, padding=5, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 192, 5, padding=2, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(192,384,3, padding=1, bias=True)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384,256,3, padding=1, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 128, 3, padding=1, bias=True)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # call x.view explicitly in forward\n",
    "        self.classifier = nn.Linear(1 * 1 * 128, outputs, bias=True)\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=bbbnet.pool3.kernel_size, stride=bbbnet.pool3.stride)\n",
    "\n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(bbbnet.conv3.in_channels, bbbnet.conv3.out_channels, bbbnet.conv3.kernel_size,\n",
    "                                stride=bbbnet.conv3.stride, padding=bbbnet.conv3.padding, dilation=bbbnet.conv3.dilation,\n",
    "                                groups=bbbnet.conv3.groups)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(bbbnet.conv4.in_channels, bbbnet.conv4.out_channels, bbbnet.conv4.kernel_size,\n",
    "                        stride=bbbnet.conv4.stride, padding=bbbnet.conv4.padding, dilation=bbbnet.conv4.dilation,\n",
    "                        groups=bbbnet.conv4.groups)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(bbbnet.conv5.in_channels, bbbnet.conv5.out_channels, bbbnet.conv5.kernel_size,\n",
    "                        stride=bbbnet.conv5.stride, padding=bbbnet.conv5.padding, dilation=bbbnet.conv5.dilation,\n",
    "                        groups=bbbnet.conv5.groups)\n",
    "\n",
    "        # Sample convolutional layers\n",
    "        self.conv1.weight.data, self.conv1.bias.data = sample_conv2d(bbbnet.conv1)\n",
    "        self.conv2.weight.data, self.conv2.bias.data = sample_conv2d(bbbnet.conv2)\n",
    "        self.conv3.weight.data, self.conv3.bias.data = sample_conv2d(bbbnet.conv3)\n",
    "        self.conv4.weight.data, self.conv4.bias.data = sample_conv2d(bbbnet.conv4)\n",
    "        self.conv5.weight.data, self.conv5.bias.data = sample_conv2d(bbbnet.conv5)\n",
    "\n",
    "        ### Create Linear Layers\n",
    "        self.classifier = nn.Linear(bbbnet.classifier.in_features, bbbnet.classifier.out_features, bbbnet.classifier.use_bias)\n",
    "        self.classifier.weight.data, self.classifier.bias.data = sample_linear(bbbnet.classifier)            \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(-1, 1 * 1 * 128)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBAlexNet(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = AlexNet(outputs, inputs, layer_type, activation_type).to(device)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(modelname, K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [AlexNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(modelname)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:37214453.350, TrainAcc:0.180, ValLoss:28585974.900, ValAcc:0.232, KL:366737977.478\n",
      "Epoch:1, TrainLoss:24041965.860, TrainAcc:0.261, ValLoss:20316113.000, ValAcc:0.282, KL:239404438.828\n",
      "Epoch:2, TrainLoss:17734344.764, TrainAcc:0.294, ValLoss:15485851.925, ValAcc:0.296, KL:176376604.535\n",
      "Epoch:3, TrainLoss:13791488.318, TrainAcc:0.309, ValLoss:12273871.925, ValAcc:0.314, KL:136969857.172\n",
      "Epoch:4, TrainLoss:11080293.503, TrainAcc:0.316, ValLoss:9990679.350, ValAcc:0.330, KL:109868120.866\n",
      "Epoch:5, TrainLoss:9109082.315, TrainAcc:0.323, ValLoss:8294710.463, ValAcc:0.329, KL:90169369.631\n",
      "Epoch:6, TrainLoss:7619997.720, TrainAcc:0.331, ValLoss:6991829.963, ValAcc:0.337, KL:75290596.637\n",
      "Epoch:7, TrainLoss:6461551.331, TrainAcc:0.343, ValLoss:5965936.200, ValAcc:0.332, KL:63722087.439\n",
      "Epoch:8, TrainLoss:5540353.185, TrainAcc:0.349, ValLoss:5139548.638, ValAcc:0.350, KL:54519510.573\n",
      "Epoch:9, TrainLoss:4793152.815, TrainAcc:0.356, ValLoss:4464883.188, ValAcc:0.353, KL:47059198.854\n",
      "Epoch:10, TrainLoss:4178783.623, TrainAcc:0.363, ValLoss:3904159.944, ValAcc:0.378, KL:40917328.178\n",
      "Epoch:11, TrainLoss:3665553.615, TrainAcc:0.366, ValLoss:3435178.337, ValAcc:0.370, KL:35794370.522\n",
      "Epoch:12, TrainLoss:3232830.634, TrainAcc:0.367, ValLoss:3041188.219, ValAcc:0.366, KL:31473793.032\n",
      "Epoch:13, TrainLoss:2864184.462, TrainAcc:0.376, ValLoss:2699559.506, ValAcc:0.369, KL:27794513.924\n",
      "Epoch:14, TrainLoss:2547547.869, TrainAcc:0.385, ValLoss:2404721.837, ValAcc:0.379, KL:24635440.790\n",
      "Epoch:15, TrainLoss:2273942.178, TrainAcc:0.385, ValLoss:2147219.212, ValAcc:0.405, KL:21904024.599\n",
      "Epoch:16, TrainLoss:2035447.969, TrainAcc:0.391, ValLoss:1926889.009, ValAcc:0.393, KL:19526095.389\n",
      "Epoch:17, TrainLoss:1827148.061, TrainAcc:0.393, ValLoss:1729564.644, ValAcc:0.407, KL:17445598.694\n",
      "Epoch:18, TrainLoss:1643335.711, TrainAcc:0.402, ValLoss:1558268.519, ValAcc:0.406, KL:15615554.363\n",
      "Epoch:19, TrainLoss:1481206.239, TrainAcc:0.403, ValLoss:1406552.644, ValAcc:0.398, KL:13999646.452\n",
      "Epoch:20, TrainLoss:1337990.618, TrainAcc:0.402, ValLoss:1270714.300, ValAcc:0.409, KL:12567250.490\n",
      "Epoch:21, TrainLoss:1210008.646, TrainAcc:0.408, ValLoss:1150994.675, ValAcc:0.398, KL:11292972.860\n",
      "Epoch:22, TrainLoss:1095919.357, TrainAcc:0.413, ValLoss:1042152.719, ValAcc:0.415, KL:10156158.389\n",
      "Epoch:23, TrainLoss:993842.715, TrainAcc:0.412, ValLoss:945239.473, ValAcc:0.421, KL:9139558.796\n",
      "Epoch:24, TrainLoss:901621.901, TrainAcc:0.419, ValLoss:858709.028, ValAcc:0.427, KL:8227624.726\n",
      "Epoch:25, TrainLoss:819590.108, TrainAcc:0.420, ValLoss:780443.827, ValAcc:0.426, KL:7408675.704\n",
      "Epoch:26, TrainLoss:746169.209, TrainAcc:0.418, ValLoss:710560.197, ValAcc:0.424, KL:6672356.933\n",
      "Epoch:27, TrainLoss:679026.718, TrainAcc:0.428, ValLoss:647394.386, ValAcc:0.432, KL:6008735.433\n",
      "Epoch:28, TrainLoss:618510.326, TrainAcc:0.430, ValLoss:589111.964, ValAcc:0.439, KL:5409664.271\n",
      "Epoch:29, TrainLoss:564138.715, TrainAcc:0.435, ValLoss:538104.241, ValAcc:0.436, KL:4869086.930\n",
      "Epoch:30, TrainLoss:514809.423, TrainAcc:0.438, ValLoss:491945.655, ValAcc:0.440, KL:4380536.482\n",
      "Epoch:31, TrainLoss:470016.377, TrainAcc:0.443, ValLoss:450403.675, ValAcc:0.432, KL:3939048.699\n",
      "Epoch:32, TrainLoss:430294.576, TrainAcc:0.443, ValLoss:411081.443, ValAcc:0.449, KL:3539740.366\n",
      "Epoch:33, TrainLoss:393478.439, TrainAcc:0.446, ValLoss:375436.253, ValAcc:0.450, KL:3178410.277\n",
      "Epoch:34, TrainLoss:360471.944, TrainAcc:0.448, ValLoss:344452.395, ValAcc:0.452, KL:2851822.041\n",
      "Epoch:35, TrainLoss:330861.905, TrainAcc:0.451, ValLoss:317358.165, ValAcc:0.449, KL:2556407.252\n",
      "Epoch:36, TrainLoss:303569.778, TrainAcc:0.456, ValLoss:290224.756, ValAcc:0.462, KL:2289254.748\n",
      "Epoch:37, TrainLoss:278956.683, TrainAcc:0.455, ValLoss:268440.052, ValAcc:0.452, KL:2047721.455\n",
      "Epoch:38, TrainLoss:257466.749, TrainAcc:0.457, ValLoss:246428.621, ValAcc:0.453, KL:1830025.098\n",
      "Epoch:39, TrainLoss:236855.597, TrainAcc:0.464, ValLoss:230387.102, ValAcc:0.454, KL:1633106.769\n",
      "Epoch:40, TrainLoss:218793.689, TrainAcc:0.470, ValLoss:210648.912, ValAcc:0.463, KL:1455852.713\n",
      "Epoch:41, TrainLoss:202541.656, TrainAcc:0.468, ValLoss:195660.126, ValAcc:0.464, KL:1296135.742\n",
      "Epoch:42, TrainLoss:188209.386, TrainAcc:0.467, ValLoss:181432.668, ValAcc:0.468, KL:1152870.752\n",
      "Epoch:43, TrainLoss:174532.550, TrainAcc:0.475, ValLoss:170417.147, ValAcc:0.458, KL:1024396.791\n",
      "Epoch:44, TrainLoss:162846.145, TrainAcc:0.475, ValLoss:157891.839, ValAcc:0.479, KL:909150.872\n",
      "Epoch:45, TrainLoss:152649.383, TrainAcc:0.474, ValLoss:147109.202, ValAcc:0.475, KL:806208.647\n",
      "Epoch:46, TrainLoss:142972.775, TrainAcc:0.479, ValLoss:139419.399, ValAcc:0.474, KL:714497.231\n",
      "Epoch:47, TrainLoss:134680.275, TrainAcc:0.483, ValLoss:130358.150, ValAcc:0.477, KL:633022.569\n",
      "Epoch:48, TrainLoss:126848.834, TrainAcc:0.486, ValLoss:124722.304, ValAcc:0.469, KL:560467.006\n",
      "Epoch:49, TrainLoss:119902.206, TrainAcc:0.489, ValLoss:118760.146, ValAcc:0.479, KL:496427.485\n",
      "Epoch:50, TrainLoss:114218.458, TrainAcc:0.488, ValLoss:113165.148, ValAcc:0.481, KL:439791.604\n",
      "Epoch:51, TrainLoss:108771.103, TrainAcc:0.493, ValLoss:106995.824, ValAcc:0.490, KL:390084.450\n",
      "Epoch:52, TrainLoss:104206.895, TrainAcc:0.499, ValLoss:103055.260, ValAcc:0.492, KL:346363.229\n",
      "Epoch:53, TrainLoss:99909.336, TrainAcc:0.498, ValLoss:99275.013, ValAcc:0.487, KL:308395.135\n",
      "Epoch:54, TrainLoss:96631.356, TrainAcc:0.498, ValLoss:95997.672, ValAcc:0.494, KL:275427.380\n",
      "Epoch:55, TrainLoss:93128.742, TrainAcc:0.503, ValLoss:91873.287, ValAcc:0.500, KL:246611.270\n",
      "Epoch:56, TrainLoss:90258.491, TrainAcc:0.506, ValLoss:90075.660, ValAcc:0.499, KL:221920.925\n",
      "Epoch:57, TrainLoss:87847.810, TrainAcc:0.511, ValLoss:88031.438, ValAcc:0.501, KL:200695.667\n",
      "Epoch:58, TrainLoss:86006.263, TrainAcc:0.511, ValLoss:86359.494, ValAcc:0.501, KL:182819.045\n",
      "Epoch:59, TrainLoss:84053.109, TrainAcc:0.511, ValLoss:85386.866, ValAcc:0.501, KL:167312.880\n",
      "Epoch:60, TrainLoss:82430.102, TrainAcc:0.514, ValLoss:82610.901, ValAcc:0.510, KL:154320.565\n",
      "Epoch:61, TrainLoss:80924.253, TrainAcc:0.521, ValLoss:81833.403, ValAcc:0.512, KL:143539.512\n",
      "Epoch:62, TrainLoss:80072.205, TrainAcc:0.520, ValLoss:81537.118, ValAcc:0.503, KL:134349.649\n",
      "Epoch:63, TrainLoss:78620.347, TrainAcc:0.525, ValLoss:78791.240, ValAcc:0.518, KL:126757.874\n",
      "Epoch:64, TrainLoss:78007.091, TrainAcc:0.525, ValLoss:79862.980, ValAcc:0.508, KL:120460.128\n",
      "Epoch:65, TrainLoss:77081.090, TrainAcc:0.527, ValLoss:79301.707, ValAcc:0.512, KL:115425.181\n",
      "Epoch:66, TrainLoss:76698.871, TrainAcc:0.524, ValLoss:79995.439, ValAcc:0.503, KL:111441.619\n",
      "Epoch:67, TrainLoss:75845.927, TrainAcc:0.529, ValLoss:78724.691, ValAcc:0.507, KL:108073.312\n",
      "Epoch:68, TrainLoss:75171.200, TrainAcc:0.534, ValLoss:77129.335, ValAcc:0.517, KL:105660.776\n",
      "Epoch:69, TrainLoss:74450.860, TrainAcc:0.539, ValLoss:77450.600, ValAcc:0.526, KL:103471.756\n",
      "Epoch:70, TrainLoss:74291.909, TrainAcc:0.537, ValLoss:76070.067, ValAcc:0.525, KL:101952.613\n",
      "Epoch:71, TrainLoss:73793.198, TrainAcc:0.541, ValLoss:76526.588, ValAcc:0.527, KL:100691.279\n",
      "Epoch:72, TrainLoss:73331.295, TrainAcc:0.544, ValLoss:76216.954, ValAcc:0.530, KL:99951.739\n",
      "Epoch:73, TrainLoss:73328.648, TrainAcc:0.544, ValLoss:76198.178, ValAcc:0.520, KL:99301.463\n",
      "Epoch:74, TrainLoss:73019.258, TrainAcc:0.547, ValLoss:76542.333, ValAcc:0.522, KL:99049.313\n",
      "Epoch:75, TrainLoss:72447.906, TrainAcc:0.550, ValLoss:75489.838, ValAcc:0.529, KL:98904.608\n",
      "Epoch:76, TrainLoss:72139.586, TrainAcc:0.553, ValLoss:75930.699, ValAcc:0.527, KL:98714.746\n",
      "Epoch:77, TrainLoss:72100.601, TrainAcc:0.556, ValLoss:74323.521, ValAcc:0.534, KL:98522.481\n",
      "Epoch:78, TrainLoss:71625.516, TrainAcc:0.554, ValLoss:75879.101, ValAcc:0.532, KL:98647.533\n",
      "Epoch:79, TrainLoss:71395.678, TrainAcc:0.559, ValLoss:74751.220, ValAcc:0.538, KL:98971.092\n",
      "Epoch:80, TrainLoss:71166.270, TrainAcc:0.561, ValLoss:74219.543, ValAcc:0.538, KL:99342.232\n",
      "Epoch:81, TrainLoss:70751.854, TrainAcc:0.564, ValLoss:75572.613, ValAcc:0.524, KL:99648.314\n",
      "Epoch:82, TrainLoss:70695.217, TrainAcc:0.568, ValLoss:74201.071, ValAcc:0.548, KL:99888.348\n",
      "Epoch:83, TrainLoss:70555.907, TrainAcc:0.566, ValLoss:73256.884, ValAcc:0.543, KL:100583.146\n",
      "Epoch:84, TrainLoss:70264.562, TrainAcc:0.570, ValLoss:74867.112, ValAcc:0.539, KL:101125.323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:85, TrainLoss:69939.746, TrainAcc:0.573, ValLoss:72759.693, ValAcc:0.549, KL:101480.104\n",
      "Epoch:86, TrainLoss:70121.433, TrainAcc:0.574, ValLoss:75698.927, ValAcc:0.534, KL:102050.370\n",
      "Epoch:87, TrainLoss:69871.786, TrainAcc:0.576, ValLoss:74472.485, ValAcc:0.544, KL:102745.868\n",
      "Epoch:88, TrainLoss:69490.342, TrainAcc:0.577, ValLoss:74114.963, ValAcc:0.548, KL:103212.180\n",
      "Epoch:89, TrainLoss:69500.877, TrainAcc:0.577, ValLoss:74101.794, ValAcc:0.553, KL:103590.836\n",
      "Epoch:90, TrainLoss:69384.001, TrainAcc:0.580, ValLoss:73920.992, ValAcc:0.554, KL:104192.281\n",
      "Epoch:91, TrainLoss:69008.504, TrainAcc:0.584, ValLoss:73370.701, ValAcc:0.554, KL:104784.658\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:92, TrainLoss:68744.340, TrainAcc:0.586, ValLoss:74119.174, ValAcc:0.550, KL:105076.706\n",
      "Epoch:93, TrainLoss:67358.088, TrainAcc:0.596, ValLoss:72972.769, ValAcc:0.559, KL:104782.431\n",
      "Epoch:94, TrainLoss:67047.497, TrainAcc:0.598, ValLoss:72959.095, ValAcc:0.560, KL:104126.926\n",
      "Epoch:95, TrainLoss:67280.788, TrainAcc:0.594, ValLoss:73999.776, ValAcc:0.551, KL:103699.443\n",
      "Epoch:96, TrainLoss:66755.850, TrainAcc:0.600, ValLoss:73099.085, ValAcc:0.567, KL:103277.802\n",
      "Epoch:97, TrainLoss:66679.077, TrainAcc:0.597, ValLoss:72735.364, ValAcc:0.559, KL:102945.150\n",
      "Epoch:98, TrainLoss:66523.943, TrainAcc:0.597, ValLoss:72313.797, ValAcc:0.566, KL:102679.048\n",
      "Epoch:99, TrainLoss:66505.199, TrainAcc:0.599, ValLoss:72565.828, ValAcc:0.554, KL:102431.280\n",
      "Saved 100 models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 100, K = 100, modelname = modelname)\n",
    "sampled_models = load_models(modelname, K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform_cifar10)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.05\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89, 3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "unbatched_shape = saliencies.shape[1:]\n",
    "print(unbatched_shape)\n",
    "newsaliency = torch.zeros(unbatched_shape)\n",
    "\n",
    "for i in range(unbatched_shape[0]):\n",
    "    for j in range(unbatched_shape[1]):\n",
    "        for k in range(unbatched_shape[2]):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j,k] = np.percentile(saliencies[:, i, j,k].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.873820224719101"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "images = images.squeeze() # squeeze out batch dimension\n",
    "image_transpose = np.transpose( images.detach().numpy() , (1,2,0))  \n",
    "plt.imshow(image_transpose)\n",
    "plt.subplot(1, 3, 2)\n",
    "perturbation = newsaliency * EPS\n",
    "perturbation_transpose = np.transpose(perturbation.detach().numpy(), (1,2,0))\n",
    "plt.imshow(perturbation_transpose, vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "new_images = new_images.squeeze()\n",
    "print(torch.max(new_images))\n",
    "new_images_transpose = np.transpose(new_images.detach().numpy(), (1,2,0))\n",
    "plt.imshow(new_images_transpose)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
