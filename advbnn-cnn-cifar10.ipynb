{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10     # for cifar10, output classes = 10\n",
    "inputs = 3       # for cifar10, color channels = 3\n",
    "modelname = 'alexnet-cifar10.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform_cifar10)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBAlexNet(common.ModuleWrapper):\n",
    "    '''The architecture of AlexNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBAlexNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 64, 11, stride=4, padding=5, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(64, 192, 5, padding=2, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = BBBConv2d(192, 384, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = BBBConv2d(384, 256, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.conv5 = BBBConv2d(256, 128, 3, padding=1, bias=True, priors=self.priors)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(1 * 1 * 128)\n",
    "        self.classifier = BBBLinear(1 * 1 * 128, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follows the procedure for sampling in the forward methods of BBBConv and \n",
    "# BBBLinear forward to create a fixed set of weights to use for the sampled model\n",
    "def sample_conv2d(bbb_layer):\n",
    "    conv_W_mu = bbb_layer.W_mu\n",
    "    conv_W_rho = bbb_layer.W_rho\n",
    "    conv_W_eps = torch.empty(conv_W_mu.size()).normal_(0,1).to(device)\n",
    "    conv_W_sigma = torch.log1p(torch.exp(conv_W_rho))\n",
    "    conv_weight = conv_W_mu + conv_W_eps * conv_W_sigma\n",
    "    if bbb_layer.use_bias:\n",
    "        conv_bias_mu = bbb_layer.bias_mu\n",
    "        conv_bias_rho = bbb_layer.bias_rho\n",
    "        conv_bias_eps = torch.empty(conv_bias_mu.size()).normal_(0,1).to(device)\n",
    "        conv_bias_sigma = torch.log1p(torch.exp(conv_bias_rho))\n",
    "        conv_bias = conv_bias_mu + conv_bias_eps * conv_bias_sigma\n",
    "    else:\n",
    "        conv_bias = None\n",
    "    return conv_weight.data, conv_bias.data\n",
    "\n",
    "def sample_linear(bbb_layer):\n",
    "        fc_W_mu = bbb_layer.W_mu\n",
    "        fc_W_rho = bbb_layer.W_rho\n",
    "        fc_W_eps = torch.empty(fc_W_mu.size()).normal_(0,1).to(device)\n",
    "        fc_W_sigma = torch.log1p(torch.exp(fc_W_rho))\n",
    "        fc_weight = fc_W_mu + fc_W_eps * fc_W_sigma\n",
    "        if bbb_layer.use_bias:\n",
    "            fc_bias_mu = bbb_layer.bias_mu\n",
    "            fc_bias_rho = bbb_layer.bias_rho\n",
    "            fc_bias_eps = torch.empty(fc_bias_mu.size()).normal_(0,1).to(device)\n",
    "            fc_bias_sigma = torch.log1p(torch.exp(fc_bias_rho))\n",
    "            fc_bias = fc_bias_mu + fc_bias_eps * fc_bias_sigma\n",
    "        else:\n",
    "            fc_bias = None\n",
    "        \n",
    "        return fc_weight.data, fc_bias.data\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base AlexNet model that matches the architecture of BayesianAlexNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBAlexNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 64, 11, stride=4, padding=5, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 192, 5, padding=2, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(192,384,3, padding=1, bias=True)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384,256,3, padding=1, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 128, 3, padding=1, bias=True)\n",
    "        self.act5 = self.act()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # call x.view explicitly in forward\n",
    "        self.classifier = nn.Linear(1 * 1 * 128, outputs, bias=True)\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=bbbnet.pool3.kernel_size, stride=bbbnet.pool3.stride)\n",
    "\n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(bbbnet.conv3.in_channels, bbbnet.conv3.out_channels, bbbnet.conv3.kernel_size,\n",
    "                                stride=bbbnet.conv3.stride, padding=bbbnet.conv3.padding, dilation=bbbnet.conv3.dilation,\n",
    "                                groups=bbbnet.conv3.groups)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(bbbnet.conv4.in_channels, bbbnet.conv4.out_channels, bbbnet.conv4.kernel_size,\n",
    "                        stride=bbbnet.conv4.stride, padding=bbbnet.conv4.padding, dilation=bbbnet.conv4.dilation,\n",
    "                        groups=bbbnet.conv4.groups)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(bbbnet.conv5.in_channels, bbbnet.conv5.out_channels, bbbnet.conv5.kernel_size,\n",
    "                        stride=bbbnet.conv5.stride, padding=bbbnet.conv5.padding, dilation=bbbnet.conv5.dilation,\n",
    "                        groups=bbbnet.conv5.groups)\n",
    "\n",
    "        # Sample convolutional layers\n",
    "        self.conv1.weight.data, self.conv1.bias.data = sample_conv2d(bbbnet.conv1)\n",
    "        self.conv2.weight.data, self.conv2.bias.data = sample_conv2d(bbbnet.conv2)\n",
    "        self.conv3.weight.data, self.conv3.bias.data = sample_conv2d(bbbnet.conv3)\n",
    "        self.conv4.weight.data, self.conv4.bias.data = sample_conv2d(bbbnet.conv4)\n",
    "        self.conv5.weight.data, self.conv5.bias.data = sample_conv2d(bbbnet.conv5)\n",
    "\n",
    "        ### Create Linear Layers\n",
    "        self.classifier = nn.Linear(bbbnet.classifier.in_features, bbbnet.classifier.out_features, bbbnet.classifier.use_bias)\n",
    "        self.classifier.weight.data, self.classifier.bias.data = sample_linear(bbbnet.classifier)            \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(-1, 1 * 1 * 128)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBAlexNet(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = AlexNet(outputs, inputs, layer_type, activation_type).to(device)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(modelname, K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [AlexNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(modelname)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:37214464.688, TrainAcc:0.179, ValLoss:28585921.050, ValAcc:0.231, KL:366738006.217\n",
      "Epoch:1, TrainLoss:24041953.325, TrainAcc:0.265, ValLoss:20316436.850, ValAcc:0.277, KL:239405345.529\n",
      "Epoch:2, TrainLoss:17734352.879, TrainAcc:0.293, ValLoss:15485544.850, ValAcc:0.299, KL:176377071.490\n",
      "Epoch:3, TrainLoss:13791490.682, TrainAcc:0.309, ValLoss:12273500.725, ValAcc:0.317, KL:136969985.834\n",
      "Epoch:4, TrainLoss:11080120.815, TrainAcc:0.317, ValLoss:9990486.275, ValAcc:0.326, KL:109868130.955\n",
      "Epoch:5, TrainLoss:9109045.968, TrainAcc:0.323, ValLoss:8294261.237, ValAcc:0.333, KL:90169560.561\n",
      "Epoch:6, TrainLoss:7620080.710, TrainAcc:0.332, ValLoss:6991838.037, ValAcc:0.335, KL:75290672.306\n",
      "Epoch:7, TrainLoss:6461571.478, TrainAcc:0.344, ValLoss:5965886.688, ValAcc:0.335, KL:63721930.446\n",
      "Epoch:8, TrainLoss:5540364.586, TrainAcc:0.348, ValLoss:5139704.650, ValAcc:0.349, KL:54519375.975\n",
      "Epoch:9, TrainLoss:4793136.500, TrainAcc:0.357, ValLoss:4464841.912, ValAcc:0.353, KL:47059009.962\n",
      "Epoch:10, TrainLoss:4178784.955, TrainAcc:0.363, ValLoss:3903716.156, ValAcc:0.383, KL:40917193.631\n",
      "Epoch:11, TrainLoss:3665534.285, TrainAcc:0.366, ValLoss:3435239.356, ValAcc:0.372, KL:35794231.096\n",
      "Epoch:12, TrainLoss:3232762.605, TrainAcc:0.368, ValLoss:3040896.144, ValAcc:0.364, KL:31473730.306\n",
      "Epoch:13, TrainLoss:2864180.197, TrainAcc:0.375, ValLoss:2699333.600, ValAcc:0.375, KL:27794501.146\n",
      "Epoch:14, TrainLoss:2547537.315, TrainAcc:0.382, ValLoss:2404796.569, ValAcc:0.379, KL:24635358.140\n",
      "Epoch:15, TrainLoss:2273896.252, TrainAcc:0.385, ValLoss:2147308.900, ValAcc:0.402, KL:21903939.070\n",
      "Epoch:16, TrainLoss:2035438.850, TrainAcc:0.391, ValLoss:1926842.262, ValAcc:0.393, KL:19526044.803\n",
      "Epoch:17, TrainLoss:1827126.350, TrainAcc:0.392, ValLoss:1729703.594, ValAcc:0.404, KL:17445516.911\n",
      "Epoch:18, TrainLoss:1643319.356, TrainAcc:0.402, ValLoss:1558434.606, ValAcc:0.404, KL:15615479.000\n",
      "Epoch:19, TrainLoss:1481226.791, TrainAcc:0.403, ValLoss:1406370.847, ValAcc:0.402, KL:13999549.178\n",
      "Epoch:20, TrainLoss:1337975.868, TrainAcc:0.401, ValLoss:1270867.459, ValAcc:0.413, KL:12567183.745\n",
      "Epoch:21, TrainLoss:1209986.784, TrainAcc:0.406, ValLoss:1150809.312, ValAcc:0.400, KL:11292936.643\n",
      "Epoch:22, TrainLoss:1095959.294, TrainAcc:0.412, ValLoss:1042300.289, ValAcc:0.416, KL:10156049.541\n",
      "Epoch:23, TrainLoss:993853.357, TrainAcc:0.412, ValLoss:945365.230, ValAcc:0.420, KL:9139483.892\n",
      "Epoch:24, TrainLoss:901645.767, TrainAcc:0.419, ValLoss:858827.177, ValAcc:0.421, KL:8227549.720\n",
      "Epoch:25, TrainLoss:819635.895, TrainAcc:0.418, ValLoss:780422.281, ValAcc:0.429, KL:7408587.006\n",
      "Epoch:26, TrainLoss:746141.704, TrainAcc:0.418, ValLoss:710461.103, ValAcc:0.425, KL:6672241.312\n",
      "Epoch:27, TrainLoss:679049.523, TrainAcc:0.426, ValLoss:647517.858, ValAcc:0.431, KL:6008635.818\n",
      "Epoch:28, TrainLoss:618500.060, TrainAcc:0.430, ValLoss:589173.316, ValAcc:0.439, KL:5409584.325\n",
      "Epoch:29, TrainLoss:564169.825, TrainAcc:0.433, ValLoss:538102.520, ValAcc:0.436, KL:4869026.551\n",
      "Epoch:30, TrainLoss:514842.778, TrainAcc:0.438, ValLoss:492056.884, ValAcc:0.442, KL:4380451.347\n",
      "Epoch:31, TrainLoss:470105.323, TrainAcc:0.441, ValLoss:450583.766, ValAcc:0.428, KL:3938966.151\n",
      "Epoch:32, TrainLoss:430285.559, TrainAcc:0.443, ValLoss:411000.848, ValAcc:0.449, KL:3539630.916\n",
      "Epoch:33, TrainLoss:393517.090, TrainAcc:0.447, ValLoss:375414.979, ValAcc:0.451, KL:3178273.229\n",
      "Epoch:34, TrainLoss:360523.847, TrainAcc:0.448, ValLoss:344589.061, ValAcc:0.450, KL:2851683.748\n",
      "Epoch:35, TrainLoss:330941.091, TrainAcc:0.450, ValLoss:317343.330, ValAcc:0.448, KL:2556286.588\n",
      "Epoch:36, TrainLoss:303604.647, TrainAcc:0.455, ValLoss:290286.922, ValAcc:0.464, KL:2289092.344\n",
      "Epoch:37, TrainLoss:278992.618, TrainAcc:0.456, ValLoss:268616.284, ValAcc:0.448, KL:2047557.744\n",
      "Epoch:38, TrainLoss:257462.900, TrainAcc:0.457, ValLoss:246496.697, ValAcc:0.457, KL:1829809.298\n",
      "Epoch:39, TrainLoss:236909.710, TrainAcc:0.464, ValLoss:230095.956, ValAcc:0.452, KL:1632892.463\n",
      "Epoch:40, TrainLoss:218800.349, TrainAcc:0.468, ValLoss:210746.177, ValAcc:0.464, KL:1455669.511\n",
      "Epoch:41, TrainLoss:202569.981, TrainAcc:0.470, ValLoss:195657.900, ValAcc:0.464, KL:1295946.290\n",
      "Epoch:42, TrainLoss:188254.578, TrainAcc:0.468, ValLoss:181587.886, ValAcc:0.468, KL:1152661.939\n",
      "Epoch:43, TrainLoss:174612.897, TrainAcc:0.472, ValLoss:170280.780, ValAcc:0.462, KL:1024233.856\n",
      "Epoch:44, TrainLoss:162862.903, TrainAcc:0.474, ValLoss:157963.750, ValAcc:0.478, KL:908973.973\n",
      "Epoch:45, TrainLoss:152671.561, TrainAcc:0.475, ValLoss:147058.511, ValAcc:0.477, KL:806047.939\n",
      "Epoch:46, TrainLoss:143026.172, TrainAcc:0.478, ValLoss:139694.025, ValAcc:0.473, KL:714307.608\n",
      "Epoch:47, TrainLoss:134697.402, TrainAcc:0.480, ValLoss:130406.143, ValAcc:0.482, KL:632907.236\n",
      "Epoch:48, TrainLoss:126927.923, TrainAcc:0.485, ValLoss:124654.702, ValAcc:0.469, KL:560340.234\n",
      "Epoch:49, TrainLoss:119986.424, TrainAcc:0.489, ValLoss:118863.932, ValAcc:0.476, KL:496245.413\n",
      "Epoch:50, TrainLoss:114282.868, TrainAcc:0.488, ValLoss:113472.333, ValAcc:0.479, KL:439576.542\n",
      "Epoch:51, TrainLoss:108819.511, TrainAcc:0.493, ValLoss:107388.085, ValAcc:0.486, KL:389886.294\n",
      "Epoch:52, TrainLoss:104249.836, TrainAcc:0.498, ValLoss:103407.276, ValAcc:0.488, KL:346136.617\n",
      "Epoch:53, TrainLoss:100039.811, TrainAcc:0.497, ValLoss:99421.156, ValAcc:0.489, KL:308221.696\n",
      "Epoch:54, TrainLoss:96671.212, TrainAcc:0.498, ValLoss:96017.835, ValAcc:0.493, KL:275242.575\n",
      "Epoch:55, TrainLoss:93206.576, TrainAcc:0.501, ValLoss:91982.407, ValAcc:0.498, KL:246493.839\n",
      "Epoch:56, TrainLoss:90334.462, TrainAcc:0.506, ValLoss:90213.125, ValAcc:0.495, KL:221799.843\n",
      "Epoch:57, TrainLoss:88035.804, TrainAcc:0.510, ValLoss:88060.335, ValAcc:0.499, KL:200644.719\n",
      "Epoch:58, TrainLoss:86083.142, TrainAcc:0.511, ValLoss:86532.372, ValAcc:0.498, KL:182719.377\n",
      "Epoch:59, TrainLoss:84193.476, TrainAcc:0.509, ValLoss:85469.818, ValAcc:0.500, KL:167246.389\n",
      "Epoch:60, TrainLoss:82540.776, TrainAcc:0.512, ValLoss:82692.113, ValAcc:0.508, KL:154263.484\n",
      "Epoch:61, TrainLoss:81000.267, TrainAcc:0.520, ValLoss:81857.136, ValAcc:0.511, KL:143495.926\n",
      "Epoch:62, TrainLoss:80149.958, TrainAcc:0.520, ValLoss:81841.983, ValAcc:0.499, KL:134351.316\n",
      "Epoch:63, TrainLoss:78673.520, TrainAcc:0.523, ValLoss:78929.880, ValAcc:0.519, KL:126808.908\n",
      "Epoch:64, TrainLoss:78112.520, TrainAcc:0.523, ValLoss:79941.271, ValAcc:0.510, KL:120530.532\n",
      "Epoch:65, TrainLoss:77183.677, TrainAcc:0.527, ValLoss:79246.534, ValAcc:0.513, KL:115538.276\n",
      "Epoch:66, TrainLoss:76851.612, TrainAcc:0.522, ValLoss:80384.822, ValAcc:0.503, KL:111607.660\n",
      "Epoch:67, TrainLoss:75957.554, TrainAcc:0.530, ValLoss:78640.673, ValAcc:0.505, KL:108238.451\n",
      "Epoch:68, TrainLoss:75242.197, TrainAcc:0.535, ValLoss:77209.131, ValAcc:0.518, KL:105697.636\n",
      "Epoch:69, TrainLoss:74523.548, TrainAcc:0.539, ValLoss:77723.349, ValAcc:0.524, KL:103517.551\n",
      "Epoch:70, TrainLoss:74404.220, TrainAcc:0.538, ValLoss:76175.375, ValAcc:0.526, KL:102102.030\n",
      "Epoch:71, TrainLoss:73790.315, TrainAcc:0.541, ValLoss:76614.641, ValAcc:0.526, KL:100849.148\n",
      "Epoch:72, TrainLoss:73400.502, TrainAcc:0.545, ValLoss:76392.547, ValAcc:0.526, KL:100135.007\n",
      "Epoch:73, TrainLoss:73377.506, TrainAcc:0.541, ValLoss:76423.039, ValAcc:0.519, KL:99493.366\n",
      "Epoch:74, TrainLoss:73086.595, TrainAcc:0.547, ValLoss:76577.351, ValAcc:0.521, KL:99233.477\n",
      "Epoch:75, TrainLoss:72456.817, TrainAcc:0.551, ValLoss:75497.370, ValAcc:0.529, KL:99004.825\n",
      "Epoch:76, TrainLoss:72282.054, TrainAcc:0.550, ValLoss:76195.759, ValAcc:0.529, KL:98798.630\n",
      "Epoch:77, TrainLoss:72119.986, TrainAcc:0.555, ValLoss:74317.674, ValAcc:0.538, KL:98554.104\n",
      "Epoch:78, TrainLoss:71617.359, TrainAcc:0.555, ValLoss:75602.207, ValAcc:0.534, KL:98621.922\n",
      "Epoch:79, TrainLoss:71380.231, TrainAcc:0.559, ValLoss:75065.345, ValAcc:0.535, KL:98888.744\n",
      "Epoch:80, TrainLoss:71277.662, TrainAcc:0.561, ValLoss:74636.036, ValAcc:0.533, KL:99345.566\n",
      "Epoch:81, TrainLoss:70783.139, TrainAcc:0.566, ValLoss:75613.807, ValAcc:0.530, KL:99692.362\n",
      "Epoch:82, TrainLoss:70878.142, TrainAcc:0.566, ValLoss:74417.522, ValAcc:0.542, KL:99925.187\n",
      "Epoch:83, TrainLoss:70587.060, TrainAcc:0.567, ValLoss:73454.826, ValAcc:0.544, KL:100405.773\n",
      "Epoch:84, TrainLoss:70350.495, TrainAcc:0.570, ValLoss:75029.395, ValAcc:0.532, KL:100937.123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:85, TrainLoss:70112.051, TrainAcc:0.572, ValLoss:73069.135, ValAcc:0.545, KL:101328.436\n",
      "Epoch:86, TrainLoss:70184.192, TrainAcc:0.571, ValLoss:75666.906, ValAcc:0.537, KL:101849.411\n",
      "Epoch:87, TrainLoss:69952.432, TrainAcc:0.577, ValLoss:74515.032, ValAcc:0.547, KL:102492.452\n",
      "Epoch:88, TrainLoss:69574.078, TrainAcc:0.577, ValLoss:74133.042, ValAcc:0.550, KL:102921.818\n",
      "Epoch:89, TrainLoss:69596.639, TrainAcc:0.577, ValLoss:74246.118, ValAcc:0.548, KL:103280.592\n",
      "Epoch:90, TrainLoss:69552.226, TrainAcc:0.579, ValLoss:74007.180, ValAcc:0.556, KL:103907.145\n",
      "Epoch:91, TrainLoss:69148.883, TrainAcc:0.582, ValLoss:73515.822, ValAcc:0.553, KL:104536.746\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:92, TrainLoss:68816.449, TrainAcc:0.585, ValLoss:74480.256, ValAcc:0.551, KL:104803.495\n",
      "Epoch:93, TrainLoss:67409.534, TrainAcc:0.595, ValLoss:73001.978, ValAcc:0.557, KL:104539.324\n",
      "Epoch:94, TrainLoss:67251.255, TrainAcc:0.595, ValLoss:73186.380, ValAcc:0.560, KL:103881.584\n",
      "Epoch:95, TrainLoss:67474.747, TrainAcc:0.594, ValLoss:73918.434, ValAcc:0.550, KL:103448.579\n",
      "Epoch:96, TrainLoss:66884.560, TrainAcc:0.597, ValLoss:73211.191, ValAcc:0.570, KL:103027.514\n",
      "Epoch:97, TrainLoss:66756.700, TrainAcc:0.596, ValLoss:73027.346, ValAcc:0.558, KL:102686.231\n",
      "Epoch:98, TrainLoss:66750.669, TrainAcc:0.597, ValLoss:72289.609, ValAcc:0.564, KL:102420.821\n",
      "Epoch:99, TrainLoss:66559.161, TrainAcc:0.599, ValLoss:72415.110, ValAcc:0.558, KL:102165.462\n",
      "Epoch:100, TrainLoss:66716.722, TrainAcc:0.596, ValLoss:73085.196, ValAcc:0.561, KL:101937.655\n",
      "Epoch:101, TrainLoss:66678.773, TrainAcc:0.597, ValLoss:72360.341, ValAcc:0.561, KL:101763.117\n",
      "Epoch:102, TrainLoss:66607.933, TrainAcc:0.598, ValLoss:72016.511, ValAcc:0.568, KL:101611.414\n",
      "Epoch:103, TrainLoss:66720.552, TrainAcc:0.598, ValLoss:73046.737, ValAcc:0.557, KL:101476.526\n",
      "Epoch:104, TrainLoss:66594.820, TrainAcc:0.599, ValLoss:72198.916, ValAcc:0.565, KL:101377.980\n",
      "Epoch:105, TrainLoss:66210.873, TrainAcc:0.598, ValLoss:71845.155, ValAcc:0.565, KL:101270.794\n",
      "Epoch:106, TrainLoss:66470.481, TrainAcc:0.601, ValLoss:72077.545, ValAcc:0.566, KL:101186.211\n",
      "Epoch:107, TrainLoss:66432.137, TrainAcc:0.598, ValLoss:72385.134, ValAcc:0.561, KL:101098.762\n",
      "Epoch:108, TrainLoss:66286.448, TrainAcc:0.600, ValLoss:72819.466, ValAcc:0.557, KL:101047.433\n",
      "Epoch:109, TrainLoss:66455.052, TrainAcc:0.599, ValLoss:72358.571, ValAcc:0.564, KL:100995.171\n",
      "Epoch:110, TrainLoss:66364.128, TrainAcc:0.599, ValLoss:71964.789, ValAcc:0.564, KL:100957.858\n",
      "Epoch:111, TrainLoss:66395.625, TrainAcc:0.600, ValLoss:71866.200, ValAcc:0.561, KL:100933.096\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:112, TrainLoss:66305.795, TrainAcc:0.601, ValLoss:72794.242, ValAcc:0.558, KL:100875.835\n",
      "Epoch:113, TrainLoss:66239.977, TrainAcc:0.601, ValLoss:71819.762, ValAcc:0.566, KL:100830.528\n",
      "Epoch:114, TrainLoss:66229.904, TrainAcc:0.599, ValLoss:72455.307, ValAcc:0.564, KL:100820.189\n",
      "Epoch:115, TrainLoss:65937.958, TrainAcc:0.602, ValLoss:73204.743, ValAcc:0.562, KL:100798.492\n",
      "Epoch:116, TrainLoss:65912.932, TrainAcc:0.604, ValLoss:72610.800, ValAcc:0.564, KL:100779.731\n",
      "Epoch:117, TrainLoss:66093.925, TrainAcc:0.603, ValLoss:72223.642, ValAcc:0.562, KL:100764.176\n",
      "Epoch:118, TrainLoss:66215.508, TrainAcc:0.602, ValLoss:71816.588, ValAcc:0.571, KL:100753.329\n",
      "Epoch:119, TrainLoss:66031.704, TrainAcc:0.603, ValLoss:71471.900, ValAcc:0.572, KL:100742.672\n",
      "Epoch:120, TrainLoss:66235.626, TrainAcc:0.602, ValLoss:71392.366, ValAcc:0.571, KL:100723.690\n",
      "Epoch:121, TrainLoss:66152.092, TrainAcc:0.602, ValLoss:71822.786, ValAcc:0.569, KL:100720.590\n",
      "Epoch:122, TrainLoss:65941.058, TrainAcc:0.606, ValLoss:72912.893, ValAcc:0.557, KL:100714.477\n",
      "Epoch:123, TrainLoss:66136.945, TrainAcc:0.603, ValLoss:71577.234, ValAcc:0.564, KL:100695.787\n",
      "Epoch:124, TrainLoss:65926.898, TrainAcc:0.605, ValLoss:71932.531, ValAcc:0.561, KL:100679.719\n",
      "Epoch:125, TrainLoss:65934.293, TrainAcc:0.603, ValLoss:71405.000, ValAcc:0.572, KL:100660.794\n",
      "Epoch:126, TrainLoss:65947.780, TrainAcc:0.602, ValLoss:72133.162, ValAcc:0.565, KL:100657.201\n",
      "Epoch   128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:127, TrainLoss:66045.567, TrainAcc:0.602, ValLoss:72318.608, ValAcc:0.569, KL:100641.956\n",
      "Epoch:128, TrainLoss:66110.837, TrainAcc:0.604, ValLoss:73070.949, ValAcc:0.557, KL:100641.707\n",
      "Epoch:129, TrainLoss:65842.402, TrainAcc:0.603, ValLoss:71883.278, ValAcc:0.568, KL:100639.178\n",
      "Epoch:130, TrainLoss:66046.858, TrainAcc:0.604, ValLoss:72543.155, ValAcc:0.561, KL:100638.493\n",
      "Epoch:131, TrainLoss:65992.976, TrainAcc:0.602, ValLoss:71842.436, ValAcc:0.563, KL:100637.399\n",
      "Epoch:132, TrainLoss:65882.351, TrainAcc:0.604, ValLoss:71647.312, ValAcc:0.566, KL:100635.415\n",
      "Epoch:133, TrainLoss:65791.230, TrainAcc:0.604, ValLoss:72534.261, ValAcc:0.563, KL:100634.476\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:134, TrainLoss:65921.115, TrainAcc:0.604, ValLoss:71660.327, ValAcc:0.566, KL:100632.713\n",
      "Epoch:135, TrainLoss:66027.586, TrainAcc:0.604, ValLoss:72594.409, ValAcc:0.563, KL:100631.702\n",
      "Epoch:136, TrainLoss:66158.399, TrainAcc:0.602, ValLoss:72740.020, ValAcc:0.558, KL:100631.607\n",
      "Epoch:137, TrainLoss:66074.286, TrainAcc:0.600, ValLoss:71828.803, ValAcc:0.564, KL:100631.865\n",
      "Epoch:138, TrainLoss:66229.614, TrainAcc:0.605, ValLoss:72235.988, ValAcc:0.565, KL:100631.990\n",
      "Epoch:139, TrainLoss:66017.700, TrainAcc:0.602, ValLoss:72433.754, ValAcc:0.564, KL:100632.029\n",
      "Epoch:140, TrainLoss:66185.405, TrainAcc:0.600, ValLoss:72605.139, ValAcc:0.560, KL:100632.120\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:141, TrainLoss:66034.766, TrainAcc:0.601, ValLoss:71466.589, ValAcc:0.562, KL:100632.128\n",
      "Epoch:142, TrainLoss:65906.507, TrainAcc:0.605, ValLoss:70699.300, ValAcc:0.571, KL:100632.116\n",
      "Epoch:143, TrainLoss:65950.756, TrainAcc:0.601, ValLoss:71660.686, ValAcc:0.566, KL:100632.092\n",
      "Epoch:144, TrainLoss:66004.127, TrainAcc:0.602, ValLoss:72460.918, ValAcc:0.555, KL:100632.089\n",
      "Epoch:145, TrainLoss:66104.633, TrainAcc:0.601, ValLoss:72374.459, ValAcc:0.557, KL:100632.077\n",
      "Epoch:146, TrainLoss:65983.159, TrainAcc:0.605, ValLoss:71882.854, ValAcc:0.567, KL:100632.083\n",
      "Epoch:147, TrainLoss:65761.882, TrainAcc:0.606, ValLoss:72227.014, ValAcc:0.559, KL:100632.070\n",
      "Epoch:148, TrainLoss:65993.957, TrainAcc:0.602, ValLoss:71327.565, ValAcc:0.569, KL:100632.074\n",
      "Epoch:149, TrainLoss:65941.834, TrainAcc:0.602, ValLoss:71571.866, ValAcc:0.567, KL:100632.048\n",
      "Epoch:150, TrainLoss:65970.823, TrainAcc:0.601, ValLoss:72816.188, ValAcc:0.560, KL:100632.034\n",
      "Epoch:151, TrainLoss:65959.868, TrainAcc:0.602, ValLoss:71772.520, ValAcc:0.565, KL:100632.027\n",
      "Epoch:152, TrainLoss:65828.310, TrainAcc:0.604, ValLoss:72389.054, ValAcc:0.558, KL:100631.999\n",
      "Epoch:153, TrainLoss:65752.314, TrainAcc:0.605, ValLoss:72469.855, ValAcc:0.555, KL:100631.982\n",
      "Epoch:154, TrainLoss:65980.385, TrainAcc:0.604, ValLoss:71261.833, ValAcc:0.573, KL:100631.973\n",
      "Epoch:155, TrainLoss:65895.364, TrainAcc:0.603, ValLoss:72578.305, ValAcc:0.564, KL:100631.974\n",
      "Epoch:156, TrainLoss:65885.017, TrainAcc:0.601, ValLoss:72297.849, ValAcc:0.559, KL:100631.975\n",
      "Epoch:157, TrainLoss:65834.640, TrainAcc:0.605, ValLoss:73283.419, ValAcc:0.560, KL:100631.966\n",
      "Epoch:158, TrainLoss:66072.044, TrainAcc:0.601, ValLoss:72655.021, ValAcc:0.562, KL:100631.953\n",
      "Epoch:159, TrainLoss:66054.284, TrainAcc:0.602, ValLoss:71620.989, ValAcc:0.572, KL:100631.943\n",
      "Epoch:160, TrainLoss:65945.758, TrainAcc:0.602, ValLoss:72552.548, ValAcc:0.564, KL:100631.949\n",
      "Epoch:161, TrainLoss:66069.988, TrainAcc:0.601, ValLoss:72262.744, ValAcc:0.564, KL:100631.946\n",
      "Epoch:162, TrainLoss:65996.803, TrainAcc:0.600, ValLoss:72327.749, ValAcc:0.563, KL:100631.935\n",
      "Epoch:163, TrainLoss:65755.391, TrainAcc:0.602, ValLoss:71931.462, ValAcc:0.558, KL:100631.925\n",
      "Epoch:164, TrainLoss:66106.067, TrainAcc:0.602, ValLoss:71444.462, ValAcc:0.566, KL:100631.911\n",
      "Epoch:165, TrainLoss:66228.612, TrainAcc:0.603, ValLoss:72070.981, ValAcc:0.565, KL:100631.907\n",
      "Epoch:166, TrainLoss:65800.154, TrainAcc:0.604, ValLoss:72255.921, ValAcc:0.558, KL:100631.897\n",
      "Epoch:167, TrainLoss:65996.421, TrainAcc:0.600, ValLoss:72555.750, ValAcc:0.563, KL:100631.908\n",
      "Epoch:168, TrainLoss:65896.860, TrainAcc:0.601, ValLoss:72265.567, ValAcc:0.558, KL:100631.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:169, TrainLoss:65751.775, TrainAcc:0.603, ValLoss:72814.349, ValAcc:0.558, KL:100631.869\n",
      "Epoch:170, TrainLoss:65705.391, TrainAcc:0.604, ValLoss:71830.517, ValAcc:0.566, KL:100631.845\n",
      "Epoch:171, TrainLoss:65900.798, TrainAcc:0.603, ValLoss:72445.858, ValAcc:0.562, KL:100631.834\n",
      "Epoch:172, TrainLoss:65918.678, TrainAcc:0.604, ValLoss:72397.077, ValAcc:0.565, KL:100631.836\n",
      "Epoch:173, TrainLoss:65923.609, TrainAcc:0.605, ValLoss:72229.898, ValAcc:0.563, KL:100631.835\n",
      "Epoch:174, TrainLoss:65881.086, TrainAcc:0.602, ValLoss:71937.629, ValAcc:0.562, KL:100631.834\n",
      "Epoch:175, TrainLoss:65869.934, TrainAcc:0.602, ValLoss:72510.456, ValAcc:0.561, KL:100631.812\n",
      "Epoch:176, TrainLoss:66151.191, TrainAcc:0.599, ValLoss:71682.266, ValAcc:0.570, KL:100631.803\n",
      "Epoch:177, TrainLoss:66135.390, TrainAcc:0.601, ValLoss:72901.053, ValAcc:0.554, KL:100631.798\n",
      "Epoch:178, TrainLoss:66021.365, TrainAcc:0.602, ValLoss:71798.622, ValAcc:0.564, KL:100631.780\n",
      "Epoch:179, TrainLoss:65944.299, TrainAcc:0.602, ValLoss:72237.285, ValAcc:0.565, KL:100631.783\n",
      "Epoch:180, TrainLoss:65839.948, TrainAcc:0.603, ValLoss:72311.815, ValAcc:0.567, KL:100631.766\n",
      "Epoch:181, TrainLoss:66207.315, TrainAcc:0.605, ValLoss:71828.806, ValAcc:0.564, KL:100631.761\n",
      "Epoch:182, TrainLoss:66073.986, TrainAcc:0.601, ValLoss:72076.215, ValAcc:0.563, KL:100631.754\n",
      "Epoch:183, TrainLoss:66181.610, TrainAcc:0.599, ValLoss:71515.909, ValAcc:0.565, KL:100631.749\n",
      "Epoch:184, TrainLoss:66015.007, TrainAcc:0.602, ValLoss:71173.803, ValAcc:0.568, KL:100631.742\n",
      "Epoch:185, TrainLoss:65881.127, TrainAcc:0.602, ValLoss:71851.037, ValAcc:0.570, KL:100631.739\n",
      "Epoch:186, TrainLoss:65919.628, TrainAcc:0.603, ValLoss:71979.570, ValAcc:0.567, KL:100631.729\n",
      "Epoch:187, TrainLoss:66122.351, TrainAcc:0.603, ValLoss:71543.317, ValAcc:0.573, KL:100631.719\n",
      "Epoch:188, TrainLoss:65733.320, TrainAcc:0.603, ValLoss:72307.686, ValAcc:0.556, KL:100631.705\n",
      "Epoch:189, TrainLoss:65941.440, TrainAcc:0.601, ValLoss:72786.653, ValAcc:0.557, KL:100631.682\n",
      "Epoch:190, TrainLoss:66052.258, TrainAcc:0.600, ValLoss:72274.527, ValAcc:0.563, KL:100631.685\n",
      "Epoch:191, TrainLoss:66212.640, TrainAcc:0.604, ValLoss:72112.598, ValAcc:0.555, KL:100631.672\n",
      "Epoch:192, TrainLoss:65888.885, TrainAcc:0.601, ValLoss:71495.195, ValAcc:0.572, KL:100631.662\n",
      "Epoch:193, TrainLoss:66006.971, TrainAcc:0.603, ValLoss:72395.468, ValAcc:0.565, KL:100631.641\n",
      "Epoch:194, TrainLoss:66011.624, TrainAcc:0.601, ValLoss:72746.125, ValAcc:0.562, KL:100631.628\n",
      "Epoch:195, TrainLoss:65851.547, TrainAcc:0.603, ValLoss:71758.221, ValAcc:0.567, KL:100631.626\n",
      "Epoch:196, TrainLoss:65793.762, TrainAcc:0.603, ValLoss:72695.264, ValAcc:0.560, KL:100631.615\n",
      "Epoch:197, TrainLoss:65835.727, TrainAcc:0.602, ValLoss:71774.756, ValAcc:0.566, KL:100631.599\n",
      "Epoch:198, TrainLoss:66073.733, TrainAcc:0.601, ValLoss:72811.574, ValAcc:0.554, KL:100631.587\n",
      "Epoch:199, TrainLoss:65727.920, TrainAcc:0.604, ValLoss:71612.570, ValAcc:0.565, KL:100631.586\n",
      "Saved 100 models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 200, K = 100, modelname = modelname)\n",
    "sampled_models = load_models(modelname, K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform_cifar10)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.05\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92, 3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "unbatched_shape = saliencies.shape[1:]\n",
    "print(unbatched_shape)\n",
    "newsaliency = torch.zeros(unbatched_shape)\n",
    "\n",
    "for i in range(unbatched_shape[0]):\n",
    "    for j in range(unbatched_shape[1]):\n",
    "        for k in range(unbatched_shape[2]):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j,k] = np.percentile(saliencies[:, i, j,k].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168478260869566"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "images = images.squeeze() # squeeze out batch dimension\n",
    "image_transpose = np.transpose( images.detach().numpy() , (1,2,0))  \n",
    "plt.imshow(image_transpose)\n",
    "plt.subplot(1, 3, 2)\n",
    "perturbation = newsaliency * EPS\n",
    "perturbation_transpose = np.transpose(perturbation.detach().numpy(), (1,2,0))\n",
    "plt.imshow(perturbation_transpose, vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "new_images = new_images.squeeze()\n",
    "print(torch.max(new_images))\n",
    "new_images_transpose = np.transpose(new_images.detach().numpy(), (1,2,0))\n",
    "plt.imshow(new_images_transpose)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
