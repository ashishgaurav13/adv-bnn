{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "def get_images_targets_nontargets():\n",
    "    images, targets = next(iter(test_loader))\n",
    "    images = images.reshape(-1, 28*28)\n",
    "    nontargets = []\n",
    "    for j in range(10):\n",
    "        if j != targets.item():\n",
    "            nontargets += [torch.tensor([j])]\n",
    "    return images, targets, nontargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, images, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "def collect_saliencies(sampled_models, images, new_targets, eps):\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), new_targets])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, eps, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, images, eps, images.grad.sign())]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    return saliencies, how_many_fooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributional saliency map\n",
    "def distr_saliency_map(saliencies):\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    newsaliency = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "    newsaliency = newsaliency.flatten()\n",
    "    return newsaliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random combination of hyperparameters\n",
    "def get_random_hyperparam(dict_of_lists):\n",
    "    hp = {}\n",
    "    for arg, argvals in dict_of_lists.items():\n",
    "        hp[arg] = np.random.choice(argvals)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => Eps:0.10, NewTarget:9, MedianOfIndivFool:0.15, AggFool(MedianPixel):0.31\n",
      "1 => Eps:0.18, NewTarget:5, MedianOfIndivFool:0.25, AggFool(MedianPixel):0.75\n",
      "3 => Eps:0.02, NewTarget:8, MedianOfIndivFool:0.01, AggFool(MedianPixel):0.02\n",
      "4 => Eps:0.12, NewTarget:2, MedianOfIndivFool:0.26, AggFool(MedianPixel):0.72\n",
      "5 => Eps:0.08, NewTarget:5, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.18\n",
      "6 => Eps:0.04, NewTarget:5, MedianOfIndivFool:0.01, AggFool(MedianPixel):0.01\n",
      "7 => Eps:0.20, NewTarget:2, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.64\n",
      "8 => Eps:0.08, NewTarget:7, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.16\n",
      "9 => Eps:0.14, NewTarget:1, MedianOfIndivFool:0.09, AggFool(MedianPixel):0.37\n",
      "10 => Eps:0.14, NewTarget:0, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.23\n",
      "11 => Eps:0.12, NewTarget:8, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.63\n",
      "12 => Eps:0.12, NewTarget:1, MedianOfIndivFool:0.15, AggFool(MedianPixel):0.37\n",
      "14 => Eps:0.16, NewTarget:9, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.65\n",
      "15 => Eps:0.06, NewTarget:9, MedianOfIndivFool:0.11, AggFool(MedianPixel):0.14\n",
      "16 => Eps:0.14, NewTarget:3, MedianOfIndivFool:0.32, AggFool(MedianPixel):0.65\n",
      "17 => Eps:0.12, NewTarget:5, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.54\n",
      "18 => Eps:0.02, NewTarget:3, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.08\n",
      "19 => Eps:0.08, NewTarget:9, MedianOfIndivFool:0.14, AggFool(MedianPixel):0.41\n",
      "20 => Eps:0.02, NewTarget:4, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.08\n",
      "22 => Eps:0.18, NewTarget:2, MedianOfIndivFool:0.43, AggFool(MedianPixel):0.92\n",
      "24 => Eps:0.18, NewTarget:2, MedianOfIndivFool:0.44, AggFool(MedianPixel):0.90\n",
      "25 => Eps:0.14, NewTarget:8, MedianOfIndivFool:0.14, AggFool(MedianPixel):0.50\n",
      "26 => Eps:0.20, NewTarget:8, MedianOfIndivFool:0.27, AggFool(MedianPixel):0.78\n",
      "27 => Eps:0.12, NewTarget:3, MedianOfIndivFool:0.27, AggFool(MedianPixel):0.66\n",
      "28 => Eps:0.14, NewTarget:8, MedianOfIndivFool:0.23, AggFool(MedianPixel):0.66\n",
      "29 => Eps:0.08, NewTarget:6, MedianOfIndivFool:0.21, AggFool(MedianPixel):0.41\n",
      "30 => Eps:0.18, NewTarget:4, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.42\n",
      "31 => Eps:0.12, NewTarget:9, MedianOfIndivFool:0.40, AggFool(MedianPixel):0.71\n",
      "33 => Eps:0.20, NewTarget:8, MedianOfIndivFool:0.51, AggFool(MedianPixel):0.87\n",
      "34 => Eps:0.14, NewTarget:7, MedianOfIndivFool:0.19, AggFool(MedianPixel):0.55\n",
      "35 => Eps:0.06, NewTarget:0, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.17\n",
      "36 => Eps:0.16, NewTarget:8, MedianOfIndivFool:0.41, AggFool(MedianPixel):0.87\n",
      "37 => Eps:0.14, NewTarget:2, MedianOfIndivFool:0.41, AggFool(MedianPixel):0.86\n",
      "38 => Eps:0.02, NewTarget:3, MedianOfIndivFool:0.07, AggFool(MedianPixel):0.11\n",
      "40 => Eps:0.12, NewTarget:4, MedianOfIndivFool:0.02, AggFool(MedianPixel):0.09\n",
      "41 => Eps:0.10, NewTarget:6, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.49\n",
      "42 => Eps:0.04, NewTarget:6, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.15\n",
      "43 => Eps:0.14, NewTarget:9, MedianOfIndivFool:0.24, AggFool(MedianPixel):0.76\n",
      "44 => Eps:0.08, NewTarget:4, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.29\n",
      "45 => Eps:0.14, NewTarget:8, MedianOfIndivFool:0.43, AggFool(MedianPixel):0.89\n",
      "46 => Eps:0.08, NewTarget:0, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.14\n",
      "47 => Eps:0.04, NewTarget:0, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.22\n",
      "48 => Eps:0.20, NewTarget:8, MedianOfIndivFool:0.62, AggFool(MedianPixel):0.99\n",
      "49 => Eps:0.02, NewTarget:8, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.14\n",
      "50 => Eps:0.18, NewTarget:9, MedianOfIndivFool:0.24, AggFool(MedianPixel):0.81\n",
      "51 => Eps:0.20, NewTarget:5, MedianOfIndivFool:0.38, AggFool(MedianPixel):0.88\n",
      "53 => Eps:0.18, NewTarget:9, MedianOfIndivFool:0.50, AggFool(MedianPixel):0.84\n",
      "54 => Eps:0.04, NewTarget:4, MedianOfIndivFool:0.15, AggFool(MedianPixel):0.24\n",
      "55 => Eps:0.02, NewTarget:2, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.15\n",
      "56 => Eps:0.14, NewTarget:2, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.16\n",
      "57 => Eps:0.20, NewTarget:6, MedianOfIndivFool:0.62, AggFool(MedianPixel):0.92\n",
      "59 => Eps:0.12, NewTarget:6, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.13\n",
      "60 => Eps:0.04, NewTarget:7, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.11\n",
      "61 => Eps:0.14, NewTarget:0, MedianOfIndivFool:0.11, AggFool(MedianPixel):0.21\n",
      "62 => Eps:0.12, NewTarget:6, MedianOfIndivFool:0.36, AggFool(MedianPixel):0.56\n",
      "63 => Eps:0.02, NewTarget:5, MedianOfIndivFool:0.03, AggFool(MedianPixel):0.03\n",
      "64 => Eps:0.12, NewTarget:0, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.24\n",
      "65 => Eps:0.12, NewTarget:6, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.49\n",
      "66 => Eps:0.02, NewTarget:3, MedianOfIndivFool:0.01, AggFool(MedianPixel):0.01\n",
      "67 => Eps:0.02, NewTarget:6, MedianOfIndivFool:0.01, AggFool(MedianPixel):0.02\n",
      "68 => Eps:0.08, NewTarget:0, MedianOfIndivFool:0.17, AggFool(MedianPixel):0.38\n",
      "69 => Eps:0.14, NewTarget:9, MedianOfIndivFool:0.27, AggFool(MedianPixel):0.66\n",
      "70 => Eps:0.18, NewTarget:5, MedianOfIndivFool:0.11, AggFool(MedianPixel):0.30\n",
      "71 => Eps:0.14, NewTarget:0, MedianOfIndivFool:0.23, AggFool(MedianPixel):0.63\n",
      "72 => Eps:0.18, NewTarget:4, MedianOfIndivFool:0.48, AggFool(MedianPixel):0.79\n",
      "73 => Eps:0.18, NewTarget:1, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.11\n",
      "74 => Eps:0.08, NewTarget:5, MedianOfIndivFool:0.02, AggFool(MedianPixel):0.09\n",
      "75 => Eps:0.12, NewTarget:9, MedianOfIndivFool:0.32, AggFool(MedianPixel):0.57\n",
      "76 => Eps:0.16, NewTarget:3, MedianOfIndivFool:0.45, AggFool(MedianPixel):0.67\n",
      "77 => Eps:0.14, NewTarget:5, MedianOfIndivFool:0.45, AggFool(MedianPixel):0.71\n",
      "78 => Eps:0.10, NewTarget:5, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.12\n",
      "79 => Eps:0.10, NewTarget:9, MedianOfIndivFool:0.14, AggFool(MedianPixel):0.41\n",
      "80 => Eps:0.10, NewTarget:9, MedianOfIndivFool:0.07, AggFool(MedianPixel):0.23\n",
      "81 => Eps:0.02, NewTarget:2, MedianOfIndivFool:0.02, AggFool(MedianPixel):0.01\n",
      "82 => Eps:0.16, NewTarget:8, MedianOfIndivFool:0.39, AggFool(MedianPixel):0.81\n",
      "84 => Eps:0.08, NewTarget:3, MedianOfIndivFool:0.14, AggFool(MedianPixel):0.42\n",
      "86 => Eps:0.10, NewTarget:1, MedianOfIndivFool:0.09, AggFool(MedianPixel):0.30\n",
      "87 => Eps:0.04, NewTarget:5, MedianOfIndivFool:0.07, AggFool(MedianPixel):0.16\n",
      "88 => Eps:0.06, NewTarget:0, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.11\n",
      "89 => Eps:0.12, NewTarget:0, MedianOfIndivFool:0.16, AggFool(MedianPixel):0.54\n",
      "90 => Eps:0.14, NewTarget:2, MedianOfIndivFool:0.34, AggFool(MedianPixel):0.84\n",
      "91 => Eps:0.20, NewTarget:9, MedianOfIndivFool:0.16, AggFool(MedianPixel):0.56\n",
      "92 => Eps:0.06, NewTarget:1, MedianOfIndivFool:0.02, AggFool(MedianPixel):0.07\n",
      "93 => Eps:0.16, NewTarget:5, MedianOfIndivFool:0.58, AggFool(MedianPixel):0.92\n",
      "94 => Eps:0.04, NewTarget:1, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.13\n",
      "95 => Eps:0.06, NewTarget:9, MedianOfIndivFool:0.17, AggFool(MedianPixel):0.35\n",
      "96 => Eps:0.02, NewTarget:4, MedianOfIndivFool:0.02, AggFool(MedianPixel):0.09\n",
      "97 => Eps:0.06, NewTarget:3, MedianOfIndivFool:0.16, AggFool(MedianPixel):0.29\n",
      "98 => Eps:0.20, NewTarget:6, MedianOfIndivFool:0.51, AggFool(MedianPixel):0.96\n",
      "99 => Eps:0.14, NewTarget:4, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.64\n"
     ]
    }
   ],
   "source": [
    "# try 100 different combinations\n",
    "for i in range(100):\n",
    "    images, targets, nontargets = get_images_targets_nontargets()\n",
    "    hp = get_random_hyperparam({\n",
    "        'eps': np.arange(0.0, 0.22, 0.02),\n",
    "        'classidx': nontargets\n",
    "    })\n",
    "    eps, classidx = hp['eps'], hp['classidx']\n",
    "    saliencies, how_many_fooled = collect_saliencies(sampled_models, images, torch.tensor([classidx]), eps)\n",
    "    if saliencies == []: continue\n",
    "    newsaliency = distr_saliency_map(saliencies)\n",
    "    indiv = np.percentile(how_many_fooled, 50)\n",
    "    agg = how_many_can_it_fool(sampled_models, images, eps, newsaliency)\n",
    "    print(\"%d => Eps:%.2f, NewTarget:%g, MedianOfIndivFool:%.2f, AggFool(MedianPixel):%.2f\" % (\n",
    "        i, eps, classidx, indiv, agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eps:0.02, NewTarget:0, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.08\n",
      "Eps:0.02, NewTarget:1, MedianOfIndivFool:0.03, AggFool(MedianPixel):0.06\n",
      "Eps:0.02, NewTarget:3, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.09\n",
      "Eps:0.02, NewTarget:4, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.06\n",
      "Eps:0.02, NewTarget:5, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.06\n",
      "Eps:0.02, NewTarget:6, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.07\n",
      "Eps:0.02, NewTarget:7, MedianOfIndivFool:0.04, AggFool(MedianPixel):0.05\n",
      "Eps:0.02, NewTarget:8, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.10\n",
      "Eps:0.02, NewTarget:9, MedianOfIndivFool:0.05, AggFool(MedianPixel):0.06\n",
      "Eps:0.04, NewTarget:0, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.15\n",
      "Eps:0.04, NewTarget:1, MedianOfIndivFool:0.06, AggFool(MedianPixel):0.17\n",
      "Eps:0.04, NewTarget:3, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.17\n",
      "Eps:0.04, NewTarget:4, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.12\n",
      "Eps:0.04, NewTarget:5, MedianOfIndivFool:0.09, AggFool(MedianPixel):0.16\n",
      "Eps:0.04, NewTarget:6, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.13\n",
      "Eps:0.04, NewTarget:7, MedianOfIndivFool:0.07, AggFool(MedianPixel):0.07\n",
      "Eps:0.04, NewTarget:8, MedianOfIndivFool:0.10, AggFool(MedianPixel):0.20\n",
      "Eps:0.04, NewTarget:9, MedianOfIndivFool:0.08, AggFool(MedianPixel):0.17\n",
      "Eps:0.06, NewTarget:0, MedianOfIndivFool:0.12, AggFool(MedianPixel):0.25\n",
      "Eps:0.06, NewTarget:1, MedianOfIndivFool:0.09, AggFool(MedianPixel):0.22\n",
      "Eps:0.06, NewTarget:3, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.28\n",
      "Eps:0.06, NewTarget:4, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.20\n",
      "Eps:0.06, NewTarget:5, MedianOfIndivFool:0.14, AggFool(MedianPixel):0.27\n",
      "Eps:0.06, NewTarget:6, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.23\n",
      "Eps:0.06, NewTarget:7, MedianOfIndivFool:0.11, AggFool(MedianPixel):0.12\n",
      "Eps:0.06, NewTarget:8, MedianOfIndivFool:0.15, AggFool(MedianPixel):0.32\n",
      "Eps:0.06, NewTarget:9, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.23\n",
      "Eps:0.08, NewTarget:0, MedianOfIndivFool:0.18, AggFool(MedianPixel):0.32\n",
      "Eps:0.08, NewTarget:1, MedianOfIndivFool:0.13, AggFool(MedianPixel):0.30\n",
      "Eps:0.08, NewTarget:3, MedianOfIndivFool:0.19, AggFool(MedianPixel):0.34\n",
      "Eps:0.08, NewTarget:4, MedianOfIndivFool:0.18, AggFool(MedianPixel):0.28\n",
      "Eps:0.08, NewTarget:5, MedianOfIndivFool:0.19, AggFool(MedianPixel):0.38\n",
      "Eps:0.08, NewTarget:6, MedianOfIndivFool:0.18, AggFool(MedianPixel):0.27\n",
      "Eps:0.08, NewTarget:7, MedianOfIndivFool:0.15, AggFool(MedianPixel):0.19\n",
      "Eps:0.08, NewTarget:8, MedianOfIndivFool:0.21, AggFool(MedianPixel):0.40\n",
      "Eps:0.08, NewTarget:9, MedianOfIndivFool:0.19, AggFool(MedianPixel):0.33\n",
      "Eps:0.10, NewTarget:0, MedianOfIndivFool:0.22, AggFool(MedianPixel):0.41\n",
      "Eps:0.10, NewTarget:1, MedianOfIndivFool:0.17, AggFool(MedianPixel):0.40\n",
      "Eps:0.10, NewTarget:3, MedianOfIndivFool:0.24, AggFool(MedianPixel):0.38\n",
      "Eps:0.10, NewTarget:4, MedianOfIndivFool:0.23, AggFool(MedianPixel):0.36\n",
      "Eps:0.10, NewTarget:5, MedianOfIndivFool:0.24, AggFool(MedianPixel):0.45\n",
      "Eps:0.10, NewTarget:6, MedianOfIndivFool:0.22, AggFool(MedianPixel):0.34\n",
      "Eps:0.10, NewTarget:7, MedianOfIndivFool:0.19, AggFool(MedianPixel):0.26\n",
      "Eps:0.10, NewTarget:8, MedianOfIndivFool:0.26, AggFool(MedianPixel):0.51\n",
      "Eps:0.10, NewTarget:9, MedianOfIndivFool:0.24, AggFool(MedianPixel):0.44\n",
      "Eps:0.12, NewTarget:0, MedianOfIndivFool:0.27, AggFool(MedianPixel):0.49\n",
      "Eps:0.12, NewTarget:1, MedianOfIndivFool:0.20, AggFool(MedianPixel):0.45\n",
      "Eps:0.12, NewTarget:3, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.52\n",
      "Eps:0.12, NewTarget:4, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.45\n",
      "Eps:0.12, NewTarget:5, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.58\n",
      "Eps:0.12, NewTarget:6, MedianOfIndivFool:0.26, AggFool(MedianPixel):0.39\n",
      "Eps:0.12, NewTarget:7, MedianOfIndivFool:0.23, AggFool(MedianPixel):0.32\n",
      "Eps:0.12, NewTarget:8, MedianOfIndivFool:0.30, AggFool(MedianPixel):0.60\n",
      "Eps:0.12, NewTarget:9, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.54\n",
      "Eps:0.14, NewTarget:0, MedianOfIndivFool:0.30, AggFool(MedianPixel):0.55\n",
      "Eps:0.14, NewTarget:1, MedianOfIndivFool:0.25, AggFool(MedianPixel):0.52\n",
      "Eps:0.14, NewTarget:3, MedianOfIndivFool:0.31, AggFool(MedianPixel):0.60\n",
      "Eps:0.14, NewTarget:4, MedianOfIndivFool:0.32, AggFool(MedianPixel):0.54\n",
      "Eps:0.14, NewTarget:5, MedianOfIndivFool:0.33, AggFool(MedianPixel):0.64\n",
      "Eps:0.14, NewTarget:6, MedianOfIndivFool:0.29, AggFool(MedianPixel):0.46\n",
      "Eps:0.14, NewTarget:7, MedianOfIndivFool:0.27, AggFool(MedianPixel):0.40\n",
      "Eps:0.14, NewTarget:8, MedianOfIndivFool:0.34, AggFool(MedianPixel):0.70\n",
      "Eps:0.14, NewTarget:9, MedianOfIndivFool:0.32, AggFool(MedianPixel):0.60\n",
      "Eps:0.16, NewTarget:0, MedianOfIndivFool:0.34, AggFool(MedianPixel):0.63\n",
      "Eps:0.16, NewTarget:1, MedianOfIndivFool:0.28, AggFool(MedianPixel):0.56\n",
      "Eps:0.16, NewTarget:3, MedianOfIndivFool:0.35, AggFool(MedianPixel):0.67\n",
      "Eps:0.16, NewTarget:4, MedianOfIndivFool:0.36, AggFool(MedianPixel):0.63\n",
      "Eps:0.16, NewTarget:5, MedianOfIndivFool:0.36, AggFool(MedianPixel):0.73\n",
      "Eps:0.16, NewTarget:6, MedianOfIndivFool:0.33, AggFool(MedianPixel):0.57\n",
      "Eps:0.16, NewTarget:7, MedianOfIndivFool:0.30, AggFool(MedianPixel):0.45\n",
      "Eps:0.16, NewTarget:8, MedianOfIndivFool:0.39, AggFool(MedianPixel):0.81\n",
      "Eps:0.16, NewTarget:9, MedianOfIndivFool:0.37, AggFool(MedianPixel):0.65\n",
      "Eps:0.18, NewTarget:0, MedianOfIndivFool:0.37, AggFool(MedianPixel):0.73\n",
      "Eps:0.18, NewTarget:1, MedianOfIndivFool:0.31, AggFool(MedianPixel):0.61\n",
      "Eps:0.18, NewTarget:3, MedianOfIndivFool:0.38, AggFool(MedianPixel):0.76\n",
      "Eps:0.18, NewTarget:4, MedianOfIndivFool:0.40, AggFool(MedianPixel):0.73\n",
      "Eps:0.18, NewTarget:5, MedianOfIndivFool:0.41, AggFool(MedianPixel):0.80\n",
      "Eps:0.18, NewTarget:6, MedianOfIndivFool:0.37, AggFool(MedianPixel):0.65\n",
      "Eps:0.18, NewTarget:7, MedianOfIndivFool:0.33, AggFool(MedianPixel):0.50\n",
      "Eps:0.18, NewTarget:8, MedianOfIndivFool:0.42, AggFool(MedianPixel):0.86\n",
      "Eps:0.18, NewTarget:9, MedianOfIndivFool:0.40, AggFool(MedianPixel):0.73\n",
      "Eps:0.20, NewTarget:0, MedianOfIndivFool:0.41, AggFool(MedianPixel):0.76\n",
      "Eps:0.20, NewTarget:1, MedianOfIndivFool:0.34, AggFool(MedianPixel):0.61\n",
      "Eps:0.20, NewTarget:3, MedianOfIndivFool:0.42, AggFool(MedianPixel):0.80\n",
      "Eps:0.20, NewTarget:4, MedianOfIndivFool:0.43, AggFool(MedianPixel):0.78\n",
      "Eps:0.20, NewTarget:5, MedianOfIndivFool:0.44, AggFool(MedianPixel):0.83\n",
      "Eps:0.20, NewTarget:6, MedianOfIndivFool:0.41, AggFool(MedianPixel):0.73\n",
      "Eps:0.20, NewTarget:7, MedianOfIndivFool:0.36, AggFool(MedianPixel):0.56\n",
      "Eps:0.20, NewTarget:8, MedianOfIndivFool:0.46, AggFool(MedianPixel):0.87\n",
      "Eps:0.20, NewTarget:9, MedianOfIndivFool:0.44, AggFool(MedianPixel):0.78\n"
     ]
    }
   ],
   "source": [
    "images, targets, _ = get_images_targets_nontargets()\n",
    "for eps in np.arange(0.0, 0.22, 0.02):\n",
    "    for classidx in range(10):\n",
    "        if classidx != targets.item():\n",
    "            saliencies, how_many_fooled = collect_saliencies(sampled_models, images, torch.tensor([classidx]), eps)\n",
    "            if saliencies == []: continue\n",
    "            newsaliency = distr_saliency_map(saliencies)\n",
    "            indiv = np.percentile(how_many_fooled, 50)\n",
    "            agg = how_many_can_it_fool(sampled_models, images, eps, newsaliency)\n",
    "            print(\"Eps:%.2f, NewTarget:%g, MedianOfIndivFool:%.2f, AggFool(MedianPixel):%.2f\" % (\n",
    "                eps, classidx, indiv, agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.7.10",
   "language": "python",
   "name": "python-3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
