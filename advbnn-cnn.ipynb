{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1\n",
    "\n",
    "net_type = 'lenet'   # (lenet/alexnet)\n",
    "dataset = 'mnist'    # (mnist/cifar10/cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample model savename: lenet-mnist.pt\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 1\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar100':\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 100\n",
    "    trainset = torchvision.datasets.CIFAR100(root='.', train=True, download=True, transform=transform)\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported dataset\")\n",
    "    \n",
    "    \n",
    "if net_type == 'lenet':\n",
    "    net_class = common.non_bayesian_models.LeNet\n",
    "    bbb_model = common.bayesian_models.BBBLeNet\n",
    "elif net_type == 'alexnet':\n",
    "    net_class = common.non_bayesian_models.AlexNet\n",
    "    bbb_model = common.bayesian_models.BBBAlexNet\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported network type\")\n",
    "    \n",
    "modelname = net_type + '-' + dataset + '.pt'\n",
    "print(f'Sample model savename: {modelname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = bbb_model(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\", force_train=False):\n",
    "    if os.path.exists(modelname) and not force_train:\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = net_class(outputs, inputs, layer_type, activation_type).to(device)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [net_class(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(modelname)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:915371.407, TrainAcc:0.359, ValLoss:592383.209, ValAcc:0.843, KL:8097310.173\n",
      "Epoch:1, TrainLoss:499981.616, TrainAcc:0.872, ValLoss:426447.589, ValAcc:0.889, KL:4747498.815\n",
      "Epoch:2, TrainLoss:373459.233, TrainAcc:0.916, ValLoss:329510.342, ValAcc:0.919, KL:3565848.080\n",
      "Epoch:3, TrainLoss:293623.090, TrainAcc:0.934, ValLoss:262545.366, ValAcc:0.941, KL:2808167.940\n",
      "Epoch:4, TrainLoss:237181.118, TrainAcc:0.946, ValLoss:213956.941, ValAcc:0.952, KL:2266242.259\n",
      "Epoch:5, TrainLoss:195199.491, TrainAcc:0.953, ValLoss:178405.611, ValAcc:0.953, KL:1860102.872\n",
      "Epoch:6, TrainLoss:163182.932, TrainAcc:0.957, ValLoss:150318.605, ValAcc:0.955, KL:1548068.644\n",
      "Epoch:7, TrainLoss:137946.166, TrainAcc:0.961, ValLoss:127105.712, ValAcc:0.963, KL:1302536.584\n",
      "Epoch:8, TrainLoss:117557.040, TrainAcc:0.964, ValLoss:109611.454, ValAcc:0.962, KL:1106251.514\n",
      "Epoch:9, TrainLoss:101107.275, TrainAcc:0.967, ValLoss:94452.122, ValAcc:0.964, KL:946545.912\n",
      "Epoch:10, TrainLoss:87576.903, TrainAcc:0.969, ValLoss:81822.572, ValAcc:0.968, KL:814605.696\n",
      "Epoch:11, TrainLoss:76431.364, TrainAcc:0.969, ValLoss:71556.393, ValAcc:0.971, KL:705402.917\n",
      "Epoch:12, TrainLoss:66945.925, TrainAcc:0.971, ValLoss:63456.730, ValAcc:0.967, KL:613603.287\n",
      "Epoch:13, TrainLoss:59027.569, TrainAcc:0.971, ValLoss:55607.980, ValAcc:0.974, KL:536241.930\n",
      "Epoch:14, TrainLoss:52166.066, TrainAcc:0.973, ValLoss:49931.646, ValAcc:0.969, KL:469957.967\n",
      "Epoch:15, TrainLoss:46100.371, TrainAcc:0.975, ValLoss:43830.472, ValAcc:0.974, KL:413092.724\n",
      "Epoch:16, TrainLoss:41201.932, TrainAcc:0.975, ValLoss:39909.772, ValAcc:0.972, KL:364312.312\n",
      "Epoch:17, TrainLoss:36706.972, TrainAcc:0.976, ValLoss:35784.936, ValAcc:0.970, KL:321810.863\n",
      "Epoch:18, TrainLoss:33093.897, TrainAcc:0.976, ValLoss:31706.355, ValAcc:0.976, KL:285270.615\n",
      "Epoch:19, TrainLoss:29720.953, TrainAcc:0.976, ValLoss:29033.674, ValAcc:0.973, KL:253161.398\n",
      "Saved 100 models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 100, modelname = modelname, force_train=False)\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 1\n",
    "    outputs = 10\n",
    "    testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "elif dataset == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 10\n",
    "    testset = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform)\n",
    "elif dataset == 'cifar100':\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 100\n",
    "    testset = torchvision.datasets.CIFAR100(root='.', train=False, download=True, transform=transform)\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported dataset\")\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 32, 32])\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "unbatched_shape = saliencies.shape[1:]\n",
    "print(unbatched_shape)\n",
    "newsaliency = torch.zeros(unbatched_shape)\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    for i in range(unbatched_shape[0]):\n",
    "        for j in range(unbatched_shape[1]):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "else:\n",
    "    for i in range(unbatched_shape[0]):\n",
    "        for j in range(unbatched_shape[1]):\n",
    "            for k in range(unbatched_shape[2]):\n",
    "                # choose median perturbation\n",
    "                newsaliency[i, j,k] = np.percentile(saliencies[:, i, j,k].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10941176470588235"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dataset != 'mnist':\n",
    "    # cifar datasets have color channels, need to arrange color channels\n",
    "    images_prepped = np.transpose( images.squeeze().detach().numpy() , (1,2,0)) # squeeze out batch dimension and arrange color channels\n",
    "    perturbation = newsaliency * EPS\n",
    "    perturbation_prepped = np.transpose(perturbation.detach().numpy(), (1,2,0))\n",
    "    new_images_prepped = np.transpose( new_images.squeeze().detach().numpy(), (1,2,0))\n",
    "else:\n",
    "    images_prepped = images.squeeze().detach().numpy()\n",
    "    perturbation = newsaliency * EPS\n",
    "    perturbation_prepped = perturbation.squeeze().detach().numpy()\n",
    "    new_images_prepped = new_images.squeeze().detach().numpy()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images_prepped)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(perturbation_prepped, vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images_prepped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
