{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1\n",
    "\n",
    "net_type = 'alexnet'   # (lenet/alexnet)\n",
    "dataset = 'cifar10'    # (mnist/cifar10/cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Sample model savename: alexnet-cifar10.pt\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 1\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 10\n",
    "    trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform)\n",
    "elif dataset == 'cifar100':\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 100\n",
    "    trainset = torchvision.datasets.CIFAR100(root='.', train=True, download=True, transform=transform)\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported dataset\")\n",
    "    \n",
    "    \n",
    "if net_type == 'lenet':\n",
    "    net_class = common.non_bayesian_models.LeNet\n",
    "    bbb_model = common.bayesian_models.BBBLeNet\n",
    "elif net_type == 'alexnet':\n",
    "    net_class = common.non_bayesian_models.AlexNet\n",
    "    bbb_model = common.bayesian_models.BBBAlexNet\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported network type\")\n",
    "    \n",
    "modelname = net_type + '-' + dataset + '.pt'\n",
    "print(f'Sample model savename: {modelname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = bbb_model(outputs, inputs, priors, layer_type, activation_type).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\", force_train=False):\n",
    "    if os.path.exists(modelname) and not force_train:\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset)).to(device)\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = net_class(outputs, inputs, layer_type, activation_type).to(device)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [net_class(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(modelname)):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:37214447.376, TrainAcc:0.179, ValLoss:28585646.050, ValAcc:0.233, KL:366737852.943\n",
      "Epoch:1, TrainLoss:24042108.076, TrainAcc:0.262, ValLoss:20316271.100, ValAcc:0.278, KL:239404641.223\n",
      "Epoch:2, TrainLoss:17734307.382, TrainAcc:0.294, ValLoss:15486328.250, ValAcc:0.289, KL:176377470.981\n",
      "Epoch:3, TrainLoss:13791453.299, TrainAcc:0.308, ValLoss:12273776.300, ValAcc:0.309, KL:136970470.981\n",
      "Epoch:4, TrainLoss:11080202.414, TrainAcc:0.318, ValLoss:9991087.400, ValAcc:0.321, KL:109868375.389\n",
      "Epoch:5, TrainLoss:9108910.627, TrainAcc:0.325, ValLoss:8293441.625, ValAcc:0.336, KL:90169811.159\n",
      "Epoch:6, TrainLoss:7619969.105, TrainAcc:0.333, ValLoss:6991449.525, ValAcc:0.341, KL:75291111.694\n",
      "Epoch:7, TrainLoss:6461548.570, TrainAcc:0.343, ValLoss:5966002.138, ValAcc:0.335, KL:63722204.484\n",
      "Epoch:8, TrainLoss:5540408.242, TrainAcc:0.348, ValLoss:5139867.450, ValAcc:0.349, KL:54519595.949\n",
      "Epoch:9, TrainLoss:4793156.268, TrainAcc:0.357, ValLoss:4464778.237, ValAcc:0.355, KL:47059217.376\n",
      "Epoch:10, TrainLoss:4178850.041, TrainAcc:0.364, ValLoss:3904046.975, ValAcc:0.377, KL:40917386.420\n",
      "Epoch:11, TrainLoss:3665557.879, TrainAcc:0.366, ValLoss:3435265.438, ValAcc:0.375, KL:35794361.580\n",
      "Epoch:12, TrainLoss:3232785.089, TrainAcc:0.369, ValLoss:3040632.812, ValAcc:0.367, KL:31473725.274\n",
      "Epoch:13, TrainLoss:2864138.059, TrainAcc:0.374, ValLoss:2699178.587, ValAcc:0.377, KL:27794489.529\n",
      "Epoch:14, TrainLoss:2547518.699, TrainAcc:0.383, ValLoss:2404563.044, ValAcc:0.382, KL:24635398.573\n",
      "Epoch:15, TrainLoss:2273841.707, TrainAcc:0.385, ValLoss:2147294.625, ValAcc:0.402, KL:21903945.108\n",
      "Epoch:16, TrainLoss:2035429.148, TrainAcc:0.392, ValLoss:1926947.041, ValAcc:0.392, KL:19525988.318\n",
      "Epoch:17, TrainLoss:1827155.042, TrainAcc:0.392, ValLoss:1729770.925, ValAcc:0.403, KL:17445598.752\n",
      "Epoch:18, TrainLoss:1643325.424, TrainAcc:0.401, ValLoss:1558577.900, ValAcc:0.401, KL:15615545.975\n",
      "Epoch:19, TrainLoss:1481230.226, TrainAcc:0.401, ValLoss:1406434.053, ValAcc:0.401, KL:13999593.166\n",
      "Saved 100 models\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 100, modelname = modelname, force_train=True)\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'mnist':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 1\n",
    "    outputs = 10\n",
    "    testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "elif dataset == 'cifar10':\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 10\n",
    "    testset = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform)\n",
    "elif dataset == 'cifar100':\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "    inputs = 3\n",
    "    outputs = 100\n",
    "    testset = torchvision.datasets.CIFAR100(root='.', train=False, download=True, transform=transform)\n",
    "else:\n",
    "    raise RuntimeException(\"Unsupported dataset\")\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "unbatched_shape = saliencies.shape[1:]\n",
    "print(unbatched_shape)\n",
    "newsaliency = torch.zeros(unbatched_shape)\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    for i in range(unbatched_shape[0]):\n",
    "        for j in range(unbatched_shape[1]):\n",
    "            # choose median perturbation\n",
    "            newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "else:\n",
    "    for i in range(unbatched_shape[0]):\n",
    "        for j in range(unbatched_shape[1]):\n",
    "            for k in range(unbatched_shape[2]):\n",
    "                # choose median perturbation\n",
    "                newsaliency[i, j,k] = np.percentile(saliencies[:, i, j,k].numpy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, newsaliency))\n",
    "new_images = otcm(images, EPS, newsaliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792857142857144"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACECAYAAACJbXCEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAv9ElEQVR4nO19aZhdVZnuu85Yc2pKKiEhAySASrpDi7S2aIMtNkOjIBBIgIRBlFav2tLdcH3u07ZDD05cRb3dhiaQQEgIAWRSEVC0bVAIIJYhEmIGSFIZK1VJVZ35rPvjnKz32ydnV+1KnRQ5Vd/7PHnynV1rr7X2XufdZ693fd+3jLUWCoVCoag+hN7qDigUCoXiyKAPcIVCoahS6ANcoVAoqhT6AFcoFIoqhT7AFQqFokqhD3CFQqGoUozoAW6MOdcY85oxZqMx5pZKdUrx1kLHdexCx3ZswRypH7gxJgxgA4BzAGwD8AKABdbaVyvXPcVoQ8d17ELHduwhMoJzzwCw0Vq7CQCMMasAfASA75ehsTZi25piAAAjjhtjypaXPy4W3h8azzkWZct5apUfrJx4iPJGnitOEMdLf+/8fwB5vixhrc+1+vQ7L+v3Obe0Fdle3kq78KG3P4tEKudX2bDHNWIiNobCuCZqxR8Sg3Q3QKFan0+eM2oT5Uz/8uJTrWggkfC25j2r/EV5j5aeX66NskVKL7SkafHB7+Z6G9lrrZ3o09Kwxra20dimtoLd1s3jpk1yg6bNy+9gi6cuydd9Hsqw4jZ5eD/P726RfN3H8p5utLK86GtLq/gAwNpm8alH2Gxvn7ioFus935URh9vYtOf5YNnVAkQ5081+5FvZj7zoX34vj29Joey4juQBPhXAm+LzNgB/PtgJbU0xfHHhKQAAY/PueCzKbpgQByudTjk7m8t46orFYs7O5VmX/BKZUM7ZoTDPtZl6lgHLRGNJZ4ch+8Q6c/mspx+ZLNvO5+U3iudnxbMyJcp4H9SsR37Z02ledy7nHS55D0PiOtLifvSL7g6kC2Xufmo7BsGwxzWGGGZjNgCgc7b4Q+dgZw1daLbPJ88ZszvLmf7lxafZooHOTm9r3rPKX5T3aOn55dooW6T0QkuaFh/8bq63ka0+rQDDHNumNmDhPxWIc9U9gq/XBOHrOZ66JF+Xe/h6r7OvEs/p0Cqef898ydflzr4mxu98GOc6+96V5OsVC1Z6+pHJ/pWz8/kH+Afz185cJvh6aZ7nS77efQ/tq66inU7TzrGrhfMX0g6tYD/SC9iP/uzZzh648yFnL34NZcd1JBp4uTe4w15HjTEfN8asNcas7Utky5yiOMYw7HHNQse1SjDk2MpxTfSNUq8UR4yRvIFvA3C8+DwNwI7SQtbaJQCWAMCMjnqbLv5mWCumgOIXOQ7+2obA1+ZIhL+2ABAqr4LARPmHlPg5zOZFXUJCCYs384io0+TFG3+WbxbyTbfQdVaQNjXOzoXjPC7L5NiIyeeEzYdgjbiGiKEdinifo7mM6KPh+Vb00QrOhsOFugYTYnAE41pXV2fdW6R4MZwrygd6GS+BPGfu3M7yf/ArL/8wt/xf/N7MD6+g/B88b9Se/gVpg2XmljTdCZ9r9V6UT72DYsixleM6qSlk0ysK35nkQn6nVghuXIc6Z3v56n03lHxdJL7G966+0tm5yzgDzoqv9kLB1/sEYSMrL3G2kW/yfJzglfwFnn7MvpsVpwURcuE1zr5oPtsYWP4xtrGAr9SXpsVLS/5q9sksc/bKxZ6mcUWG/cVVgq/iTd1m+dYdDn9EnP0wymEkb+AvAJhjjJlljIkBuALAIyOoT3FsQMd17ELHdozhiN/ArbVZY8ynATwBIAxgqbV2XcV6pnhLoOM6dqFjO/ZwxG6ER4IZHXX2f19xUuGDPeCOx8UKYy0anJ3KUAIJ15ZOycoLAXm5QCKOZ22UxzNctY+E2EY0ctDZRtyXkFxgDHt/8/rzlE0O5Cib7OzlVK0/zevr62NdYctpaWMNy8QMyzTVsc7auFdrzmUpUoaEDp0TfU8IxWcgXTh+1xPb0bUvNYSSEhymrs4eWkjzTOyPRDfxlQPmljf95BQ/hWGkuo4f/CSXI1rIDdB5f0npRWvt6UFaHQqnNBp7x+mFr8kK8Z26jooBasUkPpUR3+0Sz5pQaCHKIS+kD7uCx5ctZL2Sr4tXCr4uJseMWFSUTRkhkQJAv3AiCMTXH/jw9fogfL3c03Yuexf7iIt43P7Q2YnllE0GLuM9v/ILj5QdV43EVCgUiiqFPsAVCoWiSjESL5Rhw1iLSK7o0REWEoVY1Y6HhUwQEbP8UImEEhafhVaSlZErQmaJxjgNmzzzJGcf6Nnr7L37Blg+Qr/VEIRHSdZ7yxKW9a7fyrpsnF77mTA9a9INlFz6ehkNsH13j7Mb4mwjt5PHp3ewTwDQ1sh+1USEb67lPYzJlfbiFNAvcKoS6BzulL9Ueggid/ipB0H0Gz9tZe4gHen0u465ZYv71hMYAW7CKMhCBkAkV+DT4vCF7ngo/6iz46uETLBAcG+lV7sILRJ8FXJHVqoMQpr5mOTr/wi+fob+2nv3bXJ29Kr72Za9ztnpZfd5+pGw9CpZf+Z3nG3X/C9nZxb9kOd/wo+v9G5piPPacjspw07vEE7hANoa6XFTE6FeZOylzo4Jz5XcXV6Pt3LQN3CFQqGoUugDXKFQKKoUoyqhFFCYvptIM4+IKX1WhoeHKAWks97pSEwEyuRyInBFBMdA1BsTwTF//kGG6b747HPO3tHD5AX9QirJ5iiBbN22x9OPzdsZlh5vnuLsaR2z2Kd4I68jwn5HG5jaIJukR8m+3YytqGumFLOtb5en7aTwuOlopJdNXZTT11yGstChjACmwo5HtZDR4CKIxTO1L+9FcpjUMVzvET/vj7k+8oZvgE4J/GQTvyYCSRo+9+DwxgNUHODcEaIbwKGJvok87o5fcz+/X9mrVzk7tIJfrPTCyzx1xVYIvl4h+CrDzReJ8qvJ17b55Gvk5//j7B2nvejs/mXkWDbHvCZbtzE4CAA2n/VtZ8eX3ODsaSeQr/c8caKzL1ss+Pqw4Osl73D2vt3fcHbd47w32zIlfK1b4OyOG6kX1UXvdrbMGBIKwFN9A1coFIoqhT7AFQqFokoxqoE80zvq7c2XnwoA6M0wh0JO5BppaaBs0hRmvpRI3puN0NcLJcvzQ0I2yYtgoZZ2Tn929bDtN/fSi2Tr9l7aXZQ3wjUMNAKAXJ5tdEyd4ezjp5/AvteIaxW5TTIiV8te0XZUeN8kE/3O3rz5NU/btaA88rbj2K+ZE7mCn08xYCqbK9yb7z+2Bdv2JirmilJXV2dnzy5NqXdkOBK1oyItDBJLc1REDL88KodVEEB68mo5FQvkmdgYsh99Z0FCCMZXHo+IfCQAcLVwSlnh4SvtEFVACFqhpf1kZ/vz9W9od1HWCV/v7UeOagU+P3W6s38ZgK/zg/D1o4KvX/UOQ+2Vkq/MbTLzp4Kvl9LbJ7ucmRAvfLl8gJa+gSsUCkWVQh/gCoVCUaXQB7hCoVBUKUbVjTCbN9iTKIhh3Zlmd/yXz/7C2W+bQ5e9s9/R7uyWsFerzwvXwZDIERwSQlrOimQ34qdq89bNzu5O0E3I1tH9KNxATTnUwuiq2uYJnn6kk3RTSsukNi28jqYG2rt37nT2gf2M7GqMcShqxBZZb+wXWlvjJE/be3a+4eyGXezj5CaeXyt3Bjq0jlDhdY8EKNuOVLeulO7tnxJL5upmqbmHRWIO3Se/XN9zA1yF59zOkvJ+Gbr8IkcrmZRLoClv8KEiX78r+foyIyDfNo3lz34HSdYS9org9wrvXpmDXwZY52Refx++nif4mhd8/XrDM86+uuXjzn7oZ495+pFuI1/vFHw9TfD1bz18Jedu2882Qj58fV9S8PXGJz1t7/nW+5zd0MQbMnkOIzRr773d2Vl6HQIvoyz0DVyhUCiqFPoAVygUiirF6CazCscRmVCIeBrYJ9zpYoxw6h4Q2xmlmUimKeaNxMxbuaUR517hMF2AkmlObfbQ+wh7D3L6IiMdWybSrag/T/e7drFteLjGm+g4HWW/kv2UMZJ9PH9GR5uzB8TUa3eabpImyqlhbzfdjSAiSxP9dFECgHCM17r7wH5nd/VymjijXchLh2aMlc5lNRINpdSDLmiw4hAIcq6nzGHJr4J0pHPoIn7bqA3SwU6fUE6v/FO22soms5oUQ+STRb7eJvJzizLyq+rh67XiDwDEbmRYJdSVsNBTkmm24cfXewVfmwRfmwVfnwBlj8brP+XpRzp1J9tbUp6vLR2fcfbtsW3OzqV/7GwTZcKs3m5uo7bibvZ16x4vX+fHuF3a7gN0F/xm7x3O/oLk630lvphloG/gCoVCUaXQB7hCoVBUKUZVQqmprcfJf3IGAGDbrxlV2DCBEsoZ7znD2XXhrc5OC3kCAEIRepuYKGWNnG12duMkbsD9299tZHvNlDSmzmBUpg2JxDVCGsmnmOQqnfZOa2Q/wsLjY90rv3N2U1wkmqrnCnd9HT1dduxk4huZ0zwspJWWRq980ysy3+zvpr15J6NIj+uY7OzIIRnKVPZ3u7YWh3ZUCxS16Dv9H7ykD4arHwQMsQywH9zcYbddPp/3YWd61JujE48aBAOJeqztLPBxcS352tnAxFGSrzcIvtYsI98AIC+eNPkok1778/VLzm5opqThz9evs/5Lxc7z/1HC18X0+AgbSpjrXmG09TfjX3N2XT09Wrx8pQdZNs8o0PA17NOnE6wfAHq/T5lmSWqps6fRMQ2ZDt7bSMwbfV4O+gauUCgUVQp9gCsUCkWVYlQllFA4groJBflixgncJikhZgrTZzEpUnuGUkLPZk7PACAjvFByWXpjnPH+i1jXCcz9MmvuFme/+PIrzm5poMSwYzed8COW25fFoyLLTkkMTJ/wDOkVgTkt9TxHnpIT8kj7REpHqQyvZ+9+SiBGJO1qFAEGABAJc/jSSa76b3qTK+cTmym7zJnWWOxPZX+3E4kEOouSg1+gTKAc2SV/9M0n7gnACbB1mmcbNJ8+lWLYykUAOcXn8GAeKUFaC9DEEaF9Ygdu+GRBvrj5HG4/9pXXKEOc+fqpLD//dWf3bL7RU1cG3DvtwysFX39A/oSnf8LZy/+NW6d5+PqJp5294/+yTGThtc6Of5Pff5TsSt+3ZLh85f5v7RM/5+xUpsvZe88vz9dHHvbyNdFGvkZEmvJtQh3+4lZ+mDNNnu/1aDmEIZlsjFlqjNltjPm9ONZqjHnSGPN68f+WwepQHHvQcR270LEdPwjyKnYXgHNLjt0C4Glr7RwATxc/K6oLd0HHdaziLujYjgsMKaFYa39pjJlZcvgjAM4q2ssAPAPg5qHqMqEQwvHCSu6OXevd8XnvfJez6ydwehU+yO3KclmvdhERATGb3uS048wWbo2EOiZqaKynxFAT4WpyrQiGqYlxBVkG0Ew9jlulvfrHP3r6EYsxeOHAQfZj5rQ5zj7plLc7u7ubATcNTc3O3rFzt7ONyF3e3MLAhV4RrAMAYTFdq61jXYmDvNaN4t7UxgrlM9l8RcdVonOY8sGgk/5O3w9lj3pkBSGbeMoEdBwJFFAUVI4ZJrzX5HMdgzRYqbGVfD1714fc8RPeSZmz/hw/vi701BWJMXps0/v5nXxPy1dFKfJyUT2DcdZJvt7PbdCuj9Fz619FAM3Uf/Dn67X3ka9f6ffh61eucXb3dyVfeQ2B+Br28nWR4OttdZSYzjvILRR//OYjzq6NlZdNJI5UDO2w1nYBQPH/SUOUV1QHdFzHLnRsxyCOuheKMebjxpi1xpi1vb0Hhj5BURWQ4/pW90VROchx3beve+gTFG8pjtQLZZcxZoq1tssYMwXAbr+C1tolAJYAwMknn2yjNU0AgGSSgTKpFN1QokLSqKtvcnZ9SQ6SeJheGw0RJk64awnzClx4+adZbz+95WNx/m7Jne9nnTDV2bu7Oa1J9nEqM3kSU9wCQPcByhUpseXSCWKLsRNn0+Om9+WXnN1/kMEDB/pZTzbH4INEgsvVzSWpbHOWU8CmZq6iZ9O8pnCI92ZbV2GY0sLjpQRHNK7GVHqf+2HAP29sxer13XB+mHlbfBxpDs8e64k1EvlTRuZjEmhs5bjOmzfvqPP1bj++/rXg69sFX39Snq+1kq9Zf75GFpNnl/yO15S9hHy9+Dfk613Rofl6+XLytf5G8vWhT3n5Gv+ekHqbVzv70fQHWUbk193WdfRyoTwC4FA41WIADw9SVlE90HEdu9CxHYMI4ka4EsBzAE42xmwzxlwP4N8BnGOMeR3AOcXPiiqCjuvYhY7t+EEQL5QFPn/6q2G3ZgxMuDDVHxCyRHKAOQOiIvfHwX1yGw/vlCwKOs9PaeYq8OvrmYNhxzaRj2GAU6yt27Y4+7TJzOUwdQaDeo7b3eHs/o0MImqNN3v60djMKdqmTax3ynGc3vUcoPafEfLIrj3MsZK3XOE2IkBnQEgoJiTuB7xZYetlkE+eK+Exke8hva8wLbWwlR1XmQzFD4G3dh8sSUiZ8/0Cdvxqr2Ckiyc+aCR9Kq13RL2qIGe7u2FWFab6A0u5Q3ryJO4SH73qbc4++FOxHLL5556qomC+kCkX+vD1a4KvF3+DVX3tvc4+7XTydaLg6z+cQ76+LviaK+FrxoevtYKvvzyFfJ2/lnzd8qHbnD37xb90dsqHrx8t4et/CTsv+Nrq4Sv7mw6wBqGh9AqFQlGl0Ae4QqFQVClGNRcKLNzuOWHLqcmUdqZ3rauhhPKz39EJvyXrXZGd00qvi5o4pyoxkWRgz+4tzs6n6FQ//UQG+4RFe3VNjC5u72AQ0L5urj73HvDuNCL2VsZEkdskIqSgpPAKkR4giSRX47OiImknU2KlPOv9vW1rpyuvMbwfMcN7EDciZ4wteAxEw2/B77aPLnB4JpSh5QdPvhWfMh7vDY/UMdxAo0GK+aR9lW2MdLMc7zl+HilHMeXsUHzdUJ6vPyrh6z+30uuiZs0lzo79nt/Vb5/5L85ecNv5zp5+IqWSRTVrnP349eJ+fHGes/etF3y9MBhfLxF8nXknOfPPGe628+47LnX2shy9ZC6VfO3352uj2JT8KkPvnQHzgLPXmGvYV3urs7czRsoDfQNXKBSKKoU+wBUKhaJKMbqbGhsgGimsQE9ooFdJs9hpxuQ5fTlguVK7d793J972Rna9Pkb5IBdikMGWHVuc3dFCp/oZs5mbJClS2T7/IvOzbO+i5NLYQGklGmUuBQBYt/EN8Ym/h3lhp4SE0tdPr5DmVq4+Z4UXStcuxljUN7LfkbA3XqaujtOwmMzjkqF3S66/x9kdkwrpZCPRkhybFcRIJ/NB0qTKfCteicJPpxl68+GRO6f4hvsMvyZPVQEkooomkSWMMY6vjwm+9gi+tgq+PiT4mi7h6/d8+DpZ8PW9O+htslTkSvyP2S87O5lhsNDzt/wZ2+7id75W8PWStpHy9aPObm69y9kdlhJPVPK13Z+vN9wg+Ho/U+ciQ6+ZK/u5s9ATk65ime1Mayuhb+AKhUJRpdAHuEKhUFQpRtcLBUDYFKZWkyeJzXblVEZ4ZkyZRm+RtUIOAYAew+maDTMoaEI7V4QnNHGqFq3hZqEzhYTSMIEr6ncuvdvZA6IfBxJ0qB9IeFM8RsUdnNzC9pLdnBb1x2Wf2O8/vMYdTHbt2sP2RM6F5mY20FTPtJoAELYiJ0Va7GwigpYm1rPMhJrCvY9U+mc7kRDz/mEmCDkCeFKpCr3Bu7PN0ZEV5pZv2hvI49eLoF3yU2D82vBLsjJCmLY2hK8qTOMnT/6VOx55YaR8pTeG5OtjTfRUabz+887+H0tJpGEC09RuX/pjZ/vxtWtnML7e8V3y9UN/zz59cpvkKzn6wV3ckafnw4KvmUH4eo/g61WCr98qz9fPXH+vs5/4FMpC38AVCoWiSqEPcIVCoahSjO6mxqGQ85ZoaqGEks2xG/EIvSlOmjXd2WtfpAQCAAeizL2RN0zT2DGV06JX1//a2X/xl9c4+7lneby/X+QpSXNT490735Q9d1ZfxvubFwGnPC0heq5MrWW9vXsolWTDnA52TKKdy4kAH5FPIZlgIEK/CDYAgGyeU7dMkp7+k6L0dDmugSvfqWyi5GoqhFoAxeHo9Jnz+3lKDJYHxN+vQ8omw5UP/DZHHuSUzvJ2IA8Yv6aDwqPZBKi3gsqR6TaI3V/4zv3tcze545NaKV08FbnO2SfN6nH2yS8yXwoArBPfyQVis6AVgq/vWc/8Rpsf/pGzn2v/O2f39z/pbC9fZW6uq531nSX+fI1fRL6+98/J12cFXy9Jsx9evtJO3Cf4GhZ8bRqEr1+nt8mkawRf2+nhk1rG437QN3CFQqGoUugDXKFQKKoU+gBXKBSKKsWoa+CH8la3tDMvb9awG8lQzNk1DYy6Kt1O7I03mUzmzHe9g+f3MYlOXSPdfrq2b3P2xg0b2HaOyWfE5tLoP0A9rrGNu1z39nqT40xoYKTXySed6uwXXvmDs1/6wxb29azznC23o9q0kbmQe8Wu8jJCLJmghgYAMzq4LlBbL6LjWnncRqitZ9OFyDBrvFFyI0YCQnsNkLVK6siHic9DR016jw93T7XhZ63y3zptrs9xv53k/ZryF8f90owH8mEcIUy7Qfy6Ah9bznjCHc8acjT5I8HXT5CvDz71mKeu0JtXOPvH75rp7DrB14cEXzu2k0sbN9CFMZu7k3V6+Mp8440i+vIwvn6Cf/vyz9jGN7ZIvn7X2dvOYnRoNHaDs2dtfN7Z/8+HrzcmfuBpW/L1QcHXf3xa8HXx5c7OXns7T36WOruEvoErFApFlUIf4AqFQlGlGFUJxdo88tnCdGNCK6OU+hOMfBrIMQFMWOStnn4883MDwIZ1dPXpHeA0rKGerofHn8jyWzcw0mr7DkZRvec972LbA5QoGsUWS63HMcLsjW5OtQAgkWLbsXomp2qaeLyzT2tk3/eIbdS2bH3F2f0JSjk9veyHzFk8wbLfADCjgedMauJ8MmroEpXO0BWpviidhHA0N5H38bPzy+FdusdZoEDOo5j/ehhN+CXVCiKtDH51UoIJ4FjpuZ++1R4BLFB0u5vQyuRL/f8l+BpjkqVFK+c7++RbbvbUtOEL5OtGX76ucPbWDT9xdkX5ulTw9R3l+frNxq85+/Y95NiWrZ929pOCrwcGPuzstuafOnuCJQ8B4FcNlzn7y02rnB01Fzo7bSnf1N97pTh7BcpB38AVCoWiSqEPcIVCoahSjKqEks9mcHBfYTpUK6IKU0lOR0yeXTKGU/32ViadAoANoU3O3t3NxDD7wpwiTWhgtOcpp9KLZdNWRllmxBZLPWK7tDlz5tCeRS1maxe9UwBg3TrOWfftFfl+45SIWhq4yrxtHad0O/dximWE901YJN6SCYJmlDiPTG/kinpNiN4mqSTvQT7PSLdMtljmaCooflN+n+3gA0dADjtR1QhzcvvV5PGgGW5NQY6X/mX451cK+WwG/d2H+MqtzFKGUoBZwO3RTJjfwSe+6uXrs4Kvs84TfF0l+cpoz1NOpfS3aevjzs7kWE/PgYudPWcOd6UfjK/zBF9vE3y99jrydflq8q/bw1dKNia0mLaQeqdM+z/OPtFs9rR9oYevjBZNJVlvPst6HV8HwZBv4MaY440xPzfGrDfGrDPGfLZ4vNUY86Qx5vXi/y1D1aU4dqDjOmYR1XEdPwgioWQB3GStfRuAdwP4lDHm7QBuAfC0tXYOgKeLnxXVAx3XsQsd13GCISUUa20XgK6ifdAYsx7AVAAfAXBWsdgyAM8AuLlMFQ6pVAqbNhamQNPnvM0drwlRQsmnOXWK1Igph7ABoLGRU56GJgYQnHIKk+g89VMmohnoZeBPXSvzEW/cxu2Qjp/GFfFZJ3O7pniMt+mE6SwDAD3dTIjz6nqutOcttZntPWLFWnjcJHOUkQ70UL6ZNJleK2/s4/HW473BTPviIllOXnixZNmGjfC+pYpl0ghXdFw92awkRICJdzd3f2+MTo9GUb61QOmyRT2+6bIDOrN4TgmQ93uuT9t+nir+e8+XtuEnT3mKZay1LwGV4GsXNm38CoASvl7NYJpFaX7XIg9czzI13GkdAG68kTn4Zz4q+PpO8vVawdeLBV+fkXz9S8ojt/yCXHz3l8nXxgfJ1/eU8PUlwddmwdeb/rE8Xy8UfF13xTXOPmcFeflkveQrg28OHk95CQBu8+HrBdnlzrYh3sPUApbBf6MshrWIaYyZCeA0AL8B0FF8CBx6GEzyOefjxpi1xpi1Bw/2lSuieIsx0nEtvMwrjjWMdFz395SP/lMcOwj8ADfGNAB4AMDnrC1xcBwE1tol1trTrbWny7dmxbGBSozrW7Cxk2IIVGJcW5prhj5B8ZYiEPOMMVEUvgwrrLUPFg/vMsZMsdZ2GWOmANjtX0MBA6ksfruxUGz6qWe443lwVdrIldc83SUOHGTObwDo6WEu4LbWec4+/9yznT3vT09x9uoHHxLXw6CXCRO4ljP1OE6FGpqanR3Osn+tk723bMos5hfureUX/uVXGKTT1Uf3ERvl9HHCZK7Ut59IeSQsZI+c2K3+NbHrNwBs3MnpXSzMcokk35wGxO3M5gvXfTBXWFmv1LiiNgHMLs7jffSGuT75QQ6TD/zSXweJVZHn+sS5IED9g9UbqIhHNgniDeOfD8Zv9/lOXy2ns3J83dGE337pHADA9PuZayS/4lvOvjfB3B2L6iRfGdwCAN//PnPwL/3APGef/E/MM/TsP1E+ePSnP3P2GfeSr5/sIl+fuZ183fVos7MX1wm+3lTC19cEX79Fnu0QfD0g+LpC8DX8NPn63InM9TIhco6zc/ZlZ79mf+9p+yc7mQ/m2jC3S/tk8lZnX/xtBuwc4utgCOKFYgDcAWC9tfZW8adHABzyeVkM4OEhW1McM9BxHdPQcR0nCPIG/l4UtrjoNMb8tnjsCwD+HcBqY8z1AN4AcFn50xXHKHRcxyYaoOM6bhDEC+VXAPzyj/7VcBpL5gw29BbSKO7NiRSKUU75Q2k63lsxhQiFvNOJ46ZwDeZ9f8EV6JooZYVZM5gf4YJLOX1Z8xADA/buZHtdvQwqSCaZ3jUmFum6E94Fu41buVqONKdntp2r6y2TGDCQF1E0hZlu8XiNKCPSdWZEbpjeHMsDQE1UpPKMcIj6DVfIM1GeY/OF/uVMuKLj6vFCCZDF9fAUsj4FKxSr4ud44puqNWDbwRxahg7EGexu+G1F5ynjvWV91tqKjGsqtx8begsKzN7l5IaNNjs7fxnToi5bzfweodAWT13HTaFkeub3/gxD4YIPfcDZ/7XqBWf/UfD15PXs0x2/IF+fEnw976kSvp4p+Lq7PF+vFXy925evn6ft4etnnH1rCV+jgq+3RcQ2cYKvPVFKSqsWsH/4e5SFhtIrFApFlUIf4AqFQlGlGFX/r1TOYENP4Tfj4V9x3jdvBnfnmRyjp0VdlN2bMpl5TQBgSjtXh088QaSatXR+7xKpW5euomzy0m9fZZ9EHhZP6gHL3zYrdu3JxZsgkQtxmhQBp5NZ4emSDfF4jbzjYqabTIv2QjweER4p4TynjABgk2K3HfBv0TzrChva6UyhXlPpXCieHXmGnvJ7dq4fZDcZvw3nh50JJUhG1pLzg8gjw05q6+saM1jNwvPEE+Pk45FSwRQpyWaDDRcU+TrjT9xxydekeAW8Icodaw7exLSoAIDsO535m1/z+x1/N4vME8WX/ug5Zz/zG/L1CcHXy+nYgqcXCv4sZ5nHSvh6+Tzy9b40eQnB1zsFX+HD14zgK66mRwnuu472YXxd5uwsFjj7mgWs627DHCkL72LAlGjBA30DVygUiiqFPsAVCoWiSjGqEkoOBn3FtKlPv8SNhV//I1NEnvtO5kw48TgGt2zexLwFAPD+d3FD0hrhaXEwzanQ6p9w9frlV3c4eyArchIIiSIU5e9ZXgQRhQylCilvAEAuT6+XlJAuMjkeN4arySkIrxDLNiIRIXuI9JR1dVy5jkHkvgWQEzO0nNgYOif+kM2w77HG5kJ/QpUedmooMnBFKgaeHCd+u8mU/M0/B0mwrYLLlvDVPUoylYh+eYNp/PoRoHvBOuJbrfTe8cgpMveKf63DRg5t6AtdCwC46aUvuuM3fpnfn3NvpXdXX467Xm3+Cr1OAOD972Kek+b3MSjvV78mX59czd2nXv75N5w9kGV+JESucuZKwVc8IPjaIPnqFR/uW8Vdbi41K529RpLJCBlE8BULhfZ4n2h7leBr/f3OPoyvC1hXzlAqXr78bmdfPp99X33jp3jyp+9DOegbuEKhUFQp9AGuUCgUVYpRlVAikQja2gvTpO79nI507e9x9rOvcAeMXGaGODsGiYki5aoJUxJ5fi3zDzz+M65kp/J0zkeE5UOh8r9huRRXsq2QU/J577RIyiAyb0k0InYWCosgpDCvIyKOh8MsL5N+hUX/QlY49gPICU+ZvJzqieng5MmUoRqbCvYf4957WUn47mTj8fgYJJ1skI2CfXO0VlBAGPbOO37w6EjBKvVNc3L0d+GRkHy9YD/585+Cr03/yO/d8rfLOKFvQeLDk+9w9u9E3p7n197l7H+b8iVn9+UkXylpBOGrzKGEfEmmzIXk0Jp7hGwi+AofviIujscoaTQKqSO8krLMYXxdI/lK6fbKa5kDpr2VfJ3ZRNsrIBP6Bq5QKBRVCn2AKxQKRZViVCUUY4yTDaJiU+NsktOULbuYujjVv97Z7/+zkzx11TYzZ0Cv2MT3F79Z6+yk5fQpk+V0Jh7n9CUvnO0HBpiTQCIsPDxMaZYJMVuLCxnE4+khbBPn1LC2lgEDETGFywjPkYP9TI2Zy3sjcFJZsSFsC4MrOqbQbhCRQ4liSl5bEmBQWZSXRzz5Oub6eXX4yyNejxQ/DxG/E4bu32Hyi+9OxgGq8o38kR42UioqOcGzo4+Px07lU8YcBi9fhWeG+Z4zv/5B8vWzD3ND3zdL+ProFUwP2/xr5kGaJvn6IlPTzs8+6Ow1cXp25BeIfEW3S74y0MiY3znbmmu9F7VS2NfeII4Lvl4t7DXka82jgq8Tmp2dWc4UsB9OftTZK/L3eNu+XATv/IgSzKSffdbZDdez7UtFCu1/Q3noG7hCoVBUKfQBrlAoFFUKfYArFApFlWJ0NzO0FvlDO6ZLF7gwNek06Kqzuy/l7JdeYyQlAJw/QD34oKVWtH0/7XgD3fGyA6w3mWK9dXVC1xLJs2QZI3KRh0TSG8DrLmiF1m3Fb2NUaO59GbohpsVWbVIPl66JUufuF4l8AKChmVp380Qm+0pnWe61P9AtM1p0gcykvfWMHMwHPtfXa85f9/bAdyd6H23dN2xxsEYOIaB6HCCblZ9M7qeZD+ZF2Omnv4827D7kizum//Ae8ur8Rfw+/2j5D539HeFhO7OUrw8Kvtayrlv3c+u1eMN/OHv1ALXrdIrrV3WryZNQlBp4XvDVhngDTQlfjeTrKmGLpG/RNby+j8znRa0x5GvoQeFmLCI0V9wh1peS7BMAXPYQ3QKbb77J2enc4HwdDPoGrlAoFFUKfYArFApFlWKUJRQwSspyqhEOi62KLKc8Mtf2lt3eXemXrmZynA+cdbqzN+/Y4+yBnIx8ElOkGrothmO060QSqZjYYT5xkFMn6eIHAFZIHFHhsheOhMueExZRXjJhVmKgr+xxWb65pdXTdlsHXSn37ut2ds9ebhvV8wZjuGbPmlXsdKUTghOeCb909/NxDxxRkONh9QbYpiygF6FfIi2/nez9cpf7tTdoOi7fBOajDMHXiwRfc6uucfaHr2YippoV5OvqwHzld3EgR6lkwZV0wTP3Mkf2Az58zX6MfE3+J/lq53v5epnhdax+gHxdJPiaE+esWCUkmAUiIvti8hVLePyyMO37S/ja8Xm6Uu7dd6Gzh+TrINA3cIVCoahS6ANcoVAoqhSjKqGEI2G0NjcDAJJJTrH6E1yFjYXFtmRCngiJyE0A+OXzjLbavIMr3r39nIZ19zGPsHDMQH298E4RUYnxONuIiKlaTS1Xg8Mh76p2ROw0nRO/h1khgxhhW8u6chn2NZ1hB2trOB1sb2tzdks7JRMASAtPnlRMRFyKZFX5CKe1/cnC/cjbykZi1iKB2cWpvl/ab98U4INqKAFycg9XNvE5fnhW8vIyhq9S4pvuvPw1wDchl7/HTecoyym9kYn4cXMhUdPZi950x/t/QIkitkrIJnPF9+r3Xr4+6+Hr2c7+m/7bnL1c8LXrVubt/kk9nxVyW8H4dYKvIX7n4x8jxxaV8PW+KKM66+vn8Xie+wtcIWSQhQuF19idIhHW/Iucfbl5wNntn+HWjbOeEFs9AngoRilof+w/nX1e/Bxnl+PrYBjyDdwYU2OMed4Y84oxZp0x5kvF463GmCeNMa8X/28ZsjXFMQMd1zELo+M6fhBEQkkB+IC19k9R2Hf0XGPMuwHcAuBpa+0cAE8XPyuqBzquYxMWOq7jBkNKKLYQVXJoyTVa/GcBfATAWcXjywA8A+DmQevKW6SK04K4+OlIidXnqMi/mxWzH1uSBzhUSxlkq/A8CYnV5GyGUyEpxySTSWf3i2RRMtewlFPqY5zW1ArvlMI5rDdWw3Nq69i/dJqr2nu76S2SB49HxPZQLU3cbqmjtdnZkyd7V7V7+hkocLBnv7P7enuc3dzKc/bu2QugsM1aJcfVsym9D4LuJuZ31mA5xA/BU8Jvt7OgqcR9E0f5SStyuzM/2SRY2x65yDepur/+Y62tyLi25C0uLfK1z4evD4RFsMk6cXK45N1Q8LVrx7edvTyyiGUyS5yZzbLeCwRfH7+IfB3YzjYuXkPu/VDwFR8r4etKJpSK3UC59mN1Dzs7nWZSre90f9fZl115qbOvEFmxWv6Wk5mnBF+n3+Tl6wX9TND11e+T+31/8X1nN/+Ybe89by+GQqBFTGNM2BjzWwC7ATxprf0NgA5rbRcAFP+f5HPux40xa40xazMDB8oVUbxFqNS4jlqHFYFQqXFNHOwuV0RxDCHQA9xam7PWzgMwDcAZxphThzhFnrvEWnu6tfb0aF3TEXZTcTRQqXE9ah1UHBEqNa61ja1Dn6B4SzEsLxRrbY8x5hkA5wLYZYyZYq3tMsZMQeHXflDk83mkEoXpUFxsq1QnepHPcOVVpjHIw+s5IT0p8iJ/SjYtPD5ybEPmF5G2zAcuJZT9+ylJdIs+NTVQ3gCACcJZv0lMG2vElkm5PKWOiBEeLWKLppTImxCPmLLlswO9nrazAzynr2cfr0l4tNTEOZ1MHgoKKklqPtJx9YVnll9e05h7WFKQ8uW8AopPXbK8Xw6SoEEyfn/yOb3TRx+RffUEHclqgt2CQQOPymGk42r9+LqYZa7IMf91TvD1/lUM8ClUJn9D5jjrYsHXh3Lccd6KXP6PC75+6G7yNbKYfHvofOYoR4Z92tnl5WvvhYKvGZ5vV7C93CXk1WJzBcusYT/uTFLq+NzfPck+Cb5efruXrz0X+fD1LvL19et4E2fu93rQlEMQL5SJxpjmol0L4IMA/gDgEQCHhnIxgIfLVqA4JqHjOmYR0XEdPwjyBj4FwDJTSOsVArDaWvuYMeY5AKuNMdcDeAPAZUexn4rKQ8d1bCIK4Oc6ruMDxh7FvBiHNWbMHgD9AIZeXh17aMexc90zrLUTK1VZcVy34ti6xtHCsXbNFRtbHddj6prLjuuoPsABwBizdjwufI2H6x4P11iK8XDN4+EaS1Et16y5UBQKhaJKoQ9whUKhqFK8FQ/wJUMXGZMYD9c9Hq6xFOPhmsfDNZaiKq551DVwhUKhUFQGKqEoFApFlWJUH+DGmHONMa8ZYzYaY8ZkNjRjzPHGmJ8bY9YX03l+tnh8zKbzHA/jCoy/sdVxPfbHddQklGJgwQYA5wDYBuAFAAusta+OSgdGCcUw5SnW2peMMY0AXgRwEYBrAHRba/+9SIYWa+2g2eCqAeNlXIHxNbY6rtUxrqP5Bn4GgI3W2k3W2jSAVSikuBxTsNZ2WWtfKtoHAawHMBWFa11WLLYMhS/IWMC4GFdg3I2tjmsVjOtoPsCnAnhTfN5WPDZmYYyZCeA0AIHTeVYhxt24AuNibHVcq2BcR/MBbsocG7MuMMaYBgAPAPictXYsJ0IfV+MKjJux1XGtAozmA3wbgOPF52kAdviUrWoYY6IofBFWWGsPbcOxq6i1HdLcjjxN67GFcTOuwLgaWx3XKhjX0XyAvwBgjjFmljEmBuAKFFJcjikYYwyAOwCst9beKv40VtN5jotxBcbd2Oq4VsG4jnY2wvMBfBtAGMBSa+2/jFrjowRjzJkA/huFlPuHss9/AQVNbTWA6Sim87TWjok9q8bDuALjb2x1XI/9cdVITIVCoahSaCSmQqFQVCn0Aa5QKBRVCn2AKxQKRZVCH+AKhUJRpdAHuEKhUFQp9AGuUCgUVQp9gCsUCkWVQh/gCoVCUaX4//jO9bLmNXe+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dataset != 'mnist':\n",
    "    # cifar datasets have color channels, need to arrange color channels\n",
    "    images_prepped = np.transpose( images.squeeze().detach().numpy() , (1,2,0)) # squeeze out batch dimension and arrange color channels\n",
    "    perturbation = newsaliency * EPS\n",
    "    perturbation_prepped = np.transpose(perturbation.detach().numpy(), (1,2,0))\n",
    "    new_images_prepped = np.transpose( new_images.squeeze().detach().numpy(), (1,2,0))\n",
    "else:\n",
    "    images_prepped = images.squeeze().detach().numpy()\n",
    "    perturbation = newsaliency * EPS\n",
    "    perturbation_prepped = perturbation.squeeze().detach().numpy()\n",
    "    new_images_prepped = new_images.squeeze().detach().numpy()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images_prepped)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(perturbation_prepped, vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images_prepped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
