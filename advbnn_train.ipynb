{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train, val = random_split(train_dataset,[50000,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency,images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros(28, 28)\n",
    "    combined_mean = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.flatten()\n",
    "    combined_mean = combined_mean.flatten()\n",
    "    champ         = saliencies[success.index(max(success))].flatten()\n",
    "    return combined_med, combined_mean, champ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "saliencies, success = generate_saliency(0.18,1, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = .18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_med, combined_mean, champ = combine_saliencies(saliencies,success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, combined_med,images))\n",
    "new_images = otcm(images, EPS, combined_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACECAYAAACJbXCEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATGklEQVR4nO3dfZRWxX0H8O9vl2V3eWcDB5dlw0vkJZicgAGFFg+maEFjjqbtMdJGqW6yatSQBqvEeHJiqi05WtKTE5tKCyWtHG2q9og2RpEAkYq8xGIqyJuvQHgNRhZBZNnpH/t4752bvXfvnWfufZ559vs5x8PMM3PvzD6zOz7725k7opQCERG5p6rUHSAiIjOcwImIHMUJnIjIUZzAiYgcxQmciMhRnMCJiBxV1AQuInNEZKeI7BGRhbY6RaXFca1cHNvKIqbrwEWkGsAuAJcC2AdgM4C5Sqnt9rpHeeO4Vi6ObeXpVcS1FwDYo5R6AwBE5FEAVwKI/GboLbWqDn2LaJJs+ADv40N1WiKKU49rrz59Vc3ABi/fUd/hpatO6b/kxZVF1euO6X3yaD+pcHsmfWs/8jucbXs/alyBlGPbu6avqqsb5L9w4pSf7levV44ri6rXHdP75NF+UuH2DPvWhnePKqWGhl8vZgJvArA3kN8H4MK4C+rQFxfKrCKaJBs2qtVxxanHtWZgA0bd8E0vf/pT/jdi7av6N2xcWVS97pjeJ4/2kwq3Z9K3/d9+sLtmUo1tXd0gTJ38NS9ftX6rl+6YPEmrG1cWVa87pvfJo/2kwu2Z9u159djbXb1ezASeiIi0AmgFgDr0ybo5yklwXHsNGFzi3pAtwXGtrR1Y4t5Qd4r53W8/gOZAfkThNY1SaolSaopSakoNaotojnKSelx79WFYzBHdjm1wXHvXcFzLXTGfwDcDGCsio9H5TXANgD+30isqpdTj2lHfkSrk8JE0YYNgWVxbcfcMl5mGV+LajAsZxbFxXYJYfLqxPXEqVcjB60c4bDBjUmTdYFlcW3H3DJfFhldi+hLXZvi6pO+LrevwwmNd1jOewJVS7SJyK4BnAVQDWKaU2mZ6PyoPHNfKxbGtPEXFwJVSPwPwM0t9oTLBca1cHNvKkvkfManyVZ2qigxVpAlTJF2hkkV4w/Se4WttlUXVC9cNptMsfUykX722aiL463+aMEVcKCLuujhJwxum9wxfa6ssql64btJQC7fSExE5ihM4EZGjOIETETmKMXAqWtwyQtNdiqax5DTyjqXb2LEZd08bW/o1McsI08SWg0xjyWnkHUs37WecpMsI+QmciMhRnMCJiBzFEAoVLW4ZYRzTpXNp2ApbZBHCsfE+ZSpmGWEc06VzadgKW2QRwrHxPiVuq+g7EBFRSXACJyJyFCdwIiJHMQZOmTI9GMHGAQfhsmK20ufRRpDJFnzrW+ljpDoYwTDOnPTphMVspc+jjSAbW/C1fiRumYiIygoncCIiRzGEQkUz3YmZJrySdMlhOZ+laevJjFFl5bITM014JemSQ9OQjWl7aa6z9WTG2LAMd2ISEVUWTuBERI7iBE5E5CjGwBM4cfU0L/3eaP3/eU3ffzHv7jjF1sHBSZluwS9mu3rWW91tbd23ydbBwUml2YL/zmz//Rn+P+1aWe8UbdrY6p7m/ibvGz+BExE5ihM4EZGjGEIpaPuSHyYZfuserWzpyAe89NDqWq1s3Vf7aPlq8XfDnVVm/3+8+akWLd/vLf8+w9e+q5V1vPKaURt5SXogb7humqV6eT/9MIuQiemSylKFU9IcyBusm2apXlwIo9fokV56+3U1WtkXzt/ipX/Yslkru2LXZXobovy+qUOhVhoj2w86+O+jtPzgPR/491/3v4nuAZjt7uQncCIiR3ECJyJyFCdwIiJH9agYePW5o73069efo5WtuvZ+L91YHY4rRscZP1f/gZavgnjpDqhw9UR2XP1gZNma2+q0/O4P/a/jgQ2ztbJxLVtQCiZP0usqH3WdLaZb8EutHOLeaZ6kZ2PJYds107T8+Qv82PL02p1a2d1Ddnjpe49O0MqmNbyp9y3wM9oR+NlNZf6bkUUvHRut5Q+d6O+lT/9iiFY2bNPJ1E13+wlcRJaJyGEReTXwWoOIrBKR3YV/B6dumUqK41q5OLY9R5IQynIAc0KvLQSwWik1FsDqQp7cshwc10q1HBzbHqHbEIpS6pciMir08pUALi6kfwJgLYA7bXbMVPX4c710v6X6krvZH9vgpa8bsD90pf9r6fd/e55WsvW9EYnbP3aPv7yp910HtbJ+NacT38fEZ8e9peXbYupmOa42whF5hAlsHegQZPpUwTSirvvoQIesxtbGzsRwyKRtlB8SPDRvqlZ2zohjXvrzTWsj7/nons9q+aUvXeSlH77kodj+3PupGV565/2f1spUjb8kOHyfLz9/Y6KyWBPOaNnGxVuTXRdg+kfMYUqpA4X0QQDDDO9D5YXjWrk4thWo6FUoSikFRP+1TkRaRWSLiGw5g2w/gZI9acb1bNv7OfaMihU3tvx5dYvpBH5IRBoBoPDv4aiKSqklSqkpSqkpNaiNqkblwWhcq/v3za2DZCzR2PLn1S2mywhXApgHYFHh3yet9Sil4NJAAJj52Cte+psNO8LVI33pdf9vPqf/Qv/Gbd+7L/F9anDUS6vn9bK4mHSZsDKupksFTeqlYSsGbetEHtM4v2G/ix5b06WCcfVO3v5JL90yJvm286Ub/Dh306rQE0If2+ilvzvjhvj+nPT784lHon/j+F7r+Vp+HPwt+t+DXnbuDD22naUkywgfAbABwHgR2SciLej8JrhURHYDuKSQJ4dwXCsXx7bnSLIKZW5E0SzLfaEccVwrF8e253B+J+ZlT76s5W8a9EZk3Tfb/V2TVy35a61s9L+946XThEyo8zDdqLBJHmGCpDsqiwlhJK1r+lRD0/cp00ON+9WjY/Ik//4GT8sLO3anvtvw6pHRYZPgjsqZN7ZqZeOe2hR5XZq+Ja2b5kAJG+8TDzUmIqpwnMCJiBzFCZyIyFHOx8C/Nkh/ElhHRD0A+JMf+3Hv5kX6YcTt4cqUWEd9R2SsN83TCE2V05ME08TZ404nMmnvo6301pw4FRnrTfM0wqA//Uf964x7AuDFLV/10nXPRMe885DmAOK404lM24usZ3R3IiIqOU7gRESOcj6E8m6H/ivZwKq6iJrAHX/5Uy+9Yu3lWln1jre99NnfvWepdz1DeBmhqZx3Jlprw/Qw5qh7hOsmfV+yXkZoKhhuuGvI8sTX/Xz+RC9d+0zR3ehS0sMm0hzGHHWPcN1USwy5jJCIqLJwAicichQncCIiRzkfA5++4nYtv/3aH0XWndv/kJ9+/F+1sjWn/Nj5Tc9GP8Hs40/rS7Vqn9kcUZO6ksUJNUmlibGbLgc0/ZpMl1uW6oDjsLjYbrBs2h03aWVX3LE28p5zhm/3M7/Wy4KHBbd36J9Dq3HES+97bqRW1hRaPpw0tp1mK31cvaTLLZPGx/kJnIjIUZzAiYgcxQmciMhRzsfAxyzcoOUvn3qVl/75hOSHjsyq90/j2H3Vj6MrXhVdBABf2TvTS+/8gX66ff//eClxfypFmlhyFtvsbTymFbBzeo+txwpEXWd9K32MNFvLg2UDH9Z/BpZd6P+8fGXm2sTtPz0u4cLwCaH81/XsmOf8E+0/ec9RrSzu8Rpxseykce4094zCT+BERI7iBE5E5CjnQyhhvT7vLyH63Gx9yVKf+fu99FPjV2bS/r80r/PSxx94Viu7YPICLz3uH/STg9oPHkKlSLoNvJyeItgd01N/TCW9T6Yn8oQk3QaeJmww9jb/AOL1w/Qlfzu+4y8VvOGidcjCG3+81Evf+ZlJWtm6v5/mpcOhH1NJ3xueyENEVOE4gRMROYoTOBGRo0QplVtjA6RBXSizcmsvrKp/fy8tw4ZE1jvTOEjL759/JrLuvPEbtfztDTu9dAei39tNp/WTSG7+0a1eunHxi+HqVm1Uq3FcHYs+CiWl2jFNqum+W7ouK2L7ulFfLDyWtSsmMenu2re9Rf6tZYtx6sBea+Ma9/Oa5lGopqfSBB2fO03LD3jEj0kf+vofaGVXtkTHy+8Zuk3L/+3R8V467nSgu4fs0PKzvtzipas+PKuV2dgiH/aLF+7+lVJqSvh1fgInInIUJ3AiIkdV3DLCOB1tbX4mmA6p2qPnm1+Ivucvm8dr+TXn+L/qnfdP+q9ri87xn1x4Qa0eXhn3xV1+1xZHt1eO0pzIkzRsYLozMU1ZnCwOY05zna3QT1FSnMiTNGxgujOx394PIsuGvnxSK3vx5amR95y9vkbLH71xupduu1i/z3Xn6eHRoDeu8T/7jmv9VWS9MOMTeaLuV/QdiIioJLqdwEWkWUTWiMh2EdkmIvMLrzeIyCoR2V34d3D23SVbOK6ViePasyT5BN4OYIFSaiKAaQBuEZGJABYCWK2UGgtgdSFP7uC4ViaOaw/SbQxcKXUAwIFCuk1EXgPQBOBKABcXqv0EwFoAd2bSyzLWvnef/kIg/8x/6cubFt1cPqf3ZDmupnHuPGLLUX1J00bWywHj2o7TUd+R6biaxrlNlxGaXpcmtjzkIf9ppm0XfybxdQ9f8pCX/u6M6BO84thYXpnqj5giMgrAZAAbAQwrfLMAwEEAwyKuaQXQCgB16GPcUcpOsePaawB/Gy9HxY5rbe3AHHpJxUj8R0wR6QfgcQDfUEodD5apzt1AXe5aUUotUUpNUUpNqUFtUZ0l+2yMa68+fXPoKaVhY1x713Bcy12iT+AiUoPOb4YVSqknCi8fEpFGpdQBEWkEcDirTuZBpn7aSw/4wW+0su1P+0sFTw7Xd11NOV9fc1gl/s/FFwcnP1Bi28FGL/1xHI2paU9W45rFgbxZHIYcd0/Tpyiafr1xyxbTfr1ZjavtA3nD9zE9OHjXP+vLBkc+4aerF+hP+fybMfqSvyrxD8G4dfvYyDbuPaqfDLHiiT/y0iNwMlzdv3/MYQ+mX692TXcVREQALAXwmlIquEJ5JYB5hfQ8AMlnKyo5jmtl4rj2LEk+gf8hgGsB/J+IbC28dheARQB+KiItAN4GcHUmPaSscFwrE8e1B0myCmU9EPmUl9I9mYqKwnGtTBzXnqVHbaWPc7a22ktfMeQVrWzFbc8lvk+N+Pc5o/R4+a4zH3rplu3XamVjbvNDknGHqbogaYw47+V4cfc0PXw5fK2tuH5SpTqRJ64sTWzXxnbypubfavm1S/0geDh2Pa2uGjo/v2nyf2ol179zkZeePvB1rWzo1uifUhvb5XkiDxFRheMETkTkKIZQCoK/9jx60WStbFGrv7zo/uuXxd6nBn7Y5Du7/kwrq3vQ3/Ay8L/1XZmuh02SynpnYpqwjOnTAePaTNO3pMsKk9brqO+IrJc1K0/WSxFqCdbtO0c/IHzmF1q9dP1f7dfKEDqY4Ypdl3npHZtHaWW1gTNPfvN3+tNL67Epsm9JD3y2cTA0P4ETETmKEzgRkaM4gRMROYox8C6cPXJEyzff5+d/eN+EcPVIA7Gn+0oVyMaSuFI/mdDGAcvh+5rGx5Nel/UyQhtL4vJ4MmHdU358+uy7k7Sy2ev1fOHBjQCATwTSQPzXa/pYAePHEXAZIRFRZeEETkTkKIZQyDobOyrT7Ha0sRMyTX9MD6LIIkyT5zJCGzsq45YDpimLY3pwsulBFDaeKhjGZYRERBWOEzgRkaM4gRMROYoxcLIui+3ySZ8OWMxTBU3LTLa9d1dm0rdyXUYYFHdCTVx7ptcVU2Zj27utfnMZIRFRheEETkTkKIZQyHk2DpAo5nAJ07BJHNeeRpgFGwdIFHO4hI2nBcZdx6cREhH1YJzAiYgcxQmciMhRopTKrzGRIwDeBjAEwNHcGo7XE/syUik11NbNOK7d4rja01P70uXY5jqBe42KbFFKTcm94S6wL/aUU//ZF3vKqf/si44hFCIiR3ECJyJyVKkm8CUlarcr7Is95dR/9sWecuo/+xJQkhg4EREVjyEUIiJH5TqBi8gcEdkpIntEZGGebRfaXyYih0Xk1cBrDSKySkR2F/4dnEM/mkVkjYhsF5FtIjK/VH2xgeOq9aVixpbjqvWlLMc1twlcRKoBPAjgMgATAcwVkYl5tV+wHMCc0GsLAaxWSo0FsLqQz1o7gAVKqYkApgG4pfBelKIvReG4/p6KGFuO6+8pz3FVSuXyH4DpAJ4N5L8F4Ft5tR9odxSAVwP5nQAaC+lGADtL0KcnAVxaDn3huHJsOa7ujGueIZQmAHsD+X2F10ptmFLqQCF9EMCwPBsXkVEAJgPYWOq+GOK4RnB8bDmuEcppXPlHzADV+b/R3JbliEg/AI8D+IZS6ngp+1LJSvFecmyzx3HNdwLfD6A5kB9ReK3UDolIIwAU/j2cR6MiUoPOb4QVSqknStmXInFcQypkbDmuIeU4rnlO4JsBjBWR0SLSG8A1AFbm2H6UlQDmFdLz0BnbypSICIClAF5TSi0uZV8s4LgGVNDYclwDynZccw78Xw5gF4DXAXy7BH94eATAAQBn0BnTawHwMXT+9Xg3gOcBNOTQjxno/FXr1wC2Fv67vBR94bhybDmu7o4rd2ISETmKf8QkInIUJ3AiIkdxAicichQncCIiR3ECJyJyFCdwIiJHcQInInIUJ3AiIkf9P/Mm6vIyTbnRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((combined_med*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((champ*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Train with Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 25/25\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "temp_sals, success = generate_saliency(EPS,1,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"Batch %s\" % counter)\n",
    "\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "    images_med   = torch.vstack(images_med).reshape(-1,28, 28)\n",
    "    images_mean  = torch.vstack(images_mean).reshape(-1,28, 28)\n",
    "    images_champ = torch.vstack(images_champ).reshape(-1,28, 28)\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open('images_med_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('images_mean_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1 \n",
    "    if counter > 8:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN with Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\".\") if \"images_med\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images).int()\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = random_split(train_dataset,[51024,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(epochs = 10, K = 100, modelname = \"AdvBNN.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss  += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        # model.eval()     # Optional when not using Model Specific layer\n",
    "        val_loss = 0.\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            val_loss += svi.evaluate_loss(images, labels)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 72.8608: Val_Loss = 70.427\n",
      "Epoch 1: Loss = 68.552: Val_Loss = 67.6154\n",
      "Epoch 2: Loss = 66.6034: Val_Loss = 66.389\n",
      "Epoch 3: Loss = 65.7868: Val_Loss = 66.6741\n",
      "Epoch 4: Loss = 65.8724: Val_Loss = 66.3151\n",
      "Epoch 5: Loss = 65.568: Val_Loss = 66.1872\n",
      "Epoch 6: Loss = 65.6789: Val_Loss = 66.2749\n",
      "Epoch 7: Loss = 65.6447: Val_Loss = 66.4574\n",
      "Epoch 8: Loss = 65.7479: Val_Loss = 66.6923\n",
      "Epoch 9: Loss = 65.6441: Val_Loss = 66.1903\n",
      "Saved 100 models\n"
     ]
    }
   ],
   "source": [
    "train_adversarial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
