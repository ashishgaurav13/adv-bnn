{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os,sys\n",
    "import common\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "\n",
    "import re, pickle\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type = 'lrt'  # 'bbb' or 'lrt'\n",
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "lr_start = 0.001\n",
    "num_workers = 1\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1  \n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "outputs = 10\n",
    "inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform_mnist)\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                           sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLeNet(common.ModuleWrapper):\n",
    "    '''The architecture of LeNet with Bayesian Layers'''\n",
    "\n",
    "    def __init__(self, outputs, inputs, priors, layer_type='lrt', activation_type='softplus'):\n",
    "        super(BBBLeNet, self).__init__()\n",
    "\n",
    "        self.num_classes = outputs\n",
    "        self.layer_type = layer_type\n",
    "        self.priors = priors\n",
    "\n",
    "        if layer_type=='lrt':\n",
    "            BBBLinear = common.layers.BBB_LRT_Linear\n",
    "            BBBConv2d = common.layers.BBB_LRT_Conv2d\n",
    "        elif layer_type=='bbb':\n",
    "            BBBLinear = common.layers.BBB_Linear\n",
    "            BBBConv2d = common.layers.BBB_Conv2d\n",
    "        else:\n",
    "            raise ValueError(\"Undefined layer_type\")\n",
    "        \n",
    "        if activation_type=='softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type=='relu':\n",
    "            self.act = nn.ReLU\n",
    "        else:\n",
    "            raise ValueError(\"Only softplus or relu supported\")\n",
    "\n",
    "        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = common.layers.FlattenLayer(5 * 5 * 16)\n",
    "        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n",
    "        self.act3 = self.act()\n",
    "\n",
    "        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n",
    "        self.act4 = self.act()\n",
    "\n",
    "        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs, inputs, layer_type='lrt', activation_type='softplus'):\n",
    "        '''\n",
    "        Base LeNet model that matches the architecture of BayesianLeNet with randomly \n",
    "        initialized weights\n",
    "        '''\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # initialization follows the BBBLeNet initialization, changing\n",
    "        # BBBLinear and BBBConv2D layers to nn.Linear and nn.Conv2D\n",
    "        \n",
    "        if activation_type == 'softplus':\n",
    "            self.act = nn.Softplus\n",
    "        elif activation_type == 'relu':\n",
    "            self.act = nn.ReLU\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inputs, 6, 5, padding=0, bias=True)\n",
    "        self.act1 = self.act()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0, bias=True)\n",
    "        self.act2 = self.act()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120, bias=True)\n",
    "        self.act3 = self.act()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
    "        self.act4 = self.act()\n",
    "        self.fc3 = nn.Linear(84, outputs, bias=True)\n",
    "\n",
    "\n",
    "    def sample(self, bbbnet):\n",
    "        '''\n",
    "        Takes in a BBBLeNet instance and copies the structure into a LeNet model.\n",
    "        Replaces the BBBLinear and BBBConv2D that uses sampling in their forward steps\n",
    "        with regular nn.Linear and nn.Conv2d layers whose weights are initialized by \n",
    "        sampling the BBBLeNet model.\n",
    "        '''    \n",
    "        ### store activation function used by BNN, only relu and softplus  currently supported\n",
    "        self.act1 = bbbnet.act()\n",
    "        self.act2 = bbbnet.act()\n",
    "        self.act3 = bbbnet.act()\n",
    "        self.act4 = bbbnet.act()\n",
    "\n",
    "        ### maxpool\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=bbbnet.pool1.kernel_size, stride=bbbnet.pool1.stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=bbbnet.pool2.kernel_size, stride=bbbnet.pool2.stride)\n",
    "        \n",
    "        ### Create Convolution layers\n",
    "        self.conv1 = nn.Conv2d(bbbnet.conv1.in_channels, bbbnet.conv1.out_channels, bbbnet.conv1.kernel_size,\n",
    "                                stride=bbbnet.conv1.stride, padding=bbbnet.conv1.padding, dilation=bbbnet.conv1.dilation,\n",
    "                                groups=bbbnet.conv1.groups)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(bbbnet.conv2.in_channels, bbbnet.conv2.out_channels, bbbnet.conv2.kernel_size,\n",
    "                        stride=bbbnet.conv2.stride, padding=bbbnet.conv2.padding, dilation=bbbnet.conv2.dilation,\n",
    "                        groups=bbbnet.conv2.groups)\n",
    "        \n",
    "        # follows the procedure for sampling in the forward methods of BBBConv and \n",
    "        # BBBLinearforward to create a fixed set of weights to use for the sampled model\n",
    "\n",
    "        conv1_W_mu = bbbnet.conv1.W_mu\n",
    "        conv1_W_rho = bbbnet.conv1.W_rho\n",
    "        conv1_W_eps = torch.empty(conv1_W_mu.size()).normal_(0,1)\n",
    "        conv1_W_sigma = torch.log1p(torch.exp(conv1_W_rho))\n",
    "        conv1_weight = conv1_W_mu + conv1_W_eps * conv1_W_sigma\n",
    "        if bbbnet.conv1.use_bias:\n",
    "            conv1_bias_mu = bbbnet.conv1.bias_mu\n",
    "            conv1_bias_rho = bbbnet.conv1.bias_rho\n",
    "            conv1_bias_eps = torch.empty(conv1_bias_mu.size()).normal_(0,1)\n",
    "            conv1_bias_sigma = torch.log1p(torch.exp(conv1_bias_rho))\n",
    "            conv1_bias = conv1_bias_mu + conv1_bias_eps * conv1_bias_sigma\n",
    "        else:\n",
    "            conv1_bias = None\n",
    "        self.conv1.weight.data = conv1_weight.data\n",
    "        self.conv1.bias.data = conv1_bias.data\n",
    "\n",
    "\n",
    "        conv2_W_mu = bbbnet.conv2.W_mu\n",
    "        conv2_W_rho = bbbnet.conv2.W_rho\n",
    "        conv2_W_eps = torch.empty(conv2_W_mu.size()).normal_(0,1)\n",
    "        conv2_W_sigma = torch.log1p(torch.exp(conv2_W_rho))\n",
    "        conv2_weight = conv2_W_mu + conv2_W_eps * conv2_W_sigma\n",
    "        if bbbnet.conv2.use_bias:\n",
    "            conv2_bias_mu = bbbnet.conv2.bias_mu\n",
    "            conv2_bias_rho = bbbnet.conv2.bias_rho\n",
    "            conv2_bias_eps = torch.empty(conv2_bias_mu.size()).normal_(0,1)\n",
    "            conv2_bias_sigma = torch.log1p(torch.exp(conv2_bias_rho))\n",
    "            conv2_bias = conv2_bias_mu + conv2_bias_eps * conv2_bias_sigma\n",
    "        else:\n",
    "            conv2_bias = None\n",
    "        self.conv2.weight.data = conv2_weight.data\n",
    "        self.conv2.bias.data = conv2_bias.data\n",
    "        \n",
    "        ### Create Linear Layers\n",
    "        self.fc1 = nn.Linear(bbbnet.fc1.in_features, bbbnet.fc1.out_features, bbbnet.fc1.use_bias)\n",
    "        self.fc2 = nn.Linear(bbbnet.fc2.in_features, bbbnet.fc2.out_features, bbbnet.fc2.use_bias)\n",
    "        self.fc3 = nn.Linear(bbbnet.fc3.in_features, bbbnet.fc3.out_features, bbbnet.fc3.use_bias)\n",
    "\n",
    "        fc1_W_mu = bbbnet.fc1.W_mu\n",
    "        fc1_W_rho = bbbnet.fc1.W_rho\n",
    "        fc1_W_eps = torch.empty(fc1_W_mu.size()).normal_(0,1)\n",
    "        fc1_W_sigma = torch.log1p(torch.exp(fc1_W_rho))\n",
    "        fc1_weight = fc1_W_mu + fc1_W_eps * fc1_W_sigma\n",
    "        if bbbnet.fc1.use_bias:\n",
    "            fc1_bias_mu = bbbnet.fc1.bias_mu\n",
    "            fc1_bias_rho = bbbnet.fc1.bias_rho\n",
    "            fc1_bias_eps = torch.empty(fc1_bias_mu.size()).normal_(0,1)\n",
    "            fc1_bias_sigma = torch.log1p(torch.exp(fc1_bias_rho))\n",
    "            fc1_bias = fc1_bias_mu + fc1_bias_eps * fc1_bias_sigma\n",
    "        else:\n",
    "            fc1_bias = None\n",
    "        self.fc1.weight.data = fc1_weight.data\n",
    "        self.fc1.bias.data = fc1_bias.data\n",
    "\n",
    "        fc2_W_mu = bbbnet.fc2.W_mu\n",
    "        fc2_W_rho = bbbnet.fc2.W_rho\n",
    "        fc2_W_eps = torch.empty(fc2_W_mu.size()).normal_(0,1)\n",
    "        fc2_W_sigma = torch.log1p(torch.exp(fc2_W_rho))\n",
    "        fc2_weight = fc2_W_mu + fc2_W_eps * fc2_W_sigma\n",
    "        if bbbnet.fc2.use_bias:\n",
    "            fc2_bias_mu = bbbnet.fc2.bias_mu\n",
    "            fc2_bias_rho = bbbnet.fc2.bias_rho\n",
    "            fc2_bias_eps = torch.empty(fc2_bias_mu.size()).normal_(0,1)\n",
    "            fc2_bias_sigma = torch.log1p(torch.exp(fc2_bias_rho))\n",
    "            fc2_bias = fc2_bias_mu + fc2_bias_eps * fc2_bias_sigma\n",
    "        else:\n",
    "            fc2_bias = None\n",
    "        self.fc2.weight.data = fc2_weight.data\n",
    "        self.fc2.bias.data = fc2_bias.data\n",
    "\n",
    "        fc3_W_mu = bbbnet.fc3.W_mu\n",
    "        fc3_W_rho = bbbnet.fc3.W_rho\n",
    "        fc3_W_eps = torch.empty(fc3_W_mu.size()).normal_(0,1)\n",
    "        fc3_W_sigma = torch.log1p(torch.exp(fc3_W_rho))\n",
    "        fc3_weight = fc3_W_mu + fc3_W_eps * fc3_W_sigma\n",
    "        if bbbnet.fc3.use_bias:\n",
    "            fc3_bias_mu = bbbnet.fc3.bias_mu\n",
    "            fc3_bias_rho = bbbnet.fc3.bias_rho\n",
    "            fc3_bias_eps = torch.empty(fc3_bias_mu.size()).normal_(0,1)\n",
    "            fc3_bias_sigma = torch.log1p(torch.exp(fc3_bias_rho))\n",
    "            fc3_bias = fc3_bias_mu + fc3_bias_eps * fc3_bias_sigma\n",
    "        else:\n",
    "            fc3_bias = None\n",
    "        self.fc3.weight.data = fc3_weight.data\n",
    "        self.fc3.bias.data = fc3_bias.data\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method follow the order of BayesianLeNet\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 5 * 5 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BBBLeNet(outputs, inputs, priors, layer_type, activation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=n_epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [LeNet(outputs, inputs, layer_type, activation_type) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load('models/model-cnn.pt')):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train without Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2d3e559bb0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlr_sched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %g: Loss = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'metrics'"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "epochs = 20\n",
    "modelname = \"CNN_v2\"\n",
    "\n",
    "# Train with BinaryCrossEntropy\n",
    "val_losses = []\n",
    "model      = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "optimizer  = Adam(model.parameters(), lr=lr_start)\n",
    "lr_sched   = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "loss_fcn   = torch.nn.NLLLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        y    = torch.nn.Softmax()(model(images))\n",
    "        loss = loss_fcn(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    lr_sched.step()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g\" % (epoch, total_loss))\n",
    "\n",
    "torch.save(model, \"models/%s.pt\" % modelname)\n",
    "print(\"Saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_MNIST,self).__init__()\n",
    "        #Here, we are plementing those layers which are having learnable parameters.\n",
    "        #Start implementation of Layer 1 (C1) which has 6 kernels of size 5x5 with padding 0 and stride 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=(5,5))\n",
    "        \n",
    "        #Start implementation of Layer 3 (C3) which has 16 kernels of size 5x5 with padding 0 and stride 1\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16,kernel_size = (5,5))\n",
    "        \n",
    "        #Start implementation of Layer 5 (C5) which is basically flattening the data \n",
    "            \n",
    "        self.L1 = nn.Linear(256, 120)\n",
    "        \n",
    "        #Start implementation of Layer 6 (F6) which has 85 Linear Neurons and input of 120\n",
    "        \n",
    "        self.L2 = nn.Linear(120,84)\n",
    "        \n",
    "        #Start implementation of Layer 7 (F7) which has 10 Linear Neurons and input of 84\n",
    "        \n",
    "        self.L3 = nn.Linear(84,10)\n",
    "        \n",
    "        #We have used pooling of size 2 and stride 2 in this architecture \n",
    "        \n",
    "        self.pool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        #We have used tanh as an activation function in this architecture so we will use tanh at all layers excluding F7.\n",
    "        self.act = nn.Tanh()\n",
    "        \n",
    "    #Now we will implement forward function to produce entire flow of the architecture.\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        #We have used tanh as an activation function in this architecture so we will use tanh at all layers excluding F7.\n",
    "        x = self.act(x)\n",
    "        #Now this will be passed from pooling \n",
    "        x = self.pool(x)\n",
    "        #Next stage is convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        #next we will pass from conv3, here we will not pass data from pooling as per Architecture \n",
    "        \n",
    "        \n",
    "        #Now the data should be flaten and it would be passed from FC layers. \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.L1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.L2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.L3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 58.0535, test_loss: 20.5085, test_acc: 0.9515\n",
      "Epoch [1], train_loss: 16.8563, test_loss: 11.4889, test_acc: 0.9725\n",
      "Epoch [2], train_loss: 10.9548, test_loss: 8.8285, test_acc: 0.9769\n",
      "Epoch [3], train_loss: 8.2382, test_loss: 8.4571, test_acc: 0.9786\n",
      "Epoch [4], train_loss: 6.5166, test_loss: 6.9045, test_acc: 0.9822\n",
      "Epoch [5], train_loss: 5.5779, test_loss: 6.3462, test_acc: 0.9829\n",
      "Epoch [6], train_loss: 4.7086, test_loss: 5.5551, test_acc: 0.9867\n",
      "Epoch [7], train_loss: 4.1655, test_loss: 5.3448, test_acc: 0.9861\n",
      "Epoch [8], train_loss: 3.3988, test_loss: 5.9889, test_acc: 0.9857\n",
      "Epoch [9], train_loss: 3.1838, test_loss: 5.0938, test_acc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train = datasets.MNIST(root='.',train=True,transform=ToTensor(),download=True)\n",
    "test  = datasets.MNIST(root='.',train=False,transform=ToTensor(),download=True)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "model = LeNet_MNIST()\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "\n",
    "def accuracy(outputs,labels):\n",
    "    _,pred = torch.max(outputs,dim=1)\n",
    "    return torch.tensor(torch.sum(pred==labels).item()/len(pred))\n",
    "\n",
    "def training_step(batch):\n",
    "            image,label = batch\n",
    "            out = model(image)\n",
    "            loss = loss_fn(out,label)\n",
    "            return loss\n",
    "    \n",
    "def test_step(batch):\n",
    "            image,label = batch\n",
    "            out = model(image)\n",
    "            loss = loss_fn(out,label)\n",
    "            acc = accuracy(out, label)\n",
    "            return {'test_loss': loss.detach(), 'test_acc': acc}\n",
    "    \n",
    "def training_result(outputs):\n",
    "            batch_loss = [i['test_loss'] for i in outputs]\n",
    "            epoch_loss = torch.stack(batch_loss).mean()\n",
    "            batch_acc = [i['test_acc'] for i in outputs]\n",
    "            epoch_acc = torch.stack(batch_acc).mean()  \n",
    "            return {'test_loss': epoch_loss.item(), 'test_acc': epoch_acc.item()}\n",
    "    \n",
    "def epoch_result(epoch,result):\n",
    "            print(\"Epoch [{}], train_loss: {:.4f}, test_loss: {:.4f}, test_acc: {:.4f}\".format(\n",
    "                epoch, result['train_loss'], result['test_loss'], result['test_acc']))\n",
    "\n",
    "                        \n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return 'cpu'\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "train_dl = DeviceDataLoader(train_loader, device)\n",
    "test_dl = DeviceDataLoader(test_loader, device)\n",
    "to_device(model, device);\n",
    "\n",
    "def evaluation(model,test_loader):\n",
    "    model.eval()\n",
    "    output = [test_step(batch) for batch in test_loader] \n",
    "    return training_result(output)\n",
    "\n",
    "def fit(epochs,model,lr,train_loader,test_loader,optim = optimizer):\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(batch)\n",
    "            train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluation(model,test_loader)\n",
    "        result['train_loss'] = torch.stack(train_loss).mean().item()\n",
    "        epoch_result(epoch,result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "            \n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "\n",
    "history = fit(num_epochs,model,lr, train_dl, test_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/CNN.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = next(iter(test_loader))\n",
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 5.093798637390137, 'test_acc': 0.9867483973503113}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adv Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=transform_mnist)\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in range(len(sampled_models)):\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([1])])\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(images, EPS, images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "        saliencies += [images.grad.sign().view(32, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign())]\n",
    "            saliencies += [images.grad.sign().view(32, 32)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    \n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros_like(saliencies[0])\n",
    "    combined_mean = torch.zeros_like(saliencies[0])\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    for i in range(combined_med.shape[0]):\n",
    "        for j in range(combined_med.shape[1]):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med\n",
    "    combined_mean = combined_mean\n",
    "    champ         = saliencies[success.index(max(success))]\n",
    "    return combined_med, combined_mean, champ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(32, 32).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((c_med*EPS).reshape(32, 32).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(32, 32).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(32, 32).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((champ*EPS).reshape(32, 32).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(32, 32).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sals, success = generate_saliency(0.18,0,images)\n",
    "c_med, c_mean, champ = combine_saliencies(sals,success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 0.18\n",
    "SAVE_DIR = \"./\"\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "counter     = 1\n",
    "successes   = []\n",
    "orgTarget   = [] \n",
    "falseTarget = []\n",
    "\n",
    "skip = 0\n",
    "stop = 3\n",
    "for data in train_loader:\n",
    "    if counter < skip:\n",
    "        counter +=1\n",
    "        continue\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    print(\"\\r Batch %s\" % counter, end=\"\")\n",
    "    for i in range(images.shape[0]): #\n",
    "        print(i)\n",
    "        # the real target\n",
    "        target_org = labels[i].item()\n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(set(range(target_len)) - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:,:,:]\n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        orgTarget.append(target_org)\n",
    "        falseTarget.append(target)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "\n",
    "    images_med   = (torch.vstack(images_med)*255).type(torch.uint8).detach()\n",
    "    images_mean  = (torch.vstack(images_mean)*255).type(torch.uint8).detach()\n",
    "    images_champ = (torch.vstack(images_champ)*255).type(torch.uint8).detach()\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "\n",
    "\n",
    "#     with open(SAVE_DIR + 'train_images_med_%s.pickle'   % counter, 'wb') as handle:\n",
    "#         pickle.dump(images_med, handle, protocol  = pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(SAVE_DIR + 'train_images_mean_%s.pickle'  % counter, 'wb') as handle:\n",
    "#         pickle.dump(images_mean, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(SAVE_DIR + 'train_images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "#         pickle.dump(images_champ, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1\n",
    "    if counter > stop:\n",
    "        print(\"braking\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.05/\") if d.startswith(\"train_images_med\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.05//\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:8949.661, TrainAcc:0.973, ValLoss:8815.307, ValAcc:0.975, KL:35952.331\n",
      "Epoch:1, TrainLoss:8331.902, TrainAcc:0.975, ValLoss:8980.602, ValAcc:0.973, KL:35115.529\n",
      "Epoch:2, TrainLoss:7778.730, TrainAcc:0.978, ValLoss:8261.433, ValAcc:0.977, KL:34635.471\n",
      "Epoch:3, TrainLoss:7666.301, TrainAcc:0.978, ValLoss:8226.040, ValAcc:0.975, KL:34100.247\n",
      "Epoch:4, TrainLoss:7431.978, TrainAcc:0.979, ValLoss:8325.451, ValAcc:0.976, KL:33713.422\n",
      "Epoch:5, TrainLoss:7143.370, TrainAcc:0.981, ValLoss:8008.452, ValAcc:0.976, KL:33158.879\n",
      "Epoch:6, TrainLoss:6964.101, TrainAcc:0.980, ValLoss:8216.730, ValAcc:0.975, KL:32673.102\n",
      "Epoch:7, TrainLoss:6826.943, TrainAcc:0.982, ValLoss:8213.214, ValAcc:0.975, KL:32222.349\n",
      "Epoch:8, TrainLoss:6733.833, TrainAcc:0.982, ValLoss:7880.064, ValAcc:0.977, KL:31912.593\n",
      "Epoch:9, TrainLoss:6353.305, TrainAcc:0.983, ValLoss:7803.279, ValAcc:0.980, KL:31599.982\n",
      "Epoch:10, TrainLoss:6362.029, TrainAcc:0.983, ValLoss:7619.875, ValAcc:0.979, KL:31359.545\n",
      "Epoch:11, TrainLoss:6460.006, TrainAcc:0.984, ValLoss:7891.625, ValAcc:0.979, KL:31118.374\n",
      "Epoch:12, TrainLoss:6209.604, TrainAcc:0.984, ValLoss:7932.998, ValAcc:0.980, KL:31090.918\n",
      "Epoch:13, TrainLoss:6209.339, TrainAcc:0.984, ValLoss:7619.793, ValAcc:0.981, KL:30886.282\n",
      "Epoch:14, TrainLoss:6010.746, TrainAcc:0.985, ValLoss:7381.291, ValAcc:0.979, KL:30659.194\n",
      "Epoch:15, TrainLoss:6052.156, TrainAcc:0.985, ValLoss:7377.170, ValAcc:0.980, KL:30414.547\n",
      "Epoch:16, TrainLoss:5863.056, TrainAcc:0.986, ValLoss:7180.469, ValAcc:0.982, KL:30154.216\n",
      "Epoch:17, TrainLoss:5834.112, TrainAcc:0.985, ValLoss:7690.705, ValAcc:0.979, KL:29989.518\n",
      "Epoch:18, TrainLoss:5740.364, TrainAcc:0.986, ValLoss:7222.262, ValAcc:0.982, KL:30025.836\n",
      "Epoch:19, TrainLoss:5629.161, TrainAcc:0.986, ValLoss:7364.658, ValAcc:0.981, KL:29862.409\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn-adv-med0.05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training Champ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.05/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.05/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:5748.950, TrainAcc:0.985, ValLoss:7297.304, ValAcc:0.978, KL:28318.823\n",
      "Epoch:1, TrainLoss:5604.724, TrainAcc:0.986, ValLoss:6953.296, ValAcc:0.982, KL:28425.381\n",
      "Epoch:2, TrainLoss:5523.580, TrainAcc:0.986, ValLoss:7027.331, ValAcc:0.980, KL:28654.955\n",
      "Epoch:3, TrainLoss:5468.697, TrainAcc:0.987, ValLoss:6825.225, ValAcc:0.982, KL:28711.553\n",
      "Epoch:4, TrainLoss:5469.264, TrainAcc:0.987, ValLoss:7226.810, ValAcc:0.982, KL:28803.916\n",
      "Epoch:5, TrainLoss:5562.123, TrainAcc:0.986, ValLoss:6808.629, ValAcc:0.981, KL:29043.431\n",
      "Epoch:6, TrainLoss:5333.678, TrainAcc:0.988, ValLoss:6939.312, ValAcc:0.982, KL:29171.288\n",
      "Epoch:7, TrainLoss:5457.837, TrainAcc:0.987, ValLoss:7125.187, ValAcc:0.981, KL:29041.696\n",
      "Epoch:8, TrainLoss:5298.181, TrainAcc:0.988, ValLoss:7094.158, ValAcc:0.981, KL:28945.895\n",
      "Epoch:9, TrainLoss:5208.028, TrainAcc:0.988, ValLoss:7566.662, ValAcc:0.981, KL:28774.984\n",
      "Epoch:10, TrainLoss:5101.284, TrainAcc:0.988, ValLoss:6604.559, ValAcc:0.982, KL:28617.199\n",
      "Epoch:11, TrainLoss:5127.127, TrainAcc:0.988, ValLoss:6991.694, ValAcc:0.982, KL:28627.920\n",
      "Epoch:12, TrainLoss:4991.676, TrainAcc:0.989, ValLoss:7022.069, ValAcc:0.983, KL:28430.669\n",
      "Epoch:13, TrainLoss:5131.023, TrainAcc:0.989, ValLoss:7135.683, ValAcc:0.982, KL:28430.843\n",
      "Epoch:14, TrainLoss:5060.428, TrainAcc:0.989, ValLoss:6791.852, ValAcc:0.984, KL:28366.075\n",
      "Epoch:15, TrainLoss:4952.064, TrainAcc:0.989, ValLoss:6901.688, ValAcc:0.983, KL:28299.291\n",
      "Epoch:16, TrainLoss:4930.176, TrainAcc:0.989, ValLoss:6891.714, ValAcc:0.983, KL:28138.205\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:17, TrainLoss:4892.830, TrainAcc:0.989, ValLoss:6610.577, ValAcc:0.984, KL:28137.388\n",
      "Epoch:18, TrainLoss:4568.035, TrainAcc:0.991, ValLoss:6671.523, ValAcc:0.983, KL:28074.439\n",
      "Epoch:19, TrainLoss:4529.751, TrainAcc:0.991, ValLoss:6699.654, ValAcc:0.985, KL:27905.277\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"model-cnn-adv-champ0.05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.18/\") if d.startswith(\"train_images_med\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.18/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:6328.549, TrainAcc:0.982, ValLoss:6058.759, ValAcc:0.983, KL:28087.034\n",
      "Epoch:1, TrainLoss:6107.050, TrainAcc:0.983, ValLoss:6719.151, ValAcc:0.980, KL:28581.676\n",
      "Epoch:2, TrainLoss:5874.656, TrainAcc:0.985, ValLoss:6654.492, ValAcc:0.980, KL:28847.441\n",
      "Epoch:3, TrainLoss:5688.220, TrainAcc:0.986, ValLoss:6139.992, ValAcc:0.982, KL:29071.327\n",
      "Epoch:4, TrainLoss:5593.068, TrainAcc:0.986, ValLoss:6237.446, ValAcc:0.982, KL:29128.288\n",
      "Epoch:5, TrainLoss:5347.973, TrainAcc:0.988, ValLoss:6199.795, ValAcc:0.984, KL:28983.227\n",
      "Epoch:6, TrainLoss:5237.087, TrainAcc:0.988, ValLoss:6407.690, ValAcc:0.983, KL:28864.655\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:7, TrainLoss:5344.323, TrainAcc:0.987, ValLoss:6852.710, ValAcc:0.982, KL:28852.665\n",
      "Epoch:8, TrainLoss:4919.120, TrainAcc:0.989, ValLoss:5830.979, ValAcc:0.985, KL:28871.863\n",
      "Epoch:9, TrainLoss:4996.001, TrainAcc:0.989, ValLoss:5803.739, ValAcc:0.985, KL:28731.249\n",
      "Epoch:10, TrainLoss:4837.497, TrainAcc:0.990, ValLoss:6024.384, ValAcc:0.984, KL:28611.868\n",
      "Epoch:11, TrainLoss:4694.949, TrainAcc:0.990, ValLoss:5649.666, ValAcc:0.985, KL:28469.260\n",
      "Epoch:12, TrainLoss:4801.222, TrainAcc:0.990, ValLoss:5768.590, ValAcc:0.985, KL:28344.251\n",
      "Epoch:13, TrainLoss:4729.761, TrainAcc:0.990, ValLoss:5757.487, ValAcc:0.984, KL:28213.859\n",
      "Epoch:14, TrainLoss:4639.682, TrainAcc:0.990, ValLoss:6005.585, ValAcc:0.985, KL:28102.292\n",
      "Epoch:15, TrainLoss:4601.280, TrainAcc:0.991, ValLoss:5672.935, ValAcc:0.986, KL:27979.231\n",
      "Epoch:16, TrainLoss:4719.523, TrainAcc:0.990, ValLoss:6162.788, ValAcc:0.984, KL:27888.139\n",
      "Epoch:17, TrainLoss:4502.378, TrainAcc:0.991, ValLoss:5498.535, ValAcc:0.986, KL:27789.447\n",
      "Epoch:18, TrainLoss:4662.367, TrainAcc:0.990, ValLoss:5686.018, ValAcc:0.986, KL:27695.607\n",
      "Epoch:19, TrainLoss:4589.298, TrainAcc:0.990, ValLoss:5620.519, ValAcc:0.987, KL:27622.799\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-med0.18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training (eps= 0.18, champ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_back = transforms.Compose([transforms.Resize((28,28))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs    = [d for d in os.listdir(\"mnist_adv_CNN_0.18/\") if d.startswith(\"train_images_champ\")]\n",
    "dir_ord = sorted([int(re.findall(\"[0-9]+\",d)[0]) for d in dirs])\n",
    "dirs    = sorted(dirs, key=lambda x: dir_ord.index(int(re.findall(\"[0-9]+\",x)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv_CNN_0.18/\" + d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "images  = transform_back(images)\n",
    "images  = images[:,0,:,:]\n",
    "\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])\n",
    "\n",
    "train, val = random_split(train_dataset,[50000+images.shape[0],10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 10 # try 1\n",
    "plt.figure()\n",
    "plt.imshow(img[0][ind,0,:,:], cmap='gray')\n",
    "plt.title(\"Class: %s\" % img[1][ind].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model-cnn-adv.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with ELBO and Adam (Bayes by Backprop + LRT)\n",
    "    criterion = common.metrics.ELBO(len(trainset))\n",
    "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss, train_acc, train_kl = common.train_model(net, optimizer, criterion, train_loader, \n",
    "                                                             num_ens=train_ens, beta_type=beta_type, epoch=epoch,\n",
    "                                                             num_epochs=epochs)\n",
    "        valid_loss, valid_acc = common.validate_model(net, criterion, valid_loader, num_ens=valid_ens, \n",
    "                                                      beta_type=beta_type, epoch=epoch, num_epochs=epochs)\n",
    "        lr_sched.step(valid_loss)\n",
    "        print('Epoch:%d, TrainLoss:%.3f, TrainAcc:%.3f, ValLoss:%.3f, ValAcc:%.3f, KL:%.3f' % (\n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
    "    # Sample k models from the posterior\n",
    "    nn_dicts = []\n",
    "    for i in range(K):\n",
    "        sample_model = LeNet(outputs, inputs, layer_type, activation_type)\n",
    "        sample_model.sample(net)\n",
    "        nn_dicts += [sample_model.state_dict()]\n",
    "    # Save the models\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, TrainLoss:5160.353, TrainAcc:0.987, ValLoss:6292.461, ValAcc:0.982, KL:27207.201\n",
      "Epoch:1, TrainLoss:5070.413, TrainAcc:0.988, ValLoss:6765.373, ValAcc:0.981, KL:27369.277\n",
      "Epoch:2, TrainLoss:5380.323, TrainAcc:0.987, ValLoss:6474.751, ValAcc:0.984, KL:27846.429\n",
      "Epoch:3, TrainLoss:5080.209, TrainAcc:0.988, ValLoss:6512.425, ValAcc:0.983, KL:28110.186\n",
      "Epoch:4, TrainLoss:5079.756, TrainAcc:0.988, ValLoss:6503.413, ValAcc:0.982, KL:28056.009\n",
      "Epoch:5, TrainLoss:5125.309, TrainAcc:0.988, ValLoss:6360.784, ValAcc:0.983, KL:28119.157\n",
      "Epoch:6, TrainLoss:4976.825, TrainAcc:0.989, ValLoss:6396.181, ValAcc:0.983, KL:28137.779\n",
      "Epoch:7, TrainLoss:4958.926, TrainAcc:0.989, ValLoss:5970.411, ValAcc:0.984, KL:28019.700\n",
      "Epoch:8, TrainLoss:4828.485, TrainAcc:0.989, ValLoss:7112.312, ValAcc:0.982, KL:27940.775\n",
      "Epoch:9, TrainLoss:4892.673, TrainAcc:0.989, ValLoss:6161.597, ValAcc:0.985, KL:27869.739\n",
      "Epoch:10, TrainLoss:4908.512, TrainAcc:0.989, ValLoss:6152.966, ValAcc:0.985, KL:27877.910\n",
      "Epoch:11, TrainLoss:4800.579, TrainAcc:0.989, ValLoss:6350.641, ValAcc:0.984, KL:27829.217\n",
      "Epoch:12, TrainLoss:4879.934, TrainAcc:0.989, ValLoss:6092.205, ValAcc:0.984, KL:27851.558\n",
      "Epoch:13, TrainLoss:4812.423, TrainAcc:0.990, ValLoss:6182.821, ValAcc:0.984, KL:27876.379\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:14, TrainLoss:4837.644, TrainAcc:0.989, ValLoss:6446.608, ValAcc:0.983, KL:27734.907\n",
      "Epoch:15, TrainLoss:4491.932, TrainAcc:0.991, ValLoss:5714.341, ValAcc:0.986, KL:27660.305\n",
      "Epoch:16, TrainLoss:4419.275, TrainAcc:0.991, ValLoss:5804.200, ValAcc:0.986, KL:27538.684\n",
      "Epoch:17, TrainLoss:4411.149, TrainAcc:0.991, ValLoss:5859.347, ValAcc:0.986, KL:27408.838\n",
      "Epoch:18, TrainLoss:4434.558, TrainAcc:0.991, ValLoss:5629.992, ValAcc:0.987, KL:27295.888\n",
      "Epoch:19, TrainLoss:4322.917, TrainAcc:0.992, ValLoss:5877.044, ValAcc:0.986, KL:27184.804\n",
      "Saved 1000 models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 20, K = 1000, modelname = \"models/model-cnn-adv-champ0.18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
