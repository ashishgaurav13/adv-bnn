{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os, pickle\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        x = self.logsoftmax(x)\n",
    "        return x\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fwd = torch.nn.Sequential(\n",
    "            torch.nn.Linear(ni, nh), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh, nh), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh, no),\n",
    "            torch.nn.LogSoftmax(dim=-1))\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.fwd(x)\n",
    "        return x\n",
    "class DeepNNwBN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(DeepNNwBN, self).__init__()\n",
    "        self.fwd = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(ni),\n",
    "            torch.nn.Linear(ni, nh), torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(nh),\n",
    "            torch.nn.Linear(nh, nh), torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(nh),\n",
    "            torch.nn.Linear(nh, no),\n",
    "            torch.nn.LogSoftmax(dim=-1))\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.fwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = torch.load(\"models/FFNN_ep10.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(train_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,9,(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_targets(targets):\n",
    "    ftargets = []\n",
    "    for target in targets:\n",
    "        ftargets.append(torch.tensor(list(set(range(10)) - set([target])))[torch.randint(0,9,(1,))][0])\n",
    "    return torch.stack(ftargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = torch.nn.NLLLoss()\n",
    "model = FFNN\n",
    "EPS = 0.05\n",
    "success_rate = 0\n",
    "adv_images_lst = []\n",
    "\n",
    "for images, targets in train_loader:    \n",
    "    loss_fcn.zero_grad()    \n",
    "    # Collect noises (saliencies)\n",
    "    falseTargets = false_targets(targets)\n",
    "    # target = torch.tensor([1])\n",
    "    # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "    adv_images      = images.clone()\n",
    "    # adv_images.grad = None\n",
    "    adv_images = adv_images.view(-1,28**2)\n",
    "    adv_images.requires_grad = True\n",
    "    preds = model(adv_images)\n",
    "    loss  = loss_fcn(preds,falseTargets)\n",
    "    loss.backward()\n",
    "    new_images = otcm(adv_images, EPS, adv_images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = torch.argmax(model(new_images), dim=1)\n",
    "    success_rate += torch.sum(new_class != targets)\n",
    "    adv_images_lst.extend((new_images.reshape(-1,28, 28)*255).type(torch.uint8).detach())\n",
    "success_rate = success_rate/len(train_loader.dataset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_FF   = {'images': torch.stack(adv_images_lst),  'labels': train_loader.dataset.targets.detach()}\n",
    "with open(\"\" + 'train_images_FF0.05.pickle', 'wb') as handle:\n",
    "    pickle.dump(images_FF, handle, protocol  = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = torch.nn.NLLLoss()\n",
    "model = FFNN\n",
    "EPS = 0.05\n",
    "success_rate = 0\n",
    "adv_images_lst = []\n",
    "\n",
    "for images, targets in test_loader:    \n",
    "    loss_fcn.zero_grad()    \n",
    "    # Collect noises (saliencies)\n",
    "    falseTargets = false_targets(targets)\n",
    "    # target = torch.tensor([1])\n",
    "    # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "    adv_images      = images.clone()\n",
    "    # adv_images.grad = None\n",
    "    adv_images = adv_images.view(-1,28**2)\n",
    "    adv_images.requires_grad = True\n",
    "    preds = model(adv_images)\n",
    "    loss  = loss_fcn(preds,falseTargets)\n",
    "    loss.backward()\n",
    "    new_images = otcm(adv_images, EPS, adv_images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = torch.argmax(model(new_images), dim=1)\n",
    "    success_rate += torch.sum(new_class != targets)\n",
    "    adv_images_lst.extend((new_images.reshape(-1,28, 28)*255).type(torch.uint8).detach())\n",
    "success_rate = success_rate/len(test_loader.dataset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_FF   = {'images': torch.stack(adv_images_lst),  'labels': test_loader.dataset.targets.detach()}\n",
    "with open(\"\" + 'test_images_FF0.05.pickle', 'wb') as handle:\n",
    "    pickle.dump(images_FF, handle, protocol  = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-b97050e1fcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampled_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Compute loss w.r.t. an incorrect class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampled_models' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect noises (saliencies)\n",
    "# EPS = 0.18\n",
    "saliencies      = []\n",
    "how_many_fooled = []\n",
    "# target = torch.tensor([1])\n",
    "falseTarget = torch.tensor([0])\n",
    "smax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "for model in sampled_models:\n",
    "    # Forward pass\n",
    "    # Compute loss w.r.t. an incorrect class\n",
    "    # Note that we just have to ensure this class is different from targets\n",
    "    # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "    adv_images = images.clone()\n",
    "    adv_images = adv_images.view(-1,28**2)\n",
    "    adv_images.grad = None\n",
    "    adv_images.requires_grad = True\n",
    "    preds = smax(model(adv_images))\n",
    "    loss  = loss_fcn(preds,falseTargets)\n",
    "    loss.backward()\n",
    "    # Compute adversarial example\n",
    "    new_images = otcm(adv_images, EPS, adv_images.grad.sign())\n",
    "    # Forward pass on adv. example\n",
    "    new_class = torch.argmax(smax(model(new_images)), dim=1) \n",
    "    if targets != new_class:\n",
    "        # How many models can this adv. example fool? \n",
    "        how_many_fooled += sun([torch.argmax(smax(m(new_images)), dim=1) != targets for m in sampled_models])\n",
    "        saliencies += [images.grad.sign().view(28, 28)]\n",
    "# print(\"\\nFinished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = torch.nn.NLLLoss()\n",
    "def generate_saliency(EPS,false_target,images,targets, model):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        adv_images      = images.clone()\n",
    "        adv_images.grad = None\n",
    "        adv_images.requires_grad = True\n",
    "        preds = model(adv_images)\n",
    "        loss  = loss_fcn(preds,targets)\n",
    "        loss.backward()\n",
    "        new_images = otcm(images, EPS, adv_images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = torch.argmax(model(new_images), dim=1)\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency,images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros(28, 28)\n",
    "    combined_mean = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.flatten()\n",
    "    combined_mean = combined_mean.flatten()\n",
    "    champ         = saliencies[success.index(max(success))].flatten()\n",
    "    return combined_med, combined_mean, champ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((combined_med*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((champ*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, \n",
    "                                          generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 8"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "SAVE_DIR = \"mnist_adv/\"\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"\\r Batch %s\" % counter, end=\"\")\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "\n",
    "    images_med   = (torch.vstack(images_med).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_mean  = (torch.vstack(images_mean).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_champ = (torch.vstack(images_champ).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open(SAVE_DIR + 'train_images_med_%s.pickle'   % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol  = pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'train_images_mean_%s.pickle'  % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'train_images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1 \n",
    "    if counter > 8:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 16"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "SAVE_DIR = \"mnist_adv/\"\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"\\r Batch %s\" % counter, end=\"\")\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "    images_med   = (torch.vstack(images_med).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_mean  = (torch.vstack(images_mean).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_champ = (torch.vstack(images_champ).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open(SAVE_DIR + 'test_images_med_%s.pickle'   % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'test_images_mean_%s.pickle'  % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'test_images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1\n",
    "    if counter > 16:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
