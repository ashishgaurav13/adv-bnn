{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, pickle\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"models/model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency,images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros(28, 28)\n",
    "    combined_mean = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.flatten()\n",
    "    combined_mean = combined_mean.flatten()\n",
    "    champ         = saliencies[success.index(max(success))].flatten()\n",
    "    return combined_med, combined_mean, champ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_med' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b650047a9371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_med\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_med' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((combined_med*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'champ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d68c9fbd3c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchamp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'champ' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((champ*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, \n",
    "                                          generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 8"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "SAVE_DIR = \"mnist_adv/\"\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"\\r Batch %s\" % counter, end=\"\")\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "\n",
    "    images_med   = (torch.vstack(images_med).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_mean  = (torch.vstack(images_mean).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_champ = (torch.vstack(images_champ).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open(SAVE_DIR + 'train_images_med_%s.pickle'   % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol  = pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'train_images_mean_%s.pickle'  % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'train_images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1 \n",
    "    if counter > 8:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 16"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "SAVE_DIR = \"mnist_adv/\"\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"\\r Batch %s\" % counter, end=\"\")\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "    images_med   = (torch.vstack(images_med).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_mean  = (torch.vstack(images_mean).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_champ = (torch.vstack(images_champ).reshape(-1,28, 28)*255).type(torch.uint8).detach()\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open(SAVE_DIR + 'test_images_med_%s.pickle'   % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'test_images_mean_%s.pickle'  % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SAVE_DIR + 'test_images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1\n",
    "    if counter > 16:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
