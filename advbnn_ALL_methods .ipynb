{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Loaded 100 sample models\n"
     ]
    }
   ],
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")\n",
    "sampled_models = load_models(K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial example methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone() - (eps*saliency), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone() + (eps*saliency), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bim(image, eps, model, steps, alpha, targets):\n",
    "    with torch.no_grad():\n",
    "        image_adv = torch.clone(image)\n",
    "    for n in range(steps):\n",
    "        image_adv.requires_grad = True\n",
    "        image_adv.grad = None\n",
    "        output = model(image_adv)             #forward pass\n",
    "        output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "        loss_fct = torch.nn.NLLLoss()\n",
    "        loss = loss_fct(output, targets)       #compute loss\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            x_grad =  alpha * torch.sign(image_adv.grad)    \n",
    "            adv_temp = image_adv.data + x_grad  #add perturbation to img_variable which \n",
    "            #also contains perturbation from previous iterations\n",
    "            total_grad = torch.clamp(adv_temp - image, -eps, eps)\n",
    "            image_adv.data = image + total_grad\n",
    "\n",
    "    \n",
    "    return image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itcm(image, eps, model, steps, alpha, targets):\n",
    "    with torch.no_grad():\n",
    "        image_adv = torch.clone(image)\n",
    "    for n in range(steps):\n",
    "        image_adv.requires_grad = True\n",
    "        image_adv.grad = None\n",
    "        output = model(image_adv)             #forward pass\n",
    "        output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "        loss_fct = torch.nn.NLLLoss()\n",
    "        loss = loss_fct(output, targets)       #compute loss\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            x_grad =  alpha * torch.sign(image_adv.grad)    \n",
    "            adv_temp = image_adv.data + x_grad  #add perturbation to img_variable which \n",
    "            #also contains perturbation from previous iterations\n",
    "            total_grad = torch.clamp(adv_temp - image, -eps, eps)\n",
    "            image_adv.data = image - total_grad\n",
    "    \n",
    "    return image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, new_images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "    \n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool (for iterative methods)?\n",
    "def how_many_can_it_fool2(sampled_models, eps, saliency):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        \n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        #new_images = otcm(images, eps, saliency)\n",
    "        \n",
    "        # Fast Gradient Sign Method (FGSM)\n",
    "        new_images = fgsm(images, eps, saliency)\n",
    "        \n",
    "        #Basic Iterative Method (BIM-untargeted)\n",
    "        #new_images = bim(images, eps, sampled_models[k], 15, eps*0.25, targets)\n",
    "        \n",
    "        #Iterative Target Class Method (ITCM)\n",
    "        #new_images = itcm(images, eps, sampled_models[k], 15, eps*0.25, torch.tensor([9]))\n",
    "        \n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        \n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: \n",
    "\n",
    "\n",
    "\n",
    "Create 100 adv examples from the 100 sampled models.\n",
    "\n",
    "Test how many of the 100 sampled models each of the adv. examples can fool. \n",
    "\n",
    "Then combine to get one strong adv example, and test how many THIS example can fool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08, 0.12, 0.18, 0.14, 0.1, 0.19, 0.15, 0.11, 0.13, 0.09, 0.19, 0.07, 0.11, 0.13, 0.12, 0.09, 0.11, 0.14, 0.18, 0.17, 0.09, 0.17, 0.13, 0.09, 0.18, 0.14, 0.07, 0.14, 0.12, 0.12, 0.18, 0.12, 0.15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect noises (saliencies)\n",
    "EPS = 0.18\n",
    "saliencies = []\n",
    "how_many_fooled = []\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for k in tqdm.tqdm(range(len(sampled_models))):\n",
    "    images.grad = None\n",
    "    images.requires_grad = True\n",
    "    \n",
    "    #### Get old class/target ####\n",
    "    #Using OTCM or ITCM:\n",
    "    #old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), torch.tensor([9])])\n",
    "    \n",
    "    #Using FGSM or BIM:\n",
    "    old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), targets])\n",
    "    \n",
    "    \n",
    "    #### Compute adversarial examples ####\n",
    "    #Using OTCM:\n",
    "    #new_images = otcm(images, EPS, images.grad.sign())\n",
    "    \n",
    "    #Using FGSM:\n",
    "    new_images = fgsm(images, EPS, images.grad.sign())\n",
    "    \n",
    "    #Using BIM:\n",
    "    #new_images = bim(images, EPS, sampled_models[k], 15, EPS*0.25, targets)\n",
    "    \n",
    "    #Using ITCM:\n",
    "    #new_images = itcm(images, EPS, sampled_models[k], 15, EPS*0.25, torch.tensor([9]))\n",
    "    \n",
    "    #### Forward pass on adv. example to get new class####\n",
    "    new_class = forward_pass(sampled_models[k], new_images)\n",
    "\n",
    "    if old_class != new_class:\n",
    "        # How many models can this adv. example fool?\n",
    "        how_many_fooled += [how_many_can_it_fool(sampled_models, new_images)]\n",
    "\n",
    "        #saliencies += [(new_images-images).sign().detach().view(28, 28)]\n",
    "        saliencies += [images.grad.sign().view(28, 28)]\n",
    "print(how_many_fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#### Combining disturbance ####\n",
    "# distributional saliency map\n",
    "saliencies = torch.stack(saliencies)\n",
    "print(saliencies.shape)\n",
    "newsaliency = torch.zeros(28, 28)\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        # choose median perturbation\n",
    "        newsaliency[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "newsaliency = newsaliency.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined example fooling rate: 0.35\n"
     ]
    }
   ],
   "source": [
    "#### Get strong adv_example using new/combined saliency ####\n",
    "new_images = fgsm(images, EPS, newsaliency)\n",
    "\n",
    "\n",
    "\n",
    "#### Test how many of the 100 models this strong adv example can fool ####\n",
    "\n",
    "#For FGSM or OTCM\n",
    "#foolrate_comb = how_many_can_it_fool(sampled_models, new_images)\n",
    "\n",
    "#For BIM or ITCM\n",
    "foolrate_comb = how_many_can_it_fool2(sampled_models, EPS, newsaliency)\n",
    "\n",
    "print('Combined example fooling rate:', foolrate_comb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of individual example fooling rates 0.13030303030303034\n",
      "Increase: 0.21969696969696964\n"
     ]
    }
   ],
   "source": [
    "#Compare the average fooling rate of the individual examples to the combined fooling rate\n",
    "\n",
    "avg_foolrate_ind = np.mean(how_many_fooled)\n",
    "print('Average of individual example fooling rates', avg_foolrate_ind)\n",
    "\n",
    "print('Increase:',foolrate_comb-avg_foolrate_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((newsaliency*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
