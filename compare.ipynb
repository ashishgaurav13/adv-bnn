{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import tqdm\n",
    "import os, pickle\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "common.set_seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NN, self).__init__()\n",
    "        self.A = torch.nn.Linear(ni, nh)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.B = torch.nn.Linear(nh, no)\n",
    "    def forward(self, x):\n",
    "        # Two layer neural network\n",
    "        x = self.B(self.relu(self.A(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# Point estimate NN\n",
    "net = NN(28*28, 1024, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y):\n",
    "    # Put priors on weights and biases \n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.weight), \n",
    "            scale=torch.ones_like(net.A.weight),\n",
    "        ).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.A.bias), \n",
    "            scale=torch.ones_like(net.A.bias),\n",
    "        ).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.weight), \n",
    "            scale=torch.ones_like(net.B.weight),\n",
    "        ).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(\n",
    "            loc=torch.zeros_like(net.B.bias), \n",
    "            scale=torch.ones_like(net.B.bias),\n",
    "        ).independent(1),\n",
    "    }\n",
    "    # Create a NN module using the priors\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    regressor = lmodule()\n",
    "    # Do a forward pass on the NN module, i.e. yhat=f(x) and condition on yhat=y\n",
    "    lhat = torch.nn.LogSoftmax(dim=1)(regressor(x))\n",
    "    pyro.sample(\"obs\", pyro.distributions.Categorical(logits=lhat).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def guide(x, y):\n",
    "    # Create parameters for variational distribution priors\n",
    "    Aw_mu = pyro.param(\"Aw_mu\", torch.randn_like(net.A.weight))\n",
    "    Aw_sigma = softplus(pyro.param(\"Aw_sigma\", torch.randn_like(net.A.weight)))\n",
    "    Ab_mu = pyro.param(\"Ab_mu\", torch.randn_like(net.A.bias))\n",
    "    Ab_sigma = softplus(pyro.param(\"Ab_sigma\", torch.randn_like(net.A.bias)))\n",
    "    Bw_mu = pyro.param(\"Bw_mu\", torch.randn_like(net.B.weight))\n",
    "    Bw_sigma = softplus(pyro.param(\"Bw_sigma\", torch.randn_like(net.B.weight)))\n",
    "    Bb_mu = pyro.param(\"Bb_mu\", torch.randn_like(net.B.bias))\n",
    "    Bb_sigma = softplus(pyro.param(\"Bb_sigma\", torch.randn_like(net.B.bias)))\n",
    "    # Create random variables similarly to model\n",
    "    priors = {\n",
    "        \"A.weight\": pyro.distributions.Normal(loc=Aw_mu, scale=Aw_sigma).independent(2),\n",
    "        \"A.bias\": pyro.distributions.Normal(loc=Ab_mu, scale=Ab_sigma).independent(1),\n",
    "        \"B.weight\": pyro.distributions.Normal(loc=Bw_mu, scale=Bw_sigma).independent(2),\n",
    "        \"B.bias\": pyro.distributions.Normal(loc=Bb_mu, scale=Bb_sigma).independent(1),\n",
    "    }\n",
    "    # Return NN module from these random variables\n",
    "    lmodule = pyro.random_module(\"module\", net, priors)\n",
    "    return lmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stochastic variational inference to find q(w) closest to p(w|D)\n",
    "svi = pyro.infer.SVI(\n",
    "    model, guide, pyro.optim.Adam({'lr': 0.01}), pyro.infer.Trace_ELBO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g\" % (epoch, loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(K = 100):\n",
    "    # Load the models\n",
    "    sampled_models = [NN(28*28, 1024, 10) for i in range(K)]\n",
    "    for net, state_dict in zip(sampled_models, torch.load(\"models/model.pt\")):\n",
    "        net.load_state_dict(state_dict)\n",
    "    print(\"Loaded %d sample models\" % K)\n",
    "    return sampled_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_save_models(epochs = 10, K = 100, modelname = \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 sample models\n"
     ]
    }
   ],
   "source": [
    "sampled_models = load_models(K = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\"mnist_adv/\") if \"test_images_med\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(\"mnist_adv/\"+d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images)\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "test_dataset.data    = None\n",
    "test_dataset.targets = None\n",
    "\n",
    "test_dataset.data    = images\n",
    "test_dataset.targets = targets\n",
    "\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                                         generator=torch.Generator().manual_seed(156))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_loss(bnn_svi):\n",
    "    # Validate with SVI\n",
    "    val_loss = 0.\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        val_loss += svi.evaluate_loss(images, labels)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Test Accuracy\n",
    "\n",
    "- Sample 25 models using AdvBNN\n",
    "- Calculate overall accuracies\n",
    "  - Generate predictions per sampled model to all 25\n",
    "  - Calculate success rate per image (x/25)\n",
    "  - Calculate mean and std\n",
    "- Repeat the same with BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train, val = random_split(train_dataset,[50000,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and flatten the input\n",
    "images, targets = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, images, loss_target = None):\n",
    "    output = model(images)\n",
    "    output = torch.nn.LogSoftmax(dim=-1)(output)\n",
    "    which_class = torch.argmax(output).item()\n",
    "    if loss_target:\n",
    "        loss, target = loss_target\n",
    "        loss(output, target).backward()\n",
    "    return which_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otcm(images, eps, saliency):\n",
    "    return torch.clamp(images.clone()-eps*saliency, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many models can an adversarial example fool?\n",
    "def how_many_can_it_fool(sampled_models, eps, saliency,images):\n",
    "    fool = 0\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass on sampled model k\n",
    "        old_class = forward_pass(sampled_models[k], images)\n",
    "        # One step Target Class Method (OTCM); saliency is noise\n",
    "        new_images = otcm(images, eps, saliency)\n",
    "        # Forward pass again on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        # If we change the class, we fool the model\n",
    "        fool += int(old_class != new_class)\n",
    "    return fool/len(sampled_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency(EPS,target,images):\n",
    "    # Collect noises (saliencies)\n",
    "    # EPS = 0.18\n",
    "    saliencies = []\n",
    "    how_many_fooled = []\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    # target = torch.tensor([1])\n",
    "    target = torch.tensor([target])\n",
    "    for k in range(len(sampled_models)):\n",
    "        # Forward pass\n",
    "        # Compute loss w.r.t. an incorrect class\n",
    "        # Note that we just have to ensure this class is different from targets\n",
    "        # print(\"\\r Processing \" + str(k+1) + \"/%s\" % len(sampled_models), end=\"\")\n",
    "        images.grad = None\n",
    "        images.requires_grad = True\n",
    "        old_class = forward_pass(sampled_models[k], images, [torch.nn.NLLLoss(), target])\n",
    "        # Compute adversarial example\n",
    "        new_images = otcm(images, EPS, images.grad.sign())\n",
    "        # Forward pass on adv. example\n",
    "        new_class = forward_pass(sampled_models[k], new_images)\n",
    "        if old_class != new_class:\n",
    "            # How many models can this adv. example fool?\n",
    "            how_many_fooled += [how_many_can_it_fool(sampled_models, EPS, images.grad.sign(), images)]\n",
    "            saliencies += [images.grad.sign().view(28, 28)]\n",
    "    # print(\"\\nFinished\")\n",
    "    return saliencies, how_many_fooled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saliencies(saliencies,success):\n",
    "    # distributional saliency map\n",
    "    saliencies = torch.stack(saliencies)\n",
    "    # print(saliencies.shape)\n",
    "    combined_med  = torch.zeros(28, 28)\n",
    "    combined_mean = torch.zeros(28, 28)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # choose median perturbation\n",
    "            combined_med[i, j] = np.percentile(saliencies[:, i, j].numpy(), 50)\n",
    "            combined_mean[i, j] = saliencies[:, i, j].mean().item()\n",
    "    combined_med  = combined_med.flatten()\n",
    "    combined_mean = combined_mean.flatten()\n",
    "    champ         = saliencies[success.index(max(success))].flatten()\n",
    "    return combined_med, combined_mean, champ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliencies, success = generate_saliency(0.18,1, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = .18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_med, combined_mean, champ = combine_saliencies(saliencies,success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(how_many_can_it_fool(sampled_models, EPS, combined_med,images))\n",
    "new_images = otcm(images, EPS, combined_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((combined_med*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(images.reshape(28, 28).detach().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow((champ*EPS).reshape(28, 28).detach().numpy(), vmin=-1., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(new_images.reshape(28, 28).detach().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Train with Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 25/25\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "temp_sals, success = generate_saliency(EPS,1,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n"
     ]
    }
   ],
   "source": [
    "EPS = 0.18\n",
    "# for i in range(len(train_dataset.targets)):\n",
    "target_len = len(train_dataset.classes)\n",
    "targets    = set(range(10))\n",
    "counter    = 1 \n",
    "successes  = []\n",
    "for data in train_loader:\n",
    "    images_med   = []\n",
    "    images_mean  = []\n",
    "    images_champ = []\n",
    "    tru_labels   = []\n",
    "    images, labels = data\n",
    "    images = images.view(-1, 28*28)\n",
    "    print(\"Batch %s\" % counter)\n",
    "\n",
    "    for i in range(images.shape[0]): #\n",
    "        # the real target\n",
    "        target_org = labels[i].item() \n",
    "        # the target that wanted to be resulted in\n",
    "        target     = int(np.random.choice(list(targets - set([target_org])),size=1))\n",
    "        image      = images[i:i+1,:] \n",
    "        # generating saliency maps using each sampled network\n",
    "        temp_sals, success = generate_saliency(EPS,target,image)\n",
    "        successes.append(success)\n",
    "        # combining maps into three types\n",
    "        combined_med, combined_mean, champ = combine_saliencies(temp_sals,success)\n",
    "        # creating image\n",
    "        images_med.append(otcm(image, EPS, combined_med))\n",
    "        images_mean.append(otcm(image, EPS, combined_mean))\n",
    "        images_champ.append(otcm(image, EPS, champ))\n",
    "        tru_labels.append(target_org)\n",
    "    tru_labels   = torch.tensor(tru_labels)\n",
    "    images_med   = torch.vstack(images_med).reshape(-1,28, 28)\n",
    "    images_mean  = torch.vstack(images_mean).reshape(-1,28, 28)\n",
    "    images_champ = torch.vstack(images_champ).reshape(-1,28, 28)\n",
    "    images_med   = {'images': images_med,  'labels': tru_labels}\n",
    "    images_mean  = {'images': images_mean, 'labels': tru_labels}\n",
    "    images_champ = {'images': images_champ,'labels': tru_labels}\n",
    "    \n",
    "    \n",
    "    with open('images_med_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_med, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('images_mean_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_mean, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('images_champ_%s.pickle' % counter, 'wb') as handle:\n",
    "        pickle.dump(images_champ, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    counter += 1 \n",
    "    if counter > 8:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN with Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                       transform=torchvision.transforms.ToTensor())\n",
    "# Test data loader with batch_size 1\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(\".\") if \"images_med\" in d]\n",
    "\n",
    "images  = []\n",
    "targets = []\n",
    "for d in dirs:\n",
    "    with open(d, 'rb') as handle:\n",
    "        temp = pickle.load(handle)\n",
    "        images.append(temp[\"images\"])\n",
    "        targets.append(temp[\"labels\"])\n",
    "        \n",
    "images  = torch.vstack(images).int()\n",
    "targets = torch.hstack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data    = torch.vstack([train_dataset.data, images])\n",
    "train_dataset.targets = torch.hstack([train_dataset.targets, targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = random_split(train_dataset,[51024,10000], generator=torch.Generator().manual_seed(156))\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val  , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(epochs = 10, K = 100, modelname = \"AdvBNN.pt\"):\n",
    "    if os.path.exists(modelname):\n",
    "        print(\"File exists\")\n",
    "        return\n",
    "    # Train with SVI\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            loss  += svi.step(images, labels)\n",
    "        loss /= len(train_loader.dataset)\n",
    "        # model.eval()     # Optional when not using Model Specific layer\n",
    "        val_loss = 0.\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 28*28)\n",
    "            val_loss += svi.evaluate_loss(images, labels)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(\"Epoch %g: Loss = %g: Val_Loss = %g\" % (epoch, loss,val_loss))\n",
    "    # Sample k models from the posterior\n",
    "    sampled_models = [guide(None, None) for i in range(K)]\n",
    "    # Save the models\n",
    "    nn_dicts = []\n",
    "    for i in range(len(sampled_models)):\n",
    "        nn_dicts += [sampled_models[i].state_dict()]\n",
    "    torch.save(nn_dicts, modelname)\n",
    "    print(\"Saved %d models\" % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 72.8608: Val_Loss = 70.427\n",
      "Epoch 1: Loss = 68.552: Val_Loss = 67.6154\n",
      "Epoch 2: Loss = 66.6034: Val_Loss = 66.389\n",
      "Epoch 3: Loss = 65.7868: Val_Loss = 66.6741\n",
      "Epoch 4: Loss = 65.8724: Val_Loss = 66.3151\n",
      "Epoch 5: Loss = 65.568: Val_Loss = 66.1872\n",
      "Epoch 6: Loss = 65.6789: Val_Loss = 66.2749\n",
      "Epoch 7: Loss = 65.6447: Val_Loss = 66.4574\n",
      "Epoch 8: Loss = 65.7479: Val_Loss = 66.6923\n",
      "Epoch 9: Loss = 65.6441: Val_Loss = 66.1903\n",
      "Saved 100 models\n"
     ]
    }
   ],
   "source": [
    "train_adversarial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
